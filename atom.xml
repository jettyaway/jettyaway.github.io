<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[IT framer]]></title>
  <link href="http://www.blacklight.xin/atom.xml" rel="self"/>
  <link href="http://www.blacklight.xin/"/>
  <updated>2018-12-07T23:30:43+08:00</updated>
  <id>http://www.blacklight.xin/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[spark 内存调优【转】]]></title>
    <link href="http://www.blacklight.xin/15442384370091.html"/>
    <updated>2018-12-08T11:07:17+08:00</updated>
    <id>http://www.blacklight.xin/15442384370091.html</id>
    <content type="html"><![CDATA[
<p><strong>【Spark集群并行度】</strong></p>

<p>在Spark集群环境下，只有足够高的并行度才能使系统资源得到充分的利用，可以通过修改spark-env.sh来调整Executor的数量和使用资源，Standalone和YARN方式资源的调度管理是不同的。</p>

<span id="more"></span><!-- more -->

<p>在Standalone模式下:</p>

<p>1. 每个节点使用的最大内存数：SPARK_WORKER_INSTANCES*SPARK_WORKER_MEMORY；</p>

<p>2. 每个节点的最大并发task数：SPARK_WORKER_INSTANCES*SPARK_WORKER_CORES。</p>

<p>在YARN模式下：</p>

<p>1. 集群task并行度：SPARK_ EXECUTOR_INSTANCES* SPARK_EXECUTOR_CORES；</p>

<p>2. 集群内存总量：(executor个数) * (SPARK_EXECUTOR_MEMORY+ spark.yarn.executor.memoryOverhead)+(SPARK_DRIVER_MEMORY+spark.yarn.driver.memoryOverhead)。</p>

<p>重点强调：Spark对Executor和Driver额外添加堆内存大小，Executor端：由spark.yarn.executor.memoryOverhead设置，默认值executorMemory * 0.07与384的最大值。Driver端：由spark.yarn.driver.memoryOverhead设置，默认值driverMemory * 0.07与384的最大值。</p>

<p>通过调整上述参数，可以提高集群并行度，让系统同时执行的任务更多，那么对于相同的任务，并行度高了，可以减少轮询次数。举例说明：如果一个stage有100task，并行度为50，那么执行完这次任务，需要轮询两次才能完成，如果并行度为100，那么一次就可以了。</p>

<p>但是在资源相同的情况，并行度高了，相应的Executor内存就会减少，所以需要根据实际实况协调内存和core。此外，Spark能够非常有效的支持短时间任务（例如：200ms），因为会对所有的任务复用JVM，这样能减小任务启动的消耗，Standalone模式下，core可以允许1-2倍于物理core的数量进行超配。</p>

<p><strong>【Spark任务数量调整】</strong></p>

<p>Spark的任务数由stage中的起始的所有RDD的partition之和数量决定，所以需要了解每个RDD的partition的计算方法。以Spark应用从HDFS读取数据为例，HadoopRDD的partition切分方法完全继承于MapReduce中的FileInputFormat，具体的partition数量由HDFS的块大小、mapred.min.split.size的大小、文件的压缩方式等多个因素决定，详情需要参见FileInputFormat的代码。</p>

<p><strong>【Spark内存调优】</strong></p>

<p>内存优化有三个方面的考虑：对象所占用的内存，访问对象的消耗以及垃圾回收所占用的开销。</p>

<p><strong>1. 对象所占内存，优化数据结构</strong></p>

<p>Spark 默认使用Java序列化对象，虽然Java对象的访问速度更快，但其占用的空间通常比其内部的属性数据大2-5倍。为了减少内存的使用，减少Java序列化后的额外开销，下面列举一些Spark官网（<a href="http://spark.apache.org/docs/latest/tuning.html#tuning-data-structures%EF%BC%89%E6%8F%90%E4%BE%9B%E7%9A%84%E6%96%B9%E6%B3%95%E3%80%82">http://spark.apache.org/docs/latest/tuning.html#tuning-data-structures）提供的方法。</a></p>

<p>（1）使用对象数组以及原始类型（primitive type）数组以替代Java或者Scala集合类（collection class)。fastutil 库为原始数据类型提供了非常方便的集合类，且兼容Java标准类库。</p>

<p>（2）尽可能地避免采用含有指针的嵌套数据结构来保存小对象。</p>

<p>（3）考虑采用数字ID或者枚举类型以便替代String类型的主键。</p>

<p>（4）如果内存少于32GB，设置JVM参数-XX:+UseCom-pressedOops以便将8字节指针修改成4字节。与此同时，在Java 7或者更高版本，设置JVM参数-XX:+UseC-----ompressedStrings以便采用8比特来编码每一个ASCII字符。</p>

<p><strong>2. 内存回收</strong></p>

<p>（1）获取内存统计信息：优化内存前需要了解集群的内存回收频率、内存回收耗费时间等信息，可以在spark-env.sh中设置SPARK_JAVA_OPTS=“-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps $ SPARK_JAVA_OPTS”来获取每一次内存回收的信息。</p>

<p>（2）优化缓存大小：默认情况Spark采用运行内存（spark.executor.memory）的60%来进行RDD缓存。这表明在任务执行期间，有40%的内存可以用来进行对象创建。如果任务运行速度变慢且JVM频繁进行内存回收，或者内存空间不足，那么降低缓存大小设置可以减少内存消耗，可以降低spark.storage.memoryFraction的大小。</p>

<p><strong>3. 频繁GC或者OOM</strong></p>

<p>针对这种情况，首先要确定现象是发生在Driver端还是在Executor端，然后在分别处理。</p>

<p>Driver端：通常由于计算过大的结果集被回收到Driver端导致，需要调大Driver端的内存解决，或者进一步减少结果集的数量。</p>

<p>Executor端：</p>

<p>（1）以外部数据作为输入的Stage：这类Stage中出现GC通常是因为在Map侧进行map-side-combine时，由于group过多引起的。解决方法可以增加partition的数量（即task的数量）来减少每个task要处理的数据，来减少GC的可能性。</p>

<p>（2）以shuffle作为输入的Stage：这类Stage中出现GC的通常原因也是和shuffle有关，常见原因是某一个或多个group的数据过多，也就是所谓的数据倾斜，最简单的办法就是增加shuffle的task数量，比如在SparkSQL中设置SET spark.sql.shuffle.partitions=400，如果调大shuffle的task无法解决问题，说明你的数据倾斜很严重，某一个group的数据远远大于其他的group，需要你在业务逻辑上进行调整，预先针对较大的group做单独处理。</p>

<p><strong>【修改序列化】</strong></p>

<p>使用Kryo序列化，因为Kryo序列化结果比Java标准序列化更小，更快速。具体方法：spark-default.conf 里设置spark.serializer为org.apache.spark.serializer.KryoSerializer 。</p>

<p>参考官方文档（<a href="http://spark.apache.org/docs/latest/tuning.html#summary%EF%BC%89%EF%BC%9A%E5%AF%B9%E4%BA%8E%E5%A4%A7%E5%A4%9A%E6%95%B0%E7%A8%8B%E5%BA%8F%E8%80%8C%E8%A8%80%EF%BC%8C%E9%87%87%E7%94%A8Kryo%E6%A1%86%E6%9E%B6%E4%BB%A5%E5%8F%8A%E5%BA%8F%E5%88%97%E5%8C%96%E8%83%BD%E5%A4%9F%E8%A7%A3%E5%86%B3%E6%80%A7%E8%83%BD%E7%9B%B8%E5%85%B3%E7%9A%84%E5%A4%A7%E9%83%A8%E5%88%86%E9%97%AE%E9%A2%98%E3%80%82">http://spark.apache.org/docs/latest/tuning.html#summary）：对于大多数程序而言，采用Kryo框架以及序列化能够解决性能相关的大部分问题。</a></p>

<p><strong>【Spark 磁盘调优】</strong></p>

<p>在集群环境下，如果数据分布不均匀，造成节点间任务分布不均匀，也会导致节点间源数据不必要的网络传输，从而大大影响系统性能，那么对于磁盘调优最好先将数据资源分布均匀。除此之外，还可以对源数据做一定的处理：</p>

<p>1. 在内存允许范围内，将频繁访问的文件或数据置于内存中；</p>

<p>2. 如果磁盘充裕，可以适当增加源数据在HDFS上的备份数以减少网络传输；</p>

<p>3. Spark支持多种文件格式及压缩方式，根据不同的应用环境进行合理的选择。如果每次计算只需要其中的某几列，可以使用列式文件格式，以减少磁盘I/O，常用的列式有parquet、rcfile。如果文件过大，将原文件压缩可以减少磁盘I/O，例如：gzip、snappy、lzo。</p>

<p><strong>【其他】</strong></p>

<p>** 广播变量（broadcast）**</p>

<p>当task中需要访问一个Driver端较大的数据时，可以通过使用SparkContext的广播变量来减小每一个任务的大小以及在集群中启动作业的消耗。参考官方文档<a href="http://spark.apache.org/docs/latest/tuning.html#broadcasting-large-variables%E3%80%82">http://spark.apache.org/docs/latest/tuning.html#broadcasting-large-variables。</a></p>

<p>** 开启推测机制 **</p>

<p>推测机制后，如果集群中，某一台机器的几个task特别慢，推测机制会将任务分配到其他机器执行，最后Spark会选取最快的作为最终结果。</p>

<p>在spark-default.conf 中添加：spark.speculation true</p>

<p>推测机制与以下几个参数有关：</p>

<p>1. spark.speculation.interval 100：检测周期，单位毫秒；</p>

<p>2. spark.speculation.quantile 0.75：完成task的百分比时启动推测；</p>

<p>** 并行度设置 **</p>

<h5 id="toc_0">1.Spark的并行度指的是什么？</h5>

<p><strong>spark作业中，各个stage的task的数量，也就代表了spark作业在各个阶段stage的并行度</strong></p>

<p>当分配完所能分配的最大资源了，然后对应资源去调节程序的并行度，如果并行度没有与资源相匹配，那么<strong>导致</strong> <strong>你分配下去的资源都浪费掉了。</strong> <strong>同时并行运行，还可以让每个task要处理的数量变少</strong>（很简单的原理。合理设置并行度，可以充分利用集群资源，减少每个task处理数据量，而增加性能加快运行速度。)</p>

<p>举例：</p>

<p>假如， 现在已经在spark-submit 脚本里面，给我们的spark作业分配了足够多的资源，比如<strong>50个executor</strong>，每个<strong>executor 有10G内存</strong>，<strong>每个executor有3个cpu core</strong> 。 基本已经达到了集群或者yarn队列的资源上限。</p>

<p>task没有设置，或者设置的很少，比如就设置了，100个task 。 ** 50个executor ，每个executor 有3个core,也就是说Application 任何一个stage运行的时候,都有总数150个cpu core 可以并行运 ** 。但是，你现在只有100个task ，平均分配一下，每个executor 分配到2个task，ok，那么同时在运行的task，只有100个task，每个executor 只会并行运行 2个task。 每个executor 剩下的一个cpu core 就浪费掉了！<strong>你的资源，虽然分配充足了，但是问题是， 并行度没有与资源相匹配，导致你分配下去的资源都浪费掉了。</strong><strong>合理的并行度的设置，应该要设置的足够大，大到可以完全合理的利用你的集群资源；</strong> 比如上面的例子，总共集群有150个cpu core ，可以并行运行150个task。那么你就应该将你的Application 的并行度，至少设置成150个，才能完全有效的利用你的集群资源，让150个task ，并行执行，而且task增加到150个以后，即可以<strong>同时并行运行，还可以让每个task要处理的数量变少</strong>； 比如总共** 150G 的数据**要处理， **如果是100个task <strong>，</strong>每个task 要计算1.5G的数据<strong>。 现在</strong>增加到150个task，每个task只要处理1G数据**。</p>

<h5 id="toc_1"><strong>2.如何去提高并行度？</strong></h5>

<p><strong>1、task数量，至少设置成与spark Application 的总cpu core 数量相同</strong>（最理性情况，150个core，分配150task，一起运行，差不多同一时间运行完毕）**官方推荐，task数量，设置成spark Application 总cpu core数量的2~3倍 ，比如150个cpu core ，基本设置 task数量为 300~ 500. **与理性情况不同的，有些task 会运行快一点，比如50s 就完了，有些task 可能会慢一点，要一分半才运行完，所以如果你的task数量，刚好设置的跟cpu core 数量相同，可能会导致资源的浪费，因为 比如150task ，10个先运行完了，剩余140个还在运行，但是这个时候，就有10个cpu core空闲出来了，导致浪费。如果设置2~3倍，那么一个task运行完以后，另外一个task马上补上来，尽量让cpu core不要空闲。同时尽量提升spark运行效率和速度。提升性能。</p>

<p>**    2、如何设置一个Spark Application的并行度？**</p>

<pre><code>  **spark.defalut.parallelism** **默认是没有值的，如果设置了值比如说10，是在shuffle的过程才会起作用**（val rdd2 = rdd1.reduceByKey(_+_) //rdd2的分区数就是10，rdd1的分区数不受这个参数的影响）
</code></pre>

<p>**      new SparkConf().set(“spark.defalut.parallelism”,”“500)**</p>

<p>** 3、如果读取的数据在HDFS上，增加block数**，默认情况下split与block是一对一的，而split又与RDD中的partition对应，所以增加了block数，也就提高了并行度。</p>

<pre><code>**4、RDD.repartition**，给RDD重新设置partition的数量

**5、reduceByKey的算子指定partition的数量**

             val rdd2 = rdd1.reduceByKey(_+_,10)  val rdd3 = rdd2.map.filter.reduceByKey(_+_)
</code></pre>

<p>** 6、<strong>val rdd3 = rdd1.</strong>join**（rdd2）  <strong>rdd3里面partiiton的数量是由父RDD中最多的partition数量来决定，因此使用join算子的时候，增加父RDD中partition的数量。</strong></p>

<pre><code>**7、spark.sql.shuffle.partitions //spark sql中shuffle过程中partitions的数量**
</code></pre>

<p>3. spark.speculation.multiplier 1.5：比其他的慢多少倍时启动推测。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【转】Java Hotspot G1 GC的一些关键技术]]></title>
    <link href="http://www.blacklight.xin/15442383252409.html"/>
    <updated>2018-12-08T11:05:25+08:00</updated>
    <id>http://www.blacklight.xin/15442383252409.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">前言</h2>

<p>G1 GC，全称Garbage-First Garbage Collector，通过-XX:+UseG1GC参数来启用，作为体验版随着JDK 6u14版本面世，在JDK 7u4版本发行时被正式推出，相信熟悉JVM的同学们都不会对它感到陌生。在JDK 9中，G1被提议设置为默认垃圾收集器（JEP 248）。在官网中，是这样描述G1的：</p>

<span id="more"></span><!-- more -->   

<blockquote>
<p>The Garbage-First (G1) collector is a server-style garbage collector, targeted for multi-processor machines with large memories. It meets garbage collection (GC) pause time goals with a high probability, while achieving high throughput. The G1 garbage collector is fully supported in Oracle JDK 7 update 4 and later releases. The G1 collector is designed for applications that:</p>

<ul>
<li>  Can operate concurrently with applications threads like the CMS collector.</li>
<li>  Compact free space without lengthy GC induced pause times.</li>
<li>  Need more predictable GC pause durations.</li>
<li>  Do not want to sacrifice a lot of throughput performance.</li>
<li>  Do not require a much larger Java heap.</li>
</ul>
</blockquote>

<p>从官网的描述中，我们知道G1是一种服务器端的垃圾收集器，应用在多处理器和大容量内存环境中，在实现高吞吐量的同时，尽可能的满足垃圾收集暂停时间的要求。它是专门针对以下应用场景设计的:</p>

<ul>
<li>  像CMS收集器一样，能与应用程序线程并发执行。</li>
<li>  整理空闲空间更快。</li>
<li>  需要GC停顿时间更好预测。</li>
<li>  不希望牺牲大量的吞吐性能。</li>
<li>  不需要更大的Java Heap。</li>
</ul>

<p>G1收集器的设计目标是取代CMS收集器，它同CMS相比，在以下方面表现的更出色：</p>

<ul>
<li>  G1是一个有整理内存过程的垃圾收集器，不会产生很多内存碎片。</li>
<li>  G1的Stop The World(STW)更可控，G1在停顿时间上添加了预测机制，用户可以指定期望停顿时间。</li>
</ul>

<p>有了以上的特性，难怪有人说它是一款驾驭一切的垃圾收集器（<a href="http://www.infoq.com/articles/G1-One-Garbage-Collector-To-Rule-Them-All">G1: One Garbage Collector To Rule Them All</a>）。本文带大家来了解一下G1 GC的一些关键技术，为能正确的使用它，做好理论基础的铺垫。</p>

<h2 id="toc_1">G1中几个重要概念</h2>

<p>在G1的实现过程中，引入了一些新的概念，对于实现高吞吐、没有内存碎片、收集时间可控等功能起到了关键作用。下面我们就一起看一下G1中的这几个重要概念。</p>

<h3 id="toc_2">Region</h3>

<p>传统的GC收集器将连续的内存空间划分为新生代、老年代和永久代（JDK 8去除了永久代，引入了元空间Metaspace），这种划分的特点是各代的存储地址（逻辑地址，下同）是连续的。如下图所示：<br/>
<img src="https://tech.meituan.com/img/g1/conventional_gc_layout.png" alt=""/></p>

<p>而G1的各代存储地址是不连续的，每一代都使用了n个不连续的大小相同的Region，每个Region占有一块连续的虚拟内存地址。如下图所示：<br/>
<img src="https://tech.meituan.com/img/g1/g1_gc_layout.png" alt=""/></p>

<p>在上图中，我们注意到还有一些Region标明了H，它代表Humongous，这表示这些Region存储的是巨大对象（humongous object，H-obj），即大小大于等于region一半的对象。H-obj有如下几个特征：</p>

<ul>
<li>  H-obj直接分配到了old gen，防止了反复拷贝移动。</li>
<li>  H-obj在global concurrent marking阶段的cleanup 和 full GC阶段回收。</li>
<li>  在分配H-obj之前先检查是否超过 initiating heap occupancy percent和the marking threshold, 如果超过的话，就启动global concurrent marking，为的是提早回收，防止 evacuation failures 和 full GC。</li>
</ul>

<p>为了减少连续H-objs分配对GC的影响，需要把大对象变为普通的对象，建议增大Region size。</p>

<p>一个Region的大小可以通过参数-XX:G1HeapRegionSize设定，取值范围从1M到32M，且是2的指数。如果不设定，那么G1会根据Heap大小自动决定。相关的设置代码如下：</p>

<pre><code>// share/vm/gc_implementation/g1/heapRegion.cpp
// Minimum region size; we won&#39;t go lower than that.
// We might want to decrease this in the future, to deal with small
// heaps a bit more efficiently.
#define MIN_REGION_SIZE  (      1024 * 1024 )
// Maximum region size; we don&#39;t go higher than that. There&#39;s a good
// reason for having an upper bound. We don&#39;t want regions to get too
// large, otherwise cleanup&#39;s effectiveness would decrease as there
// will be fewer opportunities to find totally empty regions after
// marking.
#define MAX_REGION_SIZE  ( 32 * 1024 * 1024 )
// The automatic region size calculation will try to have around this
// many regions in the heap (based on the min heap size).
#define TARGET_REGION_NUMBER          2048
void HeapRegion::setup_heap_region_size(size_t initial_heap_size, size_t max_heap_size) {
  uintx region_size = G1HeapRegionSize;
  if (FLAG_IS_DEFAULT(G1HeapRegionSize)) {
    size_t average_heap_size = (initial_heap_size + max_heap_size) / 2;
    region_size = MAX2(average_heap_size / TARGET_REGION_NUMBER,
                       (uintx) MIN_REGION_SIZE);
  }
  int region_size_log = log2_long((jlong) region_size);
  // Recalculate the region size to make sure it&#39;s a power of
  // 2\. This means that region_size is the largest power of 2 that&#39;s
  // &lt;= what we&#39;ve calculated so far.
  region_size = ((uintx)1 &lt;&lt; region_size_log);
  // Now make sure that we don&#39;t go over or under our limits.
  if (region_size &lt; MIN_REGION_SIZE) {
    region_size = MIN_REGION_SIZE;
  } else if (region_size &gt; MAX_REGION_SIZE) {
    region_size = MAX_REGION_SIZE;
  }
}

</code></pre>

<h3 id="toc_3">SATB</h3>

<p>全称是Snapshot-At-The-Beginning，由字面理解，是GC开始时活着的对象的一个快照。它是通过Root Tracing得到的，作用是维持并发GC的正确性。<br/>
那么它是怎么维持并发GC的正确性的呢？根据三色标记算法，我们知道对象存在三种状态：</p>

<ul>
<li>  白：对象没有被标记到，标记阶段结束后，会被当做垃圾回收掉。</li>
<li>  灰：对象被标记了，但是它的field还没有被标记或标记完。</li>
<li>  黑：对象被标记了，且它的所有field也被标记完了。</li>
</ul>

<p>由于并发阶段的存在，Mutator和Garbage Collector线程同时对对象进行修改，就会出现白对象漏标的情况，这种情况发生的前提是：</p>

<ul>
<li>  Mutator赋予一个黑对象该白对象的引用。</li>
<li>  Mutator删除了所有从灰对象到该白对象的直接或者间接引用。</li>
</ul>

<p>对于第一个条件，在并发标记阶段，如果该白对象是new出来的，并没有被灰对象持有，那么它会不会被漏标呢？Region中有两个top-at-mark-start（TAMS）指针，分别为prevTAMS和nextTAMS。在TAMS以上的对象是新分配的，这是一种隐式的标记。对于在GC时已经存在的白对象，如果它是活着的，它必然会被另一个对象引用，即条件二中的灰对象。如果灰对象到白对象的直接引用或者间接引用被替换了，或者删除了，白对象就会被漏标，从而导致被回收掉，这是非常严重的错误，所以SATB破坏了第二个条件。也就是说，一个对象的引用被替换时，可以通过write barrier 将旧引用记录下来。</p>

<pre><code>//  share/vm/gc_implementation/g1/g1SATBCardTableModRefBS.hpp
// This notes that we don&#39;t need to access any BarrierSet data
// structures, so this can be called from a static context.
template &lt;class T&gt; static void write_ref_field_pre_static(T* field, oop newVal) {
  T heap_oop = oopDesc::load_heap_oop(field);
  if (!oopDesc::is_null(heap_oop)) {
    enqueue(oopDesc::decode_heap_oop(heap_oop));
  }
}
// share/vm/gc_implementation/g1/g1SATBCardTableModRefBS.cpp
void G1SATBCardTableModRefBS::enqueue(oop pre_val) {
  // Nulls should have been already filtered.
  assert(pre_val-&gt;is_oop(true), &quot;Error&quot;);
  if (!JavaThread::satb_mark_queue_set().is_active()) return;
  Thread* thr = Thread::current();
  if (thr-&gt;is_Java_thread()) {
    JavaThread* jt = (JavaThread*)thr;
    jt-&gt;satb_mark_queue().enqueue(pre_val);
  } else {
    MutexLockerEx x(Shared_SATB_Q_lock, Mutex::_no_safepoint_check_flag);
    JavaThread::satb_mark_queue_set().shared_satb_queue()-&gt;enqueue(pre_val);
  }
}

</code></pre>

<p>SATB也是有副作用的，如果被替换的白对象就是要被收集的垃圾，这次的标记会让它躲过GC，这就是float garbage。因为SATB的做法精度比较低，所以造成的float garbage也会比较多。</p>

<h4 id="toc_4">RSet</h4>

<p>全称是Remembered Set，是辅助GC过程的一种结构，典型的空间换时间工具，和Card Table有些类似。还有一种数据结构也是辅助GC的：Collection Set（CSet），它记录了GC要收集的Region集合，集合里的Region可以是任意年代的。在GC的时候，对于old-&gt;young和old-&gt;old的跨代对象引用，只要扫描对应的CSet中的RSet即可。<br/>
逻辑上说每个Region都有一个RSet，RSet记录了其他Region中的对象引用本Region中对象的关系，属于points-into结构（谁引用了我的对象）。而Card Table则是一种points-out（我引用了谁的对象）的结构，每个Card 覆盖一定范围的Heap（一般为512Bytes）。G1的RSet是在Card Table的基础上实现的：每个Region会记录下别的Region有指向自己的指针，并标记这些指针分别在哪些Card的范围内。 这个RSet其实是一个Hash Table，Key是别的Region的起始地址，Value是一个集合，里面的元素是Card Table的Index。</p>

<p>下图表示了RSet、Card和Region的关系（<a href="http://www.infoq.com/articles/tuning-tips-G1-GC">出处</a>）：<br/>
<img src="https://tech.meituan.com/img/g1/remembered_sets.jpg" alt=""/></p>

<p>上图中有三个Region，每个Region被分成了多个Card，在不同Region中的Card会相互引用，Region1中的Card中的对象引用了Region2中的Card中的对象，蓝色实线表示的就是points-out的关系，而在Region2的RSet中，记录了Region1的Card，即红色虚线表示的关系，这就是points-into。<br/>
而维系RSet中的引用关系靠post-write barrier和Concurrent refinement threads来维护，操作伪代码如下（<a href="http://hllvm.group.iteye.com/group/topic/44381">出处</a>）：</p>

<pre><code>void oop_field_store(oop* field, oop new_value) {
  pre_write_barrier(field);             // pre-write barrier: for maintaining SATB invariant
  *field = new_value;                   // the actual store
  post_write_barrier(field, new_value); // post-write barrier: for tracking cross-region reference
}

</code></pre>

<p>post-write barrier记录了跨Region的引用更新，更新日志缓冲区则记录了那些包含更新引用的Cards。一旦缓冲区满了，Post-write barrier就停止服务了，会由Concurrent refinement threads处理这些缓冲区日志。<br/>
RSet究竟是怎么辅助GC的呢？在做YGC的时候，只需要选定young generation region的RSet作为根集，这些RSet记录了old-&gt;young的跨代引用，避免了扫描整个old generation。 而mixed gc的时候，old generation中记录了old-&gt;old的RSet，young-&gt;old的引用由扫描全部young generation region得到，这样也不用扫描全部old generation region。所以RSet的引入大大减少了GC的工作量。</p>

<h3 id="toc_5">Pause Prediction Model</h3>

<p>Pause Prediction Model 即停顿预测模型。它在G1中的作用是：</p>

<blockquote>
<p>G1 uses a pause prediction model to meet a user-defined pause time target and selects the number of regions to collect based on the specified pause time target.</p>
</blockquote>

<p>G1 GC是一个响应时间优先的GC算法，它与CMS最大的不同是，用户可以设定整个GC过程的期望停顿时间，参数-XX:MaxGCPauseMillis指定一个G1收集过程目标停顿时间，默认值200ms，不过它不是硬性条件，只是期望值。那么G1怎么满足用户的期望呢？就需要这个停顿预测模型了。G1根据这个模型统计计算出来的历史数据来预测本次收集需要选择的Region数量，从而尽量满足用户设定的目标停顿时间。<br/>
停顿预测模型是以衰减标准偏差为理论基础实现的：</p>

<pre><code>//  share/vm/gc_implementation/g1/g1CollectorPolicy.hpp
double get_new_prediction(TruncatedSeq* seq) {
    return MAX2(seq-&gt;davg() + sigma() * seq-&gt;dsd(),
                seq-&gt;davg() * confidence_factor(seq-&gt;num()));
}

</code></pre>

<p>在这个预测计算公式中：davg表示衰减均值，sigma()返回一个系数，表示信赖度，dsd表示衰减标准偏差，confidence_factor表示可信度相关系数。而方法的参数TruncateSeq，顾名思义，是一个截断的序列，它只跟踪了序列中的最新的n个元素。</p>

<p>在G1 GC过程中，每个可测量的步骤花费的时间都会记录到TruncateSeq（继承了AbsSeq）中，用来计算衰减均值、衰减变量，衰减标准偏差等：</p>

<pre><code>// src/share/vm/utilities/numberSeq.cpp

void AbsSeq::add(double val) {
  if (_num == 0) {
    // if the sequence is empty, the davg is the same as the value
    _davg = val;
    // and the variance is 0
    _dvariance = 0.0;
  } else {
    // otherwise, calculate both
    _davg = (1.0 - _alpha) * val + _alpha * _davg;
    double diff = val - _davg;
    _dvariance = (1.0 - _alpha) * diff * diff + _alpha * _dvariance;
  }
}

</code></pre>

<p>比如要预测一次GC过程中，RSet的更新时间，这个操作主要是将Dirty Card加入到RSet中，具体原理参考前面的RSet。每个Dirty Card的时间花费通过_cost_per_card_ms_seq来记录，具体预测代码如下：</p>

<pre><code>//  share/vm/gc_implementation/g1/g1CollectorPolicy.hpp

 double predict_rs_update_time_ms(size_t pending_cards) {
    return (double) pending_cards * predict_cost_per_card_ms();
 }
 double predict_cost_per_card_ms() {
    return get_new_prediction(_cost_per_card_ms_seq);
 }

</code></pre>

<p>get_new_prediction就是我们开头说的方法，现在大家应该基本明白停顿预测模型的实现原理了。</p>

<h2 id="toc_6">GC过程</h2>

<p>讲完了一些基本概念，下面我们就来看看G1的GC过程是怎样的。</p>

<h3 id="toc_7">G1 GC模式</h3>

<p>G1提供了两种GC模式，Young GC和Mixed GC，两种都是完全Stop The World的。</p>

<ul>
<li>  Young GC：选定所有年轻代里的Region。通过控制年轻代的region个数，即年轻代内存大小，来控制young GC的时间开销。</li>
<li>  Mixed GC：选定所有年轻代里的Region，外加根据global concurrent marking统计得出收集收益高的若干老年代Region。在用户指定的开销目标范围内尽可能选择收益高的老年代Region。</li>
</ul>

<p>由上面的描述可知，Mixed GC不是full GC，它只能回收部分老年代的Region，如果mixed GC实在无法跟上程序分配内存的速度，导致老年代填满无法继续进行Mixed GC，就会使用serial old GC（full GC）来收集整个GC heap。所以我们可以知道，G1是不提供full GC的。</p>

<p>上文中，多次提到了global concurrent marking，它的执行过程类似CMS，但是不同的是，在G1 GC中，它主要是为Mixed GC提供标记服务的，并不是一次GC过程的一个必须环节。global concurrent marking的执行过程分为四个步骤：</p>

<ul>
<li>  初始标记（initial mark，STW）。它标记了从GC Root开始直接可达的对象。</li>
<li>  并发标记（Concurrent Marking）。这个阶段从GC Root开始对heap中的对象标记，标记线程与应用程序线程并行执行，并且收集各个Region的存活对象信息。</li>
<li>  最终标记（Remark，STW）。标记那些在并发标记阶段发生变化的对象，将被回收。</li>
<li>  清除垃圾（Cleanup）。清除空Region（没有存活对象的），加入到free list。</li>
</ul>

<p>第一阶段initial mark是共用了Young GC的暂停，这是因为他们可以复用root scan操作，所以可以说global concurrent marking是伴随Young GC而发生的。第四阶段Cleanup只是回收了没有存活对象的Region，所以它并不需要STW。</p>

<p>Young GC发生的时机大家都知道，那什么时候发生Mixed GC呢？其实是由一些参数控制着的，另外也控制着哪些老年代Region会被选入CSet。</p>

<ul>
<li>  G1HeapWastePercent：在global concurrent marking结束之后，我们可以知道old gen regions中有多少空间要被回收，在每次YGC之后和再次发生Mixed GC之前，会检查垃圾占比是否达到此参数，只有达到了，下次才会发生Mixed GC。</li>
<li>  G1MixedGCLiveThresholdPercent：old generation region中的存活对象的占比，只有在此参数之下，才会被选入CSet。</li>
<li>  G1MixedGCCountTarget：一次global concurrent marking之后，最多执行Mixed GC的次数。</li>
<li>  G1OldCSetRegionThresholdPercent：一次Mixed GC中能被选入CSet的最多old generation region数量。</li>
</ul>

<p>除了以上的参数，G1 GC相关的其他主要的参数有：</p>

<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
</tr>
</thead>

<tbody>
<tr>
<td>-XX:G1HeapRegionSize=n</td>
<td>设置Region大小，并非最终值</td>
</tr>
<tr>
<td>-XX:MaxGCPauseMillis</td>
<td>设置G1收集过程目标时间，默认值200ms，不是硬性条件</td>
</tr>
<tr>
<td>-XX:G1NewSizePercent</td>
<td>新生代最小值，默认值5%</td>
</tr>
<tr>
<td>-XX:G1MaxNewSizePercent</td>
<td>新生代最大值，默认值60%</td>
</tr>
<tr>
<td>-XX:ParallelGCThreads</td>
<td>STW期间，并行GC线程数</td>
</tr>
<tr>
<td>-XX:ConcGCThreads=n</td>
<td>并发标记阶段，并行执行的线程数</td>
</tr>
<tr>
<td>-XX:InitiatingHeapOccupancyPercent</td>
<td>设置触发标记周期的 Java 堆占用率阈值。默认值是45%。这里的java堆占比指的是non_young_capacity_bytes，包括old+humongous</td>
</tr>
</tbody>
</table>

<h3 id="toc_8">GC日志</h3>

<p>G1收集器的日志与其他收集器有很大不同，源于G1独立的体系架构和数据结构，下面这两段日志来源于美团点评的CRM系统线上生产环境。</p>

<h4 id="toc_9">Young GC日志</h4>

<p>我们先来看看Young GC的日志：</p>

<pre><code>{Heap before GC invocations=12 (full 1):
 garbage-first heap   total 3145728K, used 336645K [0x0000000700000000, 0x00000007c0000000, 0x00000007c0000000)
  region size 1024K, 172 young (176128K), 13 survivors (13312K)
 Metaspace       used 29944K, capacity 30196K, committed 30464K, reserved 1077248K
  class space    used 3391K, capacity 3480K, committed 3584K, reserved 1048576K
2014-11-14T17:57:23.654+0800: 27.884: [GC pause (G1 Evacuation Pause) (young)
Desired survivor size 11534336 bytes, new threshold 15 (max 15)
- age   1:    5011600 bytes,    5011600 total
 27.884: [G1Ergonomics (CSet Construction) start choosing CSet, _pending_cards: 1461, predicted base time: 35.25 ms, remaining time: 64.75 ms, target pause time: 100.00 ms]
 27.884: [G1Ergonomics (CSet Construction) add young regions to CSet, eden: 159 regions, survivors: 13 regions, predicted young region time: 44.09 ms]
 27.884: [G1Ergonomics (CSet Construction) finish choosing CSet, eden: 159 regions, survivors: 13 regions, old: 0 regions, predicted pause time: 79.34 ms, target pause time: 100.00 ms]
, 0.0158389 secs]
   [Parallel Time: 8.1 ms, GC Workers: 4]
      [GC Worker Start (ms): Min: 27884.5, Avg: 27884.5, Max: 27884.5, Diff: 0.1]
      [Ext Root Scanning (ms): Min: 0.4, Avg: 0.8, Max: 1.2, Diff: 0.8, Sum: 3.1]
      [Update RS (ms): Min: 0.0, Avg: 0.3, Max: 0.6, Diff: 0.6, Sum: 1.4]
         [Processed Buffers: Min: 0, Avg: 2.8, Max: 5, Diff: 5, Sum: 11]
      [Scan RS (ms): Min: 0.0, Avg: 0.1, Max: 0.1, Diff: 0.1, Sum: 0.3]
      [Code Root Scanning (ms): Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 0.6]
      [Object Copy (ms): Min: 4.9, Avg: 5.1, Max: 5.2, Diff: 0.3, Sum: 20.4]
      [Termination (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]
      [GC Worker Other (ms): Min: 0.0, Avg: 0.4, Max: 1.3, Diff: 1.3, Sum: 1.4]
      [GC Worker Total (ms): Min: 6.4, Avg: 6.8, Max: 7.8, Diff: 1.4, Sum: 27.2]
      [GC Worker End (ms): Min: 27891.0, Avg: 27891.3, Max: 27892.3, Diff: 1.3]
   [Code Root Fixup: 0.5 ms]
   [Code Root Migration: 1.3 ms]
   [Code Root Purge: 0.0 ms]
   [Clear CT: 0.2 ms]
   [Other: 5.8 ms]
      [Choose CSet: 0.0 ms]
      [Ref Proc: 5.0 ms]
      [Ref Enq: 0.1 ms]
      [Redirty Cards: 0.0 ms]
      [Free CSet: 0.2 ms]
   [Eden: 159.0M(159.0M)-&gt;0.0B(301.0M) Survivors: 13.0M-&gt;11.0M Heap: 328.8M(3072.0M)-&gt;167.3M(3072.0M)]
Heap after GC invocations=13 (full 1):
 garbage-first heap   total 3145728K, used 171269K [0x0000000700000000, 0x00000007c0000000, 0x00000007c0000000)
  region size 1024K, 11 young (11264K), 11 survivors (11264K)
 Metaspace       used 29944K, capacity 30196K, committed 30464K, reserved 1077248K
  class space    used 3391K, capacity 3480K, committed 3584K, reserved 1048576K
}
 [Times: user=0.05 sys=0.01, real=0.02 secs]

</code></pre>

<p>每个过程的作用如下：</p>

<ul>
<li>  garbage-first heap total 3145728K, used 336645K [0x0000000700000000, 0x00000007c0000000, 0x00000007c0000000)
这行表示使用了G1垃圾收集器，total heap 3145728K，使用了336645K。</li>
<li>  region size 1024K, 172 young (176128K), 13 survivors (13312K)
Region大小为1M，青年代占用了172个（共176128K），幸存区占用了13个（共13312K）。</li>
<li>  Metaspace used 29944K, capacity 30196K, committed 30464K, reserved 1077248K
class space used 3391K, capacity 3480K, committed 3584K, reserved 1048576K
java 8的新特性，去掉永久区，添加了元数据区，这块不是本文重点，不再赘述。需要注意的是，之所以有committed和reserved，是因为没有设置MetaspaceSize=MaxMetaspaceSize。</li>
<li>  [GC pause (G1 Evacuation Pause) (young)
GC原因，新生代minor GC。</li>
<li>  [G1Ergonomics (CSet Construction) start choosing CSet, <u>pending</u>cards: 1461, predicted base time: 35.25 ms, remaining time: 64.75 ms, target pause time: 100.00 ms]
发生minor GC和full GC时，所有相关region都是要回收的。而发生并发GC时，会根据目标停顿时间动态选择部分垃圾对并多的Region回收，这一步就是选择Region。_pending_cards是关于RSet的Card Table。predicted base time是预测的扫描card table时间。</li>
<li>  [G1Ergonomics (CSet Construction) add young regions to CSet, eden: 159 regions, survivors: 13 regions, predicted young region time: 44.09 ms]
这一步是添加Region到collection set，新生代一共159个Region，13个幸存区Region，这也和之前的（172 young (176128K), 13 survivors (13312K)）吻合。预计收集时间是44.09 ms。</li>
<li>  [G1Ergonomics (CSet Construction) finish choosing CSet, eden: 159 regions, survivors: 13 regions, old: 0 regions, predicted pause time: 79.34 ms, target pause time: 100.00 ms]
这一步是对上面两步的总结。预计总收集时间79.34ms。</li>
<li>  [Parallel Time: 8.1 ms, GC Workers: 4]
由于收集过程是多线程并行（并发）进行，这里是4个线程，总共耗时8.1ms（wall clock time）</li>
<li>  [GC Worker Start (ms): Min: 27884.5, Avg: 27884.5, Max: 27884.5, Diff: 0.1]
收集线程开始的时间，使用的是相对时间，Min是最早开始时间，Avg是平均开始时间，Max是最晚开始时间，Diff是Max-Min（此处的0.1貌似有问题）</li>
<li>  [Ext Root Scanning (ms): Min: 0.4, Avg: 0.8, Max: 1.2, Diff: 0.8, Sum: 3.1]
扫描Roots花费的时间，Sum表示total cpu time，下同。</li>
<li>  [Update RS (ms): Min: 0.0, Avg: 0.3, Max: 0.6, Diff: 0.6, Sum: 1.4] [Processed Buffers: Min: 0, Avg: 2.8, Max: 5, Diff: 5, Sum: 11]
Update RS (ms)是每个线程花费在更新Remembered Set上的时间。</li>
<li>  [Scan RS (ms): Min: 0.0, Avg: 0.1, Max: 0.1, Diff: 0.1, Sum: 0.3]
扫描CS中的region对应的RSet，因为RSet是points-into，所以这样实现避免了扫描old generadion region，但是会产生float garbage。</li>
<li>  [Code Root Scanning (ms): Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 0.6]
扫描code root耗时。code root指的是经过JIT编译后的代码里，引用了heap中的对象。引用关系保存在RSet中。</li>
<li>  [Object Copy (ms): Min: 4.9, Avg: 5.1, Max: 5.2, Diff: 0.3, Sum: 20.4]
拷贝活的对象到新region的耗时。</li>
<li>  [Termination (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]
线程结束，在结束前，它会检查其他线程是否还有未扫描完的引用，如果有，则&quot;偷&quot;过来，完成后再申请结束，这个时间是线程之前互相同步所花费的时间。</li>
<li>  [GC Worker Other (ms): Min: 0.0, Avg: 0.4, Max: 1.3, Diff: 1.3, Sum: 1.4]
花费在其他工作上（未列出）的时间。</li>
<li>  [GC Worker Total (ms): Min: 6.4, Avg: 6.8, Max: 7.8, Diff: 1.4, Sum: 27.2]
每个线程花费的时间和。</li>
<li>  [GC Worker End (ms): Min: 27891.0, Avg: 27891.3, Max: 27892.3, Diff: 1.3]
每个线程结束的时间。</li>
<li>  [Code Root Fixup: 0.5 ms]
用来将code root修正到正确的evacuate之后的对象位置所花费的时间。</li>
<li>  [Code Root Migration: 1.3 ms]
更新code root 引用的耗时，code root中的引用因为对象的evacuation而需要更新。</li>
<li>  [Code Root Purge: 0.0 ms]
清除code root的耗时，code root中的引用已经失效，不再指向Region中的对象，所以需要被清除。</li>
<li>  [Clear CT: 0.2 ms]
清除card table的耗时。</li>
<li>  [Other: 5.8 ms]
[Choose CSet: 0.0 ms]
[Ref Proc: 5.0 ms]
[Ref Enq: 0.1 ms]
[Redirty Cards: 0.0 ms]
[Free CSet: 0.2 ms]
其他事项共耗时5.8ms，其他事项包括选择CSet，处理已用对象，引用入ReferenceQueues，释放CSet中的region到free list。</li>
<li>  [Eden: 159.0M(159.0M)-&gt;0.0B(301.0M) Survivors: 13.0M-&gt;11.0M Heap: 328.8M(3072.0M)-&gt;167.3M(3072.0M)]
新生代清空了，下次扩容到301MB。</li>
</ul>

<h4 id="toc_10">global concurrent marking 日志</h4>

<p>对于global concurrent marking过程，它的日志如下所示：</p>

<pre><code>66955.252: [G1Ergonomics (Concurrent Cycles) request concurrent cycle initiation, reason: occupancy higher than threshold, occupancy: 1449132032 bytes, allocation request: 579608 bytes, threshold: 1449
551430 bytes (45.00 %), source: concurrent humongous allocation]
2014-12-10T11:13:09.532+0800: 66955.252: Application time: 2.5750418 seconds
 66955.259: [G1Ergonomics (Concurrent Cycles) request concurrent cycle initiation, reason: requested by GC cause, GC cause: G1 Humongous Allocation]
{Heap before GC invocations=1874 (full 4):
 garbage-first heap   total 3145728K, used 1281786K [0x0000000700000000, 0x00000007c0000000, 0x00000007c0000000)
  region size 1024K, 171 young (175104K), 27 survivors (27648K)
 Metaspace       used 116681K, capacity 137645K, committed 137984K, reserved 1171456K
  class space    used 13082K, capacity 16290K, committed 16384K, reserved 1048576K
 66955.259: [G1Ergonomics (Concurrent Cycles) initiate concurrent cycle, reason: concurrent cycle initiation requested]
2014-12-10T11:13:09.539+0800: 66955.259: [GC pause (G1 Humongous Allocation) (young) (initial-mark)
…….
2014-12-10T11:13:09.597+0800: 66955.317: [GC concurrent-root-region-scan-start]
2014-12-10T11:13:09.597+0800: 66955.318: Total time for which application threads were stopped: 0.0655753 seconds
2014-12-10T11:13:09.610+0800: 66955.330: Application time: 0.0127071 seconds
2014-12-10T11:13:09.614+0800: 66955.335: Total time for which application threads were stopped: 0.0043882 seconds
2014-12-10T11:13:09.625+0800: 66955.346: [GC concurrent-root-region-scan-end, 0.0281351 secs]
2014-12-10T11:13:09.625+0800: 66955.346: [GC concurrent-mark-start]
2014-12-10T11:13:09.645+0800: 66955.365: Application time: 0.0306801 seconds
2014-12-10T11:13:09.651+0800: 66955.371: Total time for which application threads were stopped: 0.0061326 seconds
2014-12-10T11:13:10.212+0800: 66955.933: [GC concurrent-mark-end, 0.5871129 secs]
2014-12-10T11:13:10.212+0800: 66955.933: Application time: 0.5613792 seconds
2014-12-10T11:13:10.215+0800: 66955.935: [GC remark 66955.936: [GC ref-proc, 0.0235275 secs], 0.0320865 secs]
 [Times: user=0.05 sys=0.00, real=0.03 secs]
2014-12-10T11:13:10.247+0800: 66955.968: Total time for which application threads were stopped: 0.0350098 seconds
2014-12-10T11:13:10.248+0800: 66955.968: Application time: 0.0001691 seconds
2014-12-10T11:13:10.250+0800: 66955.970: [GC cleanup 1178M-&gt;632M(3072M), 0.0060632 secs]
 [Times: user=0.02 sys=0.00, real=0.01 secs]
2014-12-10T11:13:10.256+0800: 66955.977: Total time for which application threads were stopped: 0.0088462 seconds
2014-12-10T11:13:10.257+0800: 66955.977: [GC concurrent-cleanup-start]
2014-12-10T11:13:10.259+0800: 66955.979: [GC concurrent-cleanup-end, 0.0024743 secs

</code></pre>

<p>这次发生global concurrent marking的原因是：humongous allocation，上面提过在巨大对象分配之前，会检测到old generation 使用占比是否超过了 initiating heap occupancy percent（45%），因为<br/>
1449132032(used)+ 579608(allocation request:) &gt; 1449551430(threshold)，所以触发了本次global concurrent marking。对于具体执行过程，上面的表格已经详细讲解了。值得注意的是上文中所说的initial mark往往伴随着一次YGC，在日志中也有体现：GC pause (G1 Humongous Allocation) (young) (initial-mark)。</p>

<h2 id="toc_11">后记</h2>

<p>因为篇幅的关系，也受限于能力水平，本文只是简单了介绍了G1 GC的基本原理，很多细节没有涉及到，所以说只能算是为研究和使用它的同学打开了一扇门。一个日本人专门写了一本书《<a href="http://tatsu-zine.com/books/g1gc">徹底解剖「G1GC」 アルゴリズ</a>》详细的介绍了G1 GC，这本书也被作者放到了GitHub上，详见参考文献5。另外，莫枢在这方面也研究的比较多，读者可以去<a href="http://hllvm.group.iteye.com/">高级语言虚拟机论坛</a>向他请教，本文的很多内容也是我在此论坛上请教过后整理的。总而言之，G1是一款非常优秀的垃圾收集器，尽管还有些不完美（预测模型还不够智能），但是希望有更多的同学来使用它，研究它，提出好的建议，让它变的更加完善。</p>

<h2 id="toc_12">参考文献</h2>

<ol>
<li> <a href="http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/G1GettingStarted/index.html">Getting Started with the G1 Garbage Collector</a></li>
<li> <a href="http://hllvm.group.iteye.com/group/topic/44381">请教G1算法的原理</a></li>
<li> <a href="http://hllvm.group.iteye.com/group/topic/44529">关于incremental update与SATB的一点理解</a></li>
<li> <a href="http://www.infoq.com/articles/tuning-tips-G1-GC">Tips for Tuning the Garbage First Garbage Collector</a></li>
<li> <a href="https://github.com/authorNari/g1gc-impl-book">g1gc-impl-book</a></li>
<li> <a href="http://www.oracle.com/technetwork/cn/articles/java/g1gc-1984535-zhs.html">垃圾优先型垃圾回收器调优</a></li>
<li> <a href="https://blogs.oracle.com/poonam/entry/understanding_g1_gc_logs">Understanding G1 GC Logs</a></li>
<li> <a href="http://www.infoq.com/articles/G1-One-Garbage-Collector-To-Rule-Them-All">G1: One Garbage Collector To Rule Them All</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【转】理解Heap Profling名词-Shallow和Retained Sizes]]></title>
    <link href="http://www.blacklight.xin/15442380789391.html"/>
    <updated>2018-12-08T11:01:18+08:00</updated>
    <id>http://www.blacklight.xin/15442380789391.html</id>
    <content type="html"><![CDATA[
<p>所有包含Heap Profling功能的工具（MAT, Yourkit, JProfiler, TPTP等）都会使用到两个名词，一个是Shallow Size，另一个是 Retained Size.这是两个在平时不太常见的名词，本文会对这两个名词做一个详细的解释。</p>

<span id="more"></span><!-- more -->

<p>Shallow Size<br/>
    对象自身占用的内存大小，不包括它引用的对象。<br/>
    针对非数组类型的对象，它的大小就是对象与它所有的成员变量大小的总和。当然这里面还会包括一些<a href="http://lib.csdn.net/base/java" title="Java 知识库">Java</a>语言特性的数据存储单元。<br/>
    针对数组类型的对象，它的大小是数组元素对象的大小总和。</p>

<p>Retained Size<br/>
    Retained Size=当前对象大小+当前对象可直接或间接引用到的对象的大小总和。(间接引用的含义：A-&gt;B-&gt;C, C就是间接引用)<br/>
    换句话说，Retained Size就是当前对象被GC后，从Heap上总共能释放掉的内存。不过，释放的时候还要排除被GC Roots直接或间接引用的对象。他们暂时不会被被当做Garbage。</p>

<p><img src="http://img.my.csdn.net/uploads/201301/15/1358243519_6794.png" alt=""/></p>

<p>上图中，GC Roots直接引用了A和B两个对象。</p>

<p>A对象的Retained Size=A对象的Shallow Size</p>

<p>B对象的Retained Size=B对象的Shallow Size + C对象的Shallow Size</p>

<p>这里不包括D对象，因为D对象被GC Roots直接引用。<br/>
如果GC Roots不引用D对象呢？</p>

<p><img src="http://img.my.csdn.net/uploads/201301/15/1358243533_9844.png" alt=""/></p>

<p>此时,B对象的Retained Size=B对象的Shallow Size + C对象的Shallow Size + D对象的Shallow Size</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【转】shell 基础二十篇]]></title>
    <link href="http://www.blacklight.xin/15442379465528.html"/>
    <updated>2018-12-08T10:59:06+08:00</updated>
    <id>http://www.blacklight.xin/15442379465528.html</id>
    <content type="html"><![CDATA[
<p><a href="http://bbs.chinaunix%5B.NET%5D(http://lib.csdn.net/base/dotnet">http://bbs.chinaunix[.NET](http://lib.csdn.net/base/dotnet</a> “.NET知识库”)/forum.PHP?mod=viewthread&amp;tid=452942</p>

<p>编者按：由 wingger 整理的　shell基础十二篇 以及L_kernel补充的第十三——二十篇，涉及shell　编程及使用的各个方面，又附有大量的例子，极适合初学者系统学习。如果配合網中人的shell 十三問? ，效果更加明显。</p>

<p>这里是其中的第十章 sed. 其他各章可察看相应的 link. 　</p>

<span id="more"></span><!-- more -->

<p>shell基础1：文件安全与权限<br/>
<a href="http://bbs.chinaunix.net/thread-434579-1-1.html">http://bbs.chinaunix.net/thread-434579-1-1.html</a></p>

<p>附：Linux的用户和用户组管理<br/>
<a href="http://bbs.chinaunix.net/thread-438660-1-1.html">http://bbs.chinaunix.net/thread-438660-1-1.html</a></p>

<p>shell基础二：查找技巧,find及xargs的使用<br/>
<a href="http://bbs.chinaunix.net/thread-441883-1-1.html">http://bbs.chinaunix.net/thread-441883-1-1.html</a></p>

<p>shell基础三和四：后台(crontab,at,&amp;,nohup)及(*,?,[]等)<br/>
<a href="http://bbs.chinaunix.net/thread-442596-1-1.html">http://bbs.chinaunix.net/thread-442596-1-1.html</a></p>

<p>shell基础五：输入和输出(echo,read,cat,管道,tee,重定向等)<br/>
<a href="http://bbs.chinaunix.net/thread-444209-1-1.html">http://bbs.chinaunix.net/thread-444209-1-1.html</a></p>

<p>shell基础六七：命令执行顺序(||及&amp;&amp;，{}及())和正则表?<br/>
<a href="http://bbs.chinaunix.net/thread-445229-1-1.html">http://bbs.chinaunix.net/thread-445229-1-1.html</a></p>

<p>shell基础八：文本过滤工具（grep）<br/>
<a href="http://bbs.chinaunix.net/thread-446683-1-1.html">http://bbs.chinaunix.net/thread-446683-1-1.html</a></p>

<p>shell基础九：awk<br/>
<a href="http://bbs.chinaunix.net/thread-448687-1-1.html">http://bbs.chinaunix.net/thread-448687-1-1.html</a></p>

<p>shell基础十：sed<br/>
<a href="http://bbs.chinaunix.net/thread-452942-1-1.html">http://bbs.chinaunix.net/thread-452942-1-1.html</a></p>

<p>shell基础11：文件分类、合并和分割(sort,uniq,join,cut,paste,split)<br/>
<a href="http://bbs.chinaunix.net/thread-457730-1-1.html">http://bbs.chinaunix.net/thread-457730-1-1.html</a></p>

<p>shell基础十二:tr<br/>
<a href="http://bbs.chinaunix.net/thread-459099-1-1.html">http://bbs.chinaunix.net/thread-459099-1-1.html</a></p>

<p>感谢L_kernel补充的第十三——二十篇（cjaizss添加）<br/>
shell基础第十三篇-登录环境<br/>
<a href="http://bbs.chinaunix.net/thread-1820174-1-1.html">http://bbs.chinaunix.net/thread-1820174-1-1.html</a></p>

<p>shell基础第十四篇-环境和shell变量<br/>
<a href="http://bbs.chinaunix.net/thread-1820194-1-1.html">http://bbs.chinaunix.net/thread-1820194-1-1.html</a></p>

<p>shell基础第十五篇-引号<br/>
<a href="http://bbs.chinaunix.net/thread-1820220-1-1.html">http://bbs.chinaunix.net/thread-1820220-1-1.html</a></p>

<p>shell基础第十六篇-shell脚本介绍<br/>
<a href="http://bbs.chinaunix.net/thread-1820301-1-1.html">http://bbs.chinaunix.net/thread-1820301-1-1.html</a></p>

<p>shell基础学习第十七篇-条件测试<br/>
<a href="http://bbs.chinaunix.net/thread-1820304-1-1.html">http://bbs.chinaunix.net/thread-1820304-1-1.html</a></p>

<p>shell基础第十八篇-控制流结构<br/>
<a href="http://bbs.chinaunix.net/thread-1820329-1-1.html">http://bbs.chinaunix.net/thread-1820329-1-1.html</a></p>

<p>shell学习基础第十九篇-shell函数<br/>
<a href="http://bbs.chinaunix.net/thread-1820454-1-1.html">http://bbs.chinaunix.net/thread-1820454-1-1.html</a></p>

<p>shell基础学习第二十篇-向脚本传递参数<br/>
<a href="http://bbs.chinaunix.net/thread-1823335-1-1.html">http://bbs.chinaunix.net/thread-1823335-1-1.html</a></p>

<p>其它我的笔记都在我的BLOG上，呵呵，有兴趣的可以去看看</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[mysql 分表分库【转】]]></title>
    <link href="http://www.blacklight.xin/15442377600364.html"/>
    <updated>2018-12-08T10:56:00+08:00</updated>
    <id>http://www.blacklight.xin/15442377600364.html</id>
    <content type="html"><![CDATA[
<p>原文地址：<a href="http://wentao365.iteye.com/blog/1740874">http://wentao365.iteye.com/blog/1740874</a></p>

<h1 id="toc_0">单库单表</h1>

<p>单库单表是最常见的数据库设计，例如，有一张用户(user)表放在数据库db中，所有的用户都可以在db库中的user表中查到。 </p>

<span id="more"></span><!-- more -->

<h1 id="toc_1">单库多表</h1>

<p>随着用户数量的增加，user表的数据量会越来越大，当数据量达到一定程度的时候对user表的查询会渐渐的变慢，从而影响整个DB的性能。如果使用mysql, 还有一个更严重的问题是，当需要添加一列的时候，mysql会锁表，期间所有的读写操作只能等待。 </p>

<p>可以通过某种方式将user进行水平的切分，产生两个表结构完全一样的user_0000,user_0001等表，user_0000 + user_0001 + …的数据刚好是一份完整的数据。 </p>

<h1 id="toc_2">多库多表</h1>

<p>随着数据量增加也许单台DB的存储空间不够，随着查询量的增加单台数据库服务器已经没办法支撑。这个时候可以再对数据库进行水平区分。 </p>

<h1 id="toc_3">分库分表规则</h1>

<p>设计表的时候需要确定此表按照什么样的规则进行分库分表。例如，当有新用户时，程序得确定将此用户信息添加到哪个表中；同理，当登录的时候我们得通过用户的账号找到数据库中对应的记录，所有的这些都需要按照某一规则进行。 </p>

<h1 id="toc_4">路由</h1>

<p>通过分库分表规则查找到对应的表和库的过程。如分库分表的规则是user_id mod 4的方式，当用户新注册了一个账号，账号id的123,我们可以通过id mod 4的方式确定此账号应该保存到User_0003表中。当用户123登录的时候，我们通过123 mod 4后确定记录在User_0003中。 <br/>
分库分表产生的问题，及注意事项 </p>

<ul>
<li>分库分表维度的问题 </li>
</ul>

<p>假如用户购买了商品,需要将交易记录保存取来，如果按照用户的纬度分表，则每个用户的交易记录都保存在同一表中，所以很快很方便的查找到某用户的购买情况，但是某商品被购买的情况则很有可能分布在多张表中，查找起来比较麻烦。反之，按照商品维度分表，可以很方便的查找到此商品的购买情况，但要查找到买人的交易记录比较麻烦。 </p>

<p>所以常见的解决方式有： </p>

<pre><code> a.通过扫表的方式解决，此方法基本不可能，效率太低了。 

 b.记录两份数据，一份按照用户纬度分表，一份按照商品维度分表。 

 c.通过搜索引擎解决，但如果实时性要求很高，又得关系到实时搜索。 
</code></pre>

<ul>
<li>联合查询的问题 </li>
</ul>

<p>联合查询基本不可能，因为关联的表有可能不在同一数据库中。 </p>

<ul>
<li>避免跨库事务 </li>
</ul>

<p>避免在一个事务中修改db0中的表的时候同时修改db1中的表，一个是操作起来更复杂，效率也会有一定影响。 </p>

<ul>
<li>尽量把同一组数据放到同一DB服务器上 </li>
</ul>

<p>例如将卖家a的商品和交易信息都放到db0中，当db1挂了的时候，卖家a相关的东西可以正常使用。也就是说避免数据库中的数据依赖另一数据库中的数据。 </p>

<ul>
<li>一主多备 </li>
</ul>

<p>在实际的应用中，绝大部分情况都是读远大于写。Mysql提供了读写分离的机制，所有的写操作都必须对应到Master，读操作可以在Master和Slave机器上进行，Slave与Master的结构完全一样，一个Master可以有多个Slave,甚至Slave下还可以挂Slave,通过此方式可以有效的提高DB集群的QPS.                                                       </p>

<p>所有的写操作都是先在Master上操作，然后同步更新到Slave上，所以从Master同步到Slave机器有一定的延迟，当系统很繁忙的时候，延迟问题会更加严重，Slave机器数量的增加也会使这个问题更加严重。 </p>

<p>此外，可以看出Master是集群的瓶颈，当写操作过多，会严重影响到Master的稳定性，如果Master挂掉，整个集群都将不能正常工作。 </p>

<p>所以，<br/>
   1. 当读压力很大的时候，可以考虑添加Slave机器的分式解决，但是当Slave机器达到一定的数量就得考虑分库了。 <br/>
   2. 当写压力很大的时候，就必须得进行分库操作。 </p>

<hr/>

<blockquote>
<p>MySQL使用为什么要分库分表 <br/>
可以用说用到MySQL的地方,只要数据量一大, 马上就会遇到一个问题,要分库分表. <br/>
这里引用一个问题为什么要分库分表呢?MySQL处理不了大的表吗? <br/>
其实是可以处理的大表的.我所经历的项目中单表物理上文件大小在80G多,单表记录数在5亿以上,而且这个表 <br/>
属于一个非常核用的表:朋友关系表. </p>
</blockquote>

<p>但这种方式可以说不是一个最佳方式. 因为面临文件系统如Ext3文件系统对大于大文件处理上也有许多问题. <br/>
这个层面可以用xfs文件系统进行替换.但MySQL单表太大后有一个问题是不好解决: 表结构调整相关的操作基 <br/>
本不在可能.所以大项在使用中都会面监着分库分表的应用. </p>

<p>从Innodb本身来讲数据文件的Btree上只有两个锁, 叶子节点锁和子节点锁,可以想而知道,当发生页拆分或是添加 <br/>
新叶时都会造成表里不能写入数据. <br/>
所以分库分表还就是一个比较好的选择了. </p>

<p>那么分库分表多少合适呢? </p>

<p><strong>经测试在单表1000万条记录一下,写入读取性能是比较好的. 这样在留点buffer,那么单表全是数据字型的保持在 <br/>
800万条记录以下, 有字符型的单表保持在500万以下. <br/>
如果按 100库100表来规划,如用户业务: <br/>
500万<em>100</em>100 = 50000000万 = 5000亿记录.</strong></p>

<p>心里有一个数了,按业务做规划还是比较容易的.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[java异常的选择【转载】]]></title>
    <link href="http://www.blacklight.xin/15442373245165.html"/>
    <updated>2018-12-08T10:48:44+08:00</updated>
    <id>http://www.blacklight.xin/15442373245165.html</id>
    <content type="html"><![CDATA[
<p>曾经听到过关于老司机和新手程序员的区别，其中最大的一个区别就在于异常的处理。新手程序员总是天真得把世界想得太美好，基本上没想过会出现异常的情况，而一个经验丰富的老司机会把最坏的打算考虑进去，给出相应的解决办法，使得发生异常时对系统的影响降低到最小。对此，我深表认同。现实的情况总是复杂的，而且还有很多不怀好意的人时刻准备攻击你的系统。使用你系统的用户越多，这种潜在的风险也就越大。</p>

<p>异常处理是应对这些风险的最强有力的武器。在Java的世界里，异常有两种：受检异常(checked exception)和非受检异常（unchecked exception）。想必所有的Javaer都使用过这两种异常，但是何时使用哪个异常缺失经常困扰程序员的头疼问题。在此，我分享一下自己的看法，如果你有不同的意见，请留意探讨。</p>

<span id="more"></span><!-- more -->

<h3 id="toc_0">1-如果正常情况下会出现，那么使用Checked-Exception；反之，则使用Unchecked-Exception</h3>

<p>这条准则是我在决定使用Checked Exception还是Unchecked Exception的第一原则。如果API的使用者在正常使用的过程中都会出现异常，那么这种异常就属于Checked Exception。因为这种异常时属于程序执行流程众多分支之一，API的使用者必须意识到这种情况，并做出相应的处理。</p>

<p>举个栗子：</p>

<p>我希望向zookeeper中创建一个节点，那么这种情况就隐含了两个前提条件：</p>

<ul>
<li>  父节点已经被创建（如果有的话）</li>
<li>  本节点还未被创建</li>
</ul>

<p>那么，这个API的签名大致应该是这样：</p>

<pre><code class="language-java">void createNode(String path,byte[] data) throws FatherNodeNotExist, NodeExist;

</code></pre>

<p>API的使用者看到这个签名的定义时就会得到一个强烈的心理暗示，我需要考虑父节点不存在和本节点已存在的情况，那么他就不得不显示的去处理这两种异常。</p>

<p>有的朋友可能会争论说，我正常的情况下不会出现这种情况，因为使用这个API的前提就是先创建好父节点，而后创建本节点，那我就不用抛出两种异常了，使用者也轻松了许多。但事实真的如此吗？我们想当然的认为了使用者是<code>自己人</code>，他们会乖乖的按照我们的想法去先创建父节点，再创建本节点，如果是在一个很局限的使用场景下，每个人都说经过严格培训的，那么你可以去做这样的假设，但是我还是不推荐你这么做，因为这样设计使得系统是脆弱的，不稳定的。如果能通过系统能自己避免这些错误，为什么不呢？况且，如果你把这个API开放给第三方的使用者，那么情况会更糟糕，你根本不知道他们会怎样去使用API，这非常恐怖！</p>

<p>有时候情况会变得很复杂，<code>正常情况</code>的鉴定变得很困难，你肯定会遇到这种时候，此时就需要结合你的业务场景去权衡其中的利弊。这依赖与你的经验和对业务场景的理解，我无法给你一个绝对的建议，那样是不负责任的。</p>

<p>我再举个常见的栗子：用户修改他拥有的资源信息。在菜谱APP中给出一个接口，让用户修改他菜谱的信息。那么这里一个隐含的条件就是用户修改他自己的菜谱信息，他是无权限修改别人的菜谱信息的。那么这个API的签名可能是这样的：</p>

<pre><code class="language-java">void updateMenu(long menuId,long uid,String title,String description...);

</code></pre>

<p>如果用户尝试去修改不属于他的菜谱呢？我们是否需要throws UserPermissionException之类受检异常？我认为是不需要的。判断是这属于正常情况吗？我认为这不算是正常情况。<br/>
正常情况下，客户端调用修改信息的接口，那么menuId一定是属于这个用户的。如果出现这种情况，要么是你系统设计的就有问题，要么就是不怀好意的人在破坏你的系统。前者需要重新设计我们的系统，而后者我们更不用关系，直接抛出一个RuntimeException就可以，因为他不算正常用户。</p>

<h3 id="toc_1">2. 调用者中能从异常中恢复的，推荐使用受检异常；反之，则使用非受检异常</h3>

<p>注意这里的一个关键词是<code>推荐</code>，决定使用哪种异常最为根本的还是第一条原则。如果第一条原则难以判断时，才考虑调用者。这条原则和<code>Effective Java</code>中的第58条很像，如果有这本书的朋友可以再拿出来读读。</p>

<p>我和<code>Effective Java</code>#58不同的观点在于，这条原则只能是<code>推荐</code>，另外，对于所有不能恢复的情况我都建议使用非受检异常。我对可恢复的理解是，如果API的调用者能够处理你抛出的异常，并给出积极的响应和反馈，并指导它的使用者做出调整，那么这就是可恢复的。不可恢复就是API的调用者无法处理你抛出的异常，或者仅仅只是打个LOG记录一下，不能对它的使用者做出提示，那么都可认为是不可恢复的。</p>

<p>还是最开始的栗子，如果调用<code>createNode</code>的调用者能响应<code>FatherNodeNotExist</code>，并把这种情况反应到终端上，那么使用受检异常是有积极意义的。对于不可恢复的情况，包括编程错误，我建议都是用非受检异常，这样系统能<code>fail fast</code>，把异常对系统的影响降到最低，同时你还能获得一个完整的异常堆栈信息，何乐而不为呢？！</p>

<p>基本上，这两条原则就能帮你决定到底是使用受检异常还是非受检异常了。当然，现实的情况很复杂，需要根据你所处的具体业务场景来判断，经验也是不可或缺的。在设计API的时候多问下自己这是正常情况下出现的吗，调用者可以处理这个异常吗，这会很有帮助的！</p>

<p>异常处理是一个非常大的话题，除了选择<code>checked exception</code>还是<code>unchecked exception</code>以外，还有一些一般的通用原装，例如：</p>

<ul>
<li>  只抛出与自己有关的异常</li>
<li>  封装底层异常</li>
<li>  尽量在抛出异常的同时多携带上下文信息</li>
</ul>

<p>这些在<code>Effective Java</code>中都有详细的介绍，朋友可以认真读一下这本书，写的非常好！</p>

<p>对异常处理有不同理解的朋友可以给我留言，一起讨论，共同进步！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[shell 在while中使用ssh的问题]]></title>
    <link href="http://www.blacklight.xin/15441960974362.html"/>
    <updated>2018-12-07T23:21:37+08:00</updated>
    <id>http://www.blacklight.xin/15441960974362.html</id>
    <content type="html"><![CDATA[
<p>执行如下脚本:</p>

<pre><code>#!/bin/bash
source /etc/profile

DIR=$1

if [ -z &quot;$DIR&quot;  ];then
   echo &quot;DIR can not be empty.&quot;
   exit 10
fi

BRANCH_NAME=$2
if [ &quot;${BRANCH_NAME}&quot; != &quot;storm&quot; ];then
   echo &quot;will exit 0.not build&quot;
   exit 0
fi

TARGET_DIR=&quot;/home/admin/pro&quot;

ls -l ${DIR}/bigdata-storm/*/target/*jar-with-dependencies.jar | awk  &#39;{print $NF}&#39; | while read -r filePath;do
  echo &quot;filePath:${filePath}&quot;
  fileName=`basename $filePath`
  dirName=`basename $filePath &quot;.jar&quot;`
  echo &quot;fileName:${fileName}&quot;
  echo &quot;dirName:${dirName}&quot;
  #command=&quot;${TARGET_DIR}/run.sh ${fileName}&quot;
  command=&quot;mkdir -p &#39;${TARGET_DIR}/${dirName}&#39;&quot;
  ssh -p 22 admin@10.6.0.94 &quot;${command}&quot;
  scp &quot;${filePath}&quot; &quot;admin@10.6.0.94:${TARGET_DIR}/${dirName}&quot;
done
</code></pre>

<span id="more"></span><!-- more -->

<p><strong>预期结果</strong> :目录下有多少个fat jar 就循环执行ssh 多少次<br/>
<strong>时间结果</strong>：只循环了一次<br/>
<strong>分析</strong>:</p>

<blockquote>
<p>while中使用重定向机制，file1文件中的信息都已经读入并重定向给了整个while语句,所以当我们在while循环中再一次调用read语句，就会读取到下一条记录，但是，因为ssh会读取存在的缓存，调用完ssh语句后，输入缓存中已经都被读完了，当read语句再读的时候当然也就读不到纪录，循环也就退出了。</p>
</blockquote>

<p><strong>解决方法：</strong> 对ssh使用-n 参数 或者 输入重定向，而防止它去读while的缓存，或者使用for循环避免使用重定向的方式</p>

<pre><code class="language-java">ssh -np 22 admin@10.6.0.94 &quot;${command}&quot;
</code></pre>

<p>-n 解释如下</p>

<blockquote>
<p>-n      Redirects stdin from /dev/null (actually, prevents reading from stdin).  This must be used when ssh is run in the background.  A common trick is to use this to run X11 programs on a remote machine.  For example, ssh -n shadows.cs.hut.fi emacs &amp; will start an emacs on shadows.cs.hut.fi, and the X11 connection will be automatically forwarded over an encrypted channel.  The ssh program will be put in the background.  (This does not work if ssh needs to ask for a password or passphrase; see also the -f option.)</p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[java double 精度问题]]></title>
    <link href="http://www.blacklight.xin/15011626313885.html"/>
    <updated>2017-07-27T21:37:11+08:00</updated>
    <id>http://www.blacklight.xin/15011626313885.html</id>
    <content type="html"><![CDATA[
<p>标题     在Java中实现浮点数的精确计算    AYellow（原作） 修改<br/><br/>
关键字     Java 浮点数 精确计算<br/><br/>
问题的提出：<br/>
如果我们编译运行下面这个程序会看到什么？</p>

<span id="more"></span><!-- more -->

<pre><code class="language-java">public class Test{
    public static void main(String args[]){
        System.out.println(0.05+0.01);
        System.out.println(1.0-0.42);
        System.out.println(4.015*100);
        System.out.println(123.3/100);
    }
};
</code></pre>

<p>你没有看错！结果确实是<br/>
0.060000000000000005<br/>
0.5800000000000001<br/>
401.49999999999994<br/>
1.2329999999999999<br/>
Java中的简单浮点数类型float和double不能够进行运算。不光是Java，在其它很多编程语言中也有这样的问题。在大多数情况下，计算的结果是准确的，但是多试几次（可以做一个循环）就可以试出类似上面的错误。现在终于理解为什么要有BCD码了。<br/>
这个问题相当严重，如果你有9.999999999999元，你的计算机是不会认为你可以购买10元的商品的。<br/>
在有的编程语言中提供了专门的货币类型来处理这种情况，但是Java没有。现在让我们看看如何解决这个问题。</p>

<p>四舍五入<br/>
我们的第一个反应是做四舍五入。Math类中的round方法不能设置保留几位小数，我们只能象这样（保留两位）：</p>

<pre><code class="language-java">public double round(double value){
    return Math.round(value*100)/100.0;
}
</code></pre>

<p>非常不幸，上面的代码并不能正常工作，给这个方法传入4.015它将返回4.01而不是4.02，如我们在上面看到的</p>

<p>** 4.015*100=401.49999999999994 **</p>

<p>因此如果我们要做到精确的四舍五入，不能利用简单类型做任何运算<br/>
java.text.DecimalFormat也不能解决这个问题：<br/>
System.out.println(new java.text.DecimalFormat(&quot;0.00&quot;).format(4.025));<br/>
输出是4.02</p>

<p>BigDecimal<br/>
在《Effective Java》这本书中也提到这个原则，float和double只能用来做科学计算或者是工程计算，在商业计算中我们要用 java.math.BigDecimal。BigDecimal一共有4个够造方法，我们不关心用BigInteger来够造的那两个，那么还有两个，它们是：</p>

<pre><code class="language-java">BigDecimal(double val) 
          Translates a double into a BigDecimal. 
BigDecimal(String val) 
          Translates the String repre sentation of a BigDecimal into a BigDecimal.
</code></pre>

<p>上面的API简要描述相当的明确，而且通常情况下，上面的那一个使用起来要方便一些。我们可能想都不想就用上了，会有什么问题呢？等到出了问题的时候，才发现上面哪个够造方法的详细说明中有这么一段：<br/>
Note: the results of this constructor can be somewhat unpredictable. One might assume that new BigDecimal(.1) is exactly equal to .1, but it is actually equal to .1000000000000000055511151231257827021181583404541015625. This is so because .1 cannot be represented exactly as a double (or, for that matter, as a binary fraction of any finite length). Thus, the long value that is being passed in to the constructor is not exactly equal to .1, appearances nonwithstanding. <br/>
The (String) constructor, on the other hand, is perfectly predictable: new BigDecimal(&quot;.1&quot;) is exactly equal to .1, as one would expect. Therefore, it is generally recommended that the (String) constructor be used in preference to this one.<br/>
原来我们如果需要精确计算，非要用String来够造BigDecimal不可！在《Effective Java》一书中的例子是用String来够造BigDecimal的，但是书上却没有强调这一点，这也许是一个小小的失误吧。</p>

<p>解决方案<br/>
现在我们已经可以解决这个问题了，原则是使用BigDecimal并且一定要用String来够造。<br/>
但是想像一下吧，如果我们要做一个加法运算，需要先将两个浮点数转为String，然后够造成BigDecimal，在其中一个上调用add方法，传入另一个作为参数，然后把运算的结果（BigDecimal）再转换为浮点数。你能够忍受这么烦琐的过程吗？下面我们提供一个工具类Arith来简化操作。它提供以下静态方法，包括加减乘除和四舍五入：<br/>
public static double add(double v1,double v2)<br/>
public static double sub(double v1,double v2)<br/>
public static double mul(double v1,double v2)<br/>
public static double div(double v1,double v2)<br/>
public static double div(double v1,double v2,int scale)<br/>
public static double round(double v,int scale)<br/>
附录<br/>
源文件Arith.java：</p>

<pre><code class="language-java">import java.math.BigDecimal;
/**
 * 由于Java的简单类型不能够精确的对浮点数进行运算，这个工具类提供精
 * 确的浮点数运算，包括加减乘除和四舍五入。
 */
public class Arith{
    //默认除法运算精度
    private static final int DEF_DIV_SCALE = 10;
    //这个类不能实例化
    private Arith(){
    }
 
    /**
     * 提供精确的加法运算。
     * @param v1 被加数
     * @param v2 加数
     * @return 两个参数的和
     */
    public static double add(double v1,double v2){
        BigDecimal b1 = new BigDecimal(Double.toString(v1));
        BigDecimal b2 = new BigDecimal(Double.toString(v2));
        return b1.add(b2).doubleValue();
    }
    /**
     * 提供精确的减法运算。
     * @param v1 被减数
     * @param v2 减数
     * @return 两个参数的差
     */
    public static double sub(double v1,double v2){
        BigDecimal b1 = new BigDecimal(Double.toString(v1));
        BigDecimal b2 = new BigDecimal(Double.toString(v2));
        return b1.subtract(b2).doubleValue();
    } 
    /**
     * 提供精确的乘法运算。
     * @param v1 被乘数
     * @param v2 乘数
     * @return 两个参数的积
     */
    public static double mul(double v1,double v2){
        BigDecimal b1 = new BigDecimal(Double.toString(v1));
        BigDecimal b2 = new BigDecimal(Double.toString(v2));
        return b1.multiply(b2).doubleValue();
    }
 
    /**
     * 提供（相对）精确的除法运算，当发生除不尽的情况时，精确到
     * 小数点以后10位，以后的数字四舍五入。
     * @param v1 被除数
     * @param v2 除数
     * @return 两个参数的商
     */
    public static double div(double v1,double v2){
        return div(v1,v2,DEF_DIV_SCALE);
    }
 
    /**
     * 提供（相对）精确的除法运算。当发生除不尽的情况时，由scale参数指
     * 定精度，以后的数字四舍五入。
     * @param v1 被除数
     * @param v2 除数
     * @param scale 表示表示需要精确到小数点以后几位。
     * @return 两个参数的商
     */
    public static double div(double v1,double v2,int scale){
        if(scale&lt;0){
            throw new IllegalArgumentException(
                &quot;The scale must be a positive integer or zero&quot;);
        }
        BigDecimal b1 = new BigDecimal(Double.toString(v1));
        BigDecimal b2 = new BigDecimal(Double.toString(v2));
        return b1.divide(b2,scale,BigDecimal.ROUND_HALF_UP).doubleValue();
    }
 
    /**
     * 提供精确的小数位四舍五入处理。
     * @param v 需要四舍五入的数字
     * @param scale 小数点后保留几位
     * @return 四舍五入后的结果
     */
    public static double round(double v,int scale){
        if(scale&lt;0){
            throw new IllegalArgumentException(
                &quot;The scale must be a positive integer or zero&quot;);
        }
        BigDecimal b = new BigDecimal(Double.toString(v));
        BigDecimal one = new BigDecimal(&quot;1&quot;);
        return b.divide(one,scale,BigDecimal.ROUND_HALF_UP).doubleValue();
    }
}; ﻿

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[docker 常用命令]]></title>
    <link href="http://www.blacklight.xin/14967618384549.html"/>
    <updated>2017-06-06T23:10:38+08:00</updated>
    <id>http://www.blacklight.xin/14967618384549.html</id>
    <content type="html"><![CDATA[
<p>1.容器互联</p>

<p>docker run -dt -p88:80 --name web3 --link web2:l_web nginx:v2</p>

<p>2.启动容器<br/>
docker run --name web2 -d -p 81:80 nginx:v2</p>

<p>3.执行容器命令<br/>
docker exec -it webserver bash</p>

<p>4.进入容器<br/>
docker attach 36454d9d5a1c</p>

<p>5.所有镜像<br/>
docker images</p>

<p>6.正在运行的容器</p>

<p>docker ps （docker ps -a 所有容器）</p>

<p>7.docker history nginx</p>

<p>8.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用 Dockerfile 定制镜像]]></title>
    <link href="http://www.blacklight.xin/14965523697004.html"/>
    <updated>2017-06-04T12:59:29+08:00</updated>
    <id>http://www.blacklight.xin/14965523697004.html</id>
    <content type="html"><![CDATA[
<p>镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 Dockerfile。</p>

<p>Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。</p>

<p>还以之前定制 nginx 镜像为例，这次我们使用 Dockerfile 来定制。</p>

<p>在一个空白目录中，建立一个文本文件，并命名为 Dockerfile:</p>

<pre><code>$ mkdir mynginx
$ cd mynginx
$ touch Dockerfile
</code></pre>

<p>其内容为:</p>

<pre><code>FROM nginx
RUN echo &#39;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&#39; &gt; /usr/share/nginx/html/index.html

</code></pre>

<h2 id="toc_0">FROM 指定基础镜像</h2>

<p>所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。就像我们之前运行了一个 nginx 镜像的容器，再进行修改一样，基础镜像是必须指定的。而 FROM 就是指定基础镜像，因此一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。</p>

<p>在 Docker Hub1 上有非常多的高质量的官方镜像， 有可以直接拿来使用的服务类的镜像，如 nginx、redis、mongo、mysql、httpd、php、tomcat 等； 也有一些方便开发、构建、运行各种语言应用的镜像，如 node、openjdk、python、ruby、golang 等。 可以在其中寻找一个最符合我们最终目标的镜像为基础镜像进行定制。 如果没有找到对应服务的镜像，官方镜像中还提供了一些更为基础的操作系统镜像，如 ubuntu、debian、centos、fedora、alpine 等，这些操作系统的软件库为我们提供了更广阔的扩展空间。</p>

<p>除了选择现有镜像为基础镜像外，Docker 还存在一个特殊的镜像，名为 scratch。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[shell十三问之8: $(())与$()还有${}差在哪？]]></title>
    <link href="http://www.blacklight.xin/14962015887709.html"/>
    <updated>2017-05-31T11:33:08+08:00</updated>
    <id>http://www.blacklight.xin/14962015887709.html</id>
    <content type="html"><![CDATA[
<hr/>

<p>我们上一章介绍了()与{}的不同，<br/>
这次让我们扩展一下，看看更多的变化：<br/>
\(()与\){}又是啥玩意儿呢？</p>

<p>在bash shell中, <code>$()</code>与``(反引号)都是用来做<br/>
<code>命令替换</code>(command substitution)的。</p>

<p>所谓的<code>命令替换</code>与我们第五章学过的变量替换差不多，<br/>
都是用来<code>重组命令行</code>：<br/>
完成 `` 或者<code>$()</code>里面的<br/>
命令，将其结果替换出来，<br/>
再重组命令行。</p>

<span id="more"></span><!-- more -->

<p>例如：</p>

<pre><code>$ echo the last sunday is $(date -d &quot;last sunday&quot; +%Y-%m-%d)
</code></pre>

<p>如此便可方便得到上一个星期天的日期了...<sup>_^</sup></p>

<p>在操作上， 用\(()或\`\`都无所谓,<br/>
只是我个人比较喜欢用\)(),理由是：</p>

<ol>
<li><p>``(反引号)很容易与&#39;&#39;(单引号)搞混乱，尤其对初学者来说。<br/>
有时在一些奇怪的字形显示中，两种符号是一模一样的(只取两点)。<br/>
当然了有经验的朋友还是一眼就能分辨两者。只是，若能更好的避免混乱，<br/>
又何乐而不为呢？ <sup>_^</sup></p></li>
<li><p>在多次的复合替换中， ``需要额外的转义(escape, )处理，而$()则比较直观。<br/>
例如，一个错误的使用的例子：</p></li>
</ol>

<pre><code>    command1 `command2 `command3` `
</code></pre>

<p>原来的本意是要在command2 `command3` ,<br/>
先将command3替换出来给command2处理，<br/>
然后再将command2的处理结果，给command1来处理。<br/>
然而真正的结果在命令行中却是分成了`command2`与 ``。</p>

<p>正确的输入应该如下：</p>

<pre><code>    command1 `command2 \`command3\` `
</code></pre>

<p>要不然换成$()就没有问题了：</p>

<pre><code>    command1 $(commmand2 $(command3))
</code></pre>

<p>只要你喜欢，做多少层的替换都没有问题 ~~~<sup>_^</sup></p>

<p>不过，\(()并不是没有弊端的...<br/>
首先，\`\`基本上可用在所有的unix shell中使用，<br/>
若写成 shell script，其移植性比较高。<br/>
而\)()并不是每一种shell都能使用，我只能说，<br/>
若你用bash2的话，肯定没问题... <sup>_^</sup></p>

<p>接下来，再让我们看看\({}吧...它其实就是用来做<br/>
变量替换用的啦。<br/>
一般情况下，\)var与\({var}并没有啥不一样。<br/>
但是用\){}会比较精准的界定变量名称的范围，<br/>
比方说:</p>

<pre><code>$ A=B
$ echo $AB
</code></pre>

<p>原本是打算先将\(A的结果替换出来，<br/>
然后在其后补一个字母B；<br/>
但命令行上，<br/>
真正的结果却是替换变量名称为AB的值出来...<br/>
若使用\){}就没有问题了：</p>

<pre><code>$ A=B
$ echo ${A}B
$ BB
</code></pre>

<p>不过，假如你只看到<code>${}</code>只能用来界定变量名称的话，<br/>
那你就实在太小看bash了。</p>

<p>为了完整起见，我这里再用一些例子加以说明<code>${}</code>的一些<br/>
特异功能：<br/>
假设我们定义了一个变量file为：</p>

<pre><code>file=/dir1/dir2/dir3/my.file.txt
</code></pre>

<p>我们可以用<code>${}</code>分别替换获得不同的值：</p>

<h4 id="toc_0">1. shell字符串的非贪婪(最小匹配)左删除</h4>

<hr/>

<pre><code class="language-shell">${file#*/}  #其值为：dir1/dir2/dir3/my.file.txt
</code></pre>

<p>拿掉第一个<code>/</code>及其左边的字符串，其结果为：<br/>
<code>dir1/dir2/dir3/my.file.txt</code> 。<br/>
<code>shell<br/>
${file#*.}  #其值为：file.txt<br/>
</code><br/>
拿掉第一个<code>.</code>及其左边的字符串，其结果为：<br/>
<code>file.txt</code> 。</p>

<h4 id="toc_1">2. shell字符串的贪婪(最大匹配)左删除：</h4>

<hr/>

<pre><code>${file##*/} #其值为：my.file.txt
</code></pre>

<p>拿掉最后一个<code>/</code>及其左边的字符串，其结果为：</p>

<p><code>my.file.txt</code></p>

<pre><code>${file##*.} #其值为：txt
</code></pre>

<p>拿掉最后一个<code>.</code>及其左边的字符串，其结果为：<br/>
<code>txt</code></p>

<h4 id="toc_2">3. shell字符串的非贪婪(最小匹配)右删除：</h4>

<hr/>

<pre><code>${file%/*}  #其值为：/dir1/dir2/dir3
</code></pre>

<p>拿掉最后一个<code>/</code>及其右边的字符串，其结果为：<br/>
<code>/dir1/dir2/dir3</code>。</p>

<pre><code>${file%.*}  #其值为：/dir1/dir2/dir3/my.file
</code></pre>

<p>拿掉最后一个<code>.</code>及其右边的字符串，其结果为：<br/>
<code>/dir1/dir2/dir3/my.file</code>。</p>

<h4 id="toc_3">4. shell字符串的贪婪(最大匹配)右删除：</h4>

<hr/>

<pre><code>${file%%/*}  #其值为：其值为空。
</code></pre>

<p>拿掉第一个<code>/</code>及其右边的字符串，其结果为：<br/>
空串。</p>

<pre><code>${file%%.*}  #其值为：/dir1/dir2/dir3/my。
</code></pre>

<p>拿掉第一个<code>.</code>及其右边的字符串，其结果为：<br/>
/dir1/dir2/dir3/my。</p>

<blockquote>
<p><strong>Tips:</strong></p>

<p>记忆方法：</p>

<p><code>#</code>是去掉左边(在键盘上<code>#</code>在<code>$</code>的左边);</p>

<p><code>%</code>是去掉右边(在键盘上<code>%</code>在<code>$</code>的右边);</p>

<p>单个符号是最小匹配;</p>

<p>两个符号是最大匹配;</p>
</blockquote>

<h4 id="toc_4">5. shell字符串取子串：</h4>

<hr/>

<pre><code> ${file:0:5} #提取最左边的5个字符：/dir1
 ${file:5:5} #提取第5个字符及其右边的5个字符:/dir2
</code></pre>

<p>shell字符串取子串的格式：<code>${s:pos:length}</code>,<br/>
取字符串s的子串：从pos位置开始的字符(包括该字符)的长度为length的的子串;<br/>
其中pos为子串的首字符，在s中位置；<br/>
length为子串的长度;</p>

<blockquote>
<p><strong>Note:</strong> 字符串中字符的起始编号为0.</p>
</blockquote>

<h4 id="toc_5">6. shell字符串变量值的替换：</h4>

<hr/>

<pre><code>${file/dir/path}  #将第一个dir替换为path：/path1/dir2/dir3/my.file.txt
${file//dir/path} #将全部的dir替换为path：/path1/path2/path3/my.file.txt
</code></pre>

<p>shell字符串变量值的替换格式：</p>

<ul>
<li><p>首次替换：<br/>
<code>${s/src_pattern/dst_pattern}</code> 将字符串s中的第一个src_pattern替换为dst_pattern。</p></li>
<li><p>全部替换：<br/>
<code>${s//src_pattern/dst_pattern}</code> 将字符串s中的所有出现的src_pattern替换为dst_pattern.</p></li>
</ul>

<h4 id="toc_6">7. ${}还可针对变量的不同状态(没设定、空值、非空值)进行赋值：</h4>

<hr/>

<ul>
<li><p><code>${file-my.file.txt}</code> #如果file没有设定，则使用<br/>
使用my.file.txt作为返回值, 否则返回${file};(空值及非空值时，不作处理。);</p></li>
<li><p><code>${file:-my.file.txt}</code> #如果file没有设定或者\({file}为空值, 均使用my.file.txt作为其返回值，否则，返回\){file}.(${file} 为非空值时，不作处理);</p></li>
<li><p><code>${file+my.file.txt}</code> #如果file已设定(为空值或非空值), 则使用my.file.txt作为其返回值，否则不作处理。(未设定时，不作处理);</p></li>
<li><p><code>${file:+my.file.txt}</code> #如果${file}为非空值, 则使用my.file.txt作为其返回值，否则，(未设定或者为空值时)不作处理。</p></li>
<li><p><code>${file=my.file.txt}</code> #如果file为设定，则将file赋值为my.file.txt，同时将\({file}作为其返回值；否则，file已设定(为空值或非空值)，则返回\){file}。</p></li>
<li><p><code>${file:=my.file.txt}</code> #如果file未设定或者\({file}为空值, 则my.file.txt作为其返回值，<br/>
同时，将\){file}赋值为my.file.txt，否则，(非空值时)不作处理。</p></li>
<li><p><code>${file?my.file.txt}</code> #如果file没有设定，则将my.file.txt输出至STDERR, 否侧，<br/>
已设定(空值与非空值时)，不作处理。</p></li>
<li><p><code>${file:?my.file.txt}</code> #若果file未设定或者为空值，则将my.file.txt输出至STDERR，否则，<br/>
非空值时，不作任何处理。</p></li>
</ul>

<blockquote>
<p><strong>Tips:</strong></p>

<p>以上的理解在于，你一定要分清楚，<code>unset</code>与<code>null</code>以及non-null这三种状态的赋值；<br/>
一般而言，与null有关，若不带<code>:</code>, null不受影响；<br/>
若带 <code>:</code>, 则连null值也受影响。</p>
</blockquote>

<h4 id="toc_7">8. 计算shell字符串变量的长度：<code>${#var}</code></h4>

<hr/>

<pre><code> ${#file}  #其值为27, 因为/dir1/dir2/dir3/my.file.txt刚好为27个字符。
</code></pre>

<h4 id="toc_8">9. bash数组(array)的处理方法</h4>

<hr/>

<p>接下来，为大家介绍一下bash的数组(array)的处理方法。<br/>
一般而言, <code>A=&quot;a b c def&quot;</code><br/>
这样的变量只是将<code>$A</code>替换为一个字符串，<br/>
但是改为 <code>A=(a b c def)</code>,<br/>
则是将<code>$A</code>定义为数组....</p>

<h5 id="toc_9">1). 数组替换方法可参考如下方法：</h5>

<pre><code>${A[@]} #方法一
${A[*]} #方法二
</code></pre>

<p>以上两种方法均可以得到：a b c def, 即数组的全部元素。</p>

<h5 id="toc_10">2). 访问数组的成员:</h5>

<pre><code>${A[0]}
</code></pre>

<p>其中，<code>${A[0]}</code>可得到a, 即数组A的第一个元素，<br/>
而 <code>${A[1]}</code>则为数组A的第二元素，依次类推。</p>

<h5 id="toc_11">3). 数组的length：</h5>

<pre><code>${#A[@]} #方法一
${#A[*]} #方法二
</code></pre>

<p>以上两种方法均可以得到数组的长度: 4, 即数组的所有元素的个数。</p>

<p>回忆一下，针对字符串的长度计算，使用<code>${#str_var}</code>;<br/>
我们同样可以将该方法应用于数组的成员:</p>

<pre><code>${#A[0]}
</code></pre>

<p>其中，<code>${#A[0]}</code>可以得到：1，即数组A的第一个元素(a)的长度;<br/>
同理，<code>${#A[3]}</code>可以得到: 3, 即数组A的第4个元素(def)的长度。</p>

<h5 id="toc_12">4). 数组元素的重新赋值：</h5>

<pre><code>A[3]=xyz
</code></pre>

<p>将数组A的第四个元素重新定义为xyz。</p>

<blockquote>
<p><strong>Tips:</strong></p>

<p>诸如此类的...</p>

<p>能够善用bash的\(()与\){}可以大大提高及<br/>
简化shell在变量上的处理能力哦~~~<sup>_^</sup></p>
</blockquote>

<h4 id="toc_13">10. $(())作用:</h4>

<hr/>

<p>好了，最后为大家介绍<code>$(())</code>的用途吧：<br/>
<strong><code>$(())</code>是用来作整数运算的</strong>。</p>

<p>在bash中， <code>$(())</code>的整数运算符号大致有这些：</p>

<ul>
<li> +- *  /    #分别为&quot;加、减、乘、除&quot;。</li>
<li> %            #余数运算,(模数运算)</li>
<li> &amp; | ^ !      #分别为&quot;AND、OR、XOR、NOT&quot;运算。</li>
</ul>

<p>例如：</p>

<pre><code>$ a=5; b=7; c=2;
$ echo $(( a + b * c ))
19
$ echo $(( (a + b)/c ))
6
$ echo $(( (a * b) % c ))
1
</code></pre>

<p>在<code>$(())</code>中的变量名称,<br/>
可以在其前面加 <code>$</code>符号来替换，<br/>
也可以不用，如：<br/>
<code>$(( $a + $b * $c ))</code> 也可以得到19的结果。</p>

<p>此外，<code>$(())</code>还可作不同进制(如二进制、八进制、十六进制)的运算，<br/>
只是输出结果均为十进制的。</p>

<pre><code>echo $(( 16#2a )) #输出结果为：42，(16进制的2a)
</code></pre>

<p>以一个实用的例子来看看吧 :<br/>
假如当前的umask是022,那么新建文件的权限即为：</p>

<pre><code>$ umask 022
$ echo &quot;obase=8; $(( 8#666 &amp; (8#777 ^ 8#$(umask)) ))&quot; | bc
644
</code></pre>

<p>事实上，单纯用<code>(())</code>也可以重定义变量值，或作testing：</p>

<pre><code>a=5; ((a++)) #可将$a 重定义为6
a=5; ((a--)) #可将$a 重定义为4
a=5; b=7; ((a&lt; b)) #会得到0 (true)返回值。
</code></pre>

<p>常见的用于<code>(())</code>的测试符号有如下这些：</p>

<table>
<thead>
<tr>
<th>符号</th>
<th>符号名称</th>
</tr>
</thead>

<tbody>
<tr>
<td>&lt;</td>
<td>小于号</td>
</tr>
<tr>
<td>&gt;</td>
<td>大于号</td>
</tr>
<tr>
<td>&lt;=</td>
<td>小于或等于</td>
</tr>
<tr>
<td>&gt;=</td>
<td>大于或等于</td>
</tr>
<tr>
<td>==</td>
<td>等于</td>
</tr>
<tr>
<td>!=</td>
<td>不等于</td>
</tr>
</tbody>
</table>

<blockquote>
<p><strong>Note:</strong></p>

<p>使用<code>(())</code>作整数测试时，<br/>
请不要跟<code>[]</code>的整数测试搞混乱了。</p>

<p>更多的测试，我们将于第10章为大家介绍。</p>
</blockquote>

<p>怎样？ 好玩吧... <sup>_^</sup></p>

<p>okay,这次暂时说这么多...</p>

<p>上面的介绍，并没有详列每一种可用的状态，<br/>
更多的，就请读者参考手册文件(man)吧...</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[flume-ng源码分析-核心组件分析]]></title>
    <link href="http://www.blacklight.xin/14961334971521.html"/>
    <updated>2017-05-30T16:38:17+08:00</updated>
    <id>http://www.blacklight.xin/14961334971521.html</id>
    <content type="html"><![CDATA[
<p>从第一篇分析可知,flume中所有的组件都会实现LifecycleAware 接口。该接口定义如下：</p>

<pre><code class="language-java">public interface LifecycleAware {
  public void start();
  public void stop();
  public LifecycleState getLifecycleState();
}
</code></pre>

<p>在组件启动的时候会调用start方法，当有异常时调用stop方法。getLifecycleState 方法返回该组件的状态。包含 <strong>IDLE, START, STOP, ERROR;</strong> </p>

<p>当在组件开发中需要配置一些属性的时候可以实现<strong>Configurable</strong>接口</p>

<pre><code class="language-java">public interface Configurable {
  public void configure(Context context);

}
</code></pre>

<p>下面开始分析Agent中各个组件的实现</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">source 实现</h2>

<h2 id="toc_1">source定义</h2>

<pre><code class="language-java">public interface Source extends LifecycleAware, NamedComponent {
 public void setChannelProcessor(ChannelProcessor channelProcessor);
 public ChannelProcessor getChannelProcessor();
}
</code></pre>

<p>可以看到Source 继承了LifecycleAware 接口，并且提供了<strong>ChannelProcessor</strong>的getter和setter方法,channelProcessor在<strong>常用架构篇中</strong>降到提供了日志过滤链,和channel选择的功能。所以Source的逻辑应该都在LifecycleAware中的start,stop方法中.</p>

<h2 id="toc_2">source创建</h2>

<p>在<strong>启动篇</strong>中，我们讲到了flume是如何启动的。大致流程就是读取配置文件,生成flume的各种组件,执行各个组件的start()方法。在<strong>getConfiguration()</strong>方法中调用了<strong>loadSources()</strong>方法。<br/>
可以看到在loadSources方法中如何创建Source的</p>

<pre><code class="language-java">private void loadSources(AgentConfiguration agentConf,
      Map&lt;String, ChannelComponent&gt; channelComponentMap,
      Map&lt;String, SourceRunner&gt; sourceRunnerMap)
      throws InstantiationException {

    Set&lt;String&gt; sourceNames = agentConf.getSourceSet();//获取所有的Source
    Map&lt;String, ComponentConfiguration&gt; compMap =agentConf.getSourceConfigMap(); //获取所有Source对应的配置
    for (String sourceName : sourceNames) {
      ComponentConfiguration comp = compMap.get(sourceName);//获取该source对应的配置
      if(comp != null) {
        SourceConfiguration config = (SourceConfiguration) comp; //转化为source配置

        Source source = sourceFactory.create(comp.getComponentName(),
            comp.getType());//通过sourceFactory 创建source
        try {
          Configurables.configure(source, config);//配置组件的其他属性
          Set&lt;String&gt; channelNames = config.getChannels()//获取该source的所有channel名称
          List&lt;Channel&gt; sourceChannels = new ArrayList&lt;Channel&gt;();
          for (String chName : channelNames) {//遍历所有的额channle，如果该channel已经实例化过了并且在channelComponentMap中已经存储了，那么将该channel放入sourceChannels
            ChannelComponent channelComponent = channelComponentMap.get(chName);
            if(channelComponent != null) {
              sourceChannels.add(channelComponent.channel);
            }
          }
          if(sourceChannels.isEmpty()) {//如果这个source没有关联到任何channel那么直接抛出异常
            String msg = String.format(&quot;Source %s is not connected to a &quot; +
                &quot;channel&quot;,  sourceName);
            throw new IllegalStateException(msg);
          }
          //以下创建出ChannelProcessor并且配置其他属性
          ChannelSelectorConfiguration selectorConfig =
              config.getSelectorConfiguration();

          ChannelSelector selector = ChannelSelectorFactory.create(
              sourceChannels, selectorConfig);

          ChannelProcessor channelProcessor = new ChannelProcessor(selector);
          Configurables.configure(channelProcessor, config);

          source.setChannelProcessor(channelProcessor);//设置channelSelector
          sourceRunnerMap.put(comp.getComponentName(),
              SourceRunner.forSource(source));//将改source，以及对应的SourceRunner放入SourceRunnerMap中
          for(Channel channel : sourceChannels) {//遍历改source所有的channel并且将改source添加到该channel的Component中
            ChannelComponent channelComponent = Preconditions.
                checkNotNull(channelComponentMap.get(channel.getName()),
                    String.format(&quot;Channel %s&quot;, channel.getName()));
            channelComponent.components.add(sourceName);
          }
        } catch (Exception e) {
          String msg = String.format(&quot;Source %s has been removed due to an &quot; +
              &quot;error during configuration&quot;, sourceName);
          LOGGER.error(msg, e);
        }
      }
    }
    ......
</code></pre>

<p>从上面的分析中可以看出，Source是后SourceFactory创建的，创建之后绑定到SourceRunner中，并且在SourceRunner中启动了Source。<br/>
  SourceFactory只有一个实现DefaultSourceFactory。创建Source过程如下:</p>

<pre><code class="language-java">  public Source create(String name, String type) throws FlumeException {
    Preconditions.checkNotNull(name, &quot;name&quot;);
    Preconditions.checkNotNull(type, &quot;type&quot;);
    logger.info(&quot;Creating instance of source {}, type {}&quot;, name, type);
    Class&lt;? extends Source&gt; sourceClass = getClass(type);//通过对应的类型找到对应的class
    try {
      Source source = sourceClass.newInstance();//直接创建实例
      source.setName(name);
      return source;
    } catch (Exception ex) {
      throw new FlumeException(&quot;Unable to create source: &quot; + name
          +&quot;, type: &quot; + type + &quot;, class: &quot; + sourceClass.getName(), ex);
    }
  }
</code></pre>

<p>在创建重，通过type来或者source类的class。在getClass方法中，首先会去找type对应类型的class。在SourceType中定义的。如果没有找到，则直接获得配置的类全路径。最后通过Class.forName(String)获取class对象。</p>

<p>source提供了两种方式类获取数据:轮训拉去和事件驱动</p>

<p><img src="media/14961334971521/14961359396630.jpg" alt="" style="width:811px;"/></p>

<p>PollableSource 提供的默认实现如下：<br/>
<img src="media/14961334971521/14962066716627.jpg" alt="" style="width:776px;"/></p>

<p>比如KafkaSource 利用Kafka的ConsumerApi，主动去拉去数据。</p>

<p>EventDrivenSource 提供的默认实现如下</p>

<p><img src="media/14961334971521/14962068696930.jpg" alt=""/></p>

<p>如HttpSource，NetcatSource就是事件驱动的，所谓事件驱动也就是被动等待。在HttpSource中内置了一个Jetty server，并且设置FlumeHTTPServlet 作为handler去处理数据。</p>

<h2 id="toc_3">source的启动</h2>

<p>从上面的分析中知道，在启动flume读取配置文件时,会将所有的组件封装好，然后再启动。对于Source而言,封装成了SourceRunner,通过SourceRunner间接启动Source。</p>

<pre><code class="language-java">public static SourceRunner forSource(Source source) {
    SourceRunner runner = null;

    if (source instanceof PollableSource) {//判断该source是否为PollableSource
      runner = new PollableSourceRunner();
      ((PollableSourceRunner) runner).setSource((PollableSource) source);
    } else if (source instanceof EventDrivenSource) {//判断该source是否为EventDrivenSource
      runner = new EventDrivenSourceRunner();
      ((EventDrivenSourceRunner) runner).setSource((EventDrivenSource) source);
    } else {//否则抛出异常
      throw new IllegalArgumentException(&quot;No known runner type for source &quot;
          + source);
    }
    return runner;
  }
</code></pre>

<p>从上面可以看出SourceRunner 默认提供两种实现，PollableSourceRunner,EventDrivenSource.分别对应PollableSource 和EventDrivenSource。</p>

<h3 id="toc_4">查看PollableSourceRunner是如何启动的</h3>

<pre><code class="language-java">@Override
  public void start() {
    PollableSource source = (PollableSource) getSource();
    ChannelProcessor cp = source.getChannelProcessor();
    cp.initialize();//初始化ChannelProcessor
    source.start();//启动source组件

    runner = new PollingRunner();//单独启动一个线程去轮询source

    runner.source = source;
    runner.counterGroup = counterGroup;
    runner.shouldStop = shouldStop;

    runnerThread = new Thread(runner);
    runnerThread.setName(getClass().getSimpleName() + &quot;-&quot; + 
        source.getClass().getSimpleName() + &quot;-&quot; + source.getName());
    runnerThread.start();

    lifecycleState = LifecycleState.START;//设置状态为START
  }
</code></pre>

<p>在PollableSourceRunner中我们看到单独启动一个线程去执行PollingRunner,这个线程的作用就是不断的去轮询。查看PollingRunner的实现</p>

<pre><code class="language-java">@Override
    public void run() {
      logger.debug(&quot;Polling runner starting. Source:{}&quot;, source);

      while (!shouldStop.get()) {//没有停止，那就继续吧
        counterGroup.incrementAndGet(&quot;runner.polls&quot;);

        try {
        //真正的拉去逻辑在source process()方法中，调用该方法进行拉去数据,并且
        //判断返回状态是否为BACKOFF(失败补偿),如果是那么等待時間超时之后就会重试
          if (source.process().equals(PollableSource.Status.BACKOFF)) {
            counterGroup.incrementAndGet(&quot;runner.backoffs&quot;);

            Thread.sleep(Math.min(
                counterGroup.incrementAndGet(&quot;runner.backoffs.consecutive&quot;)
                * backoffSleepIncrement, maxBackoffSleep));
          } else {
            counterGroup.set(&quot;runner.backoffs.consecutive&quot;, 0L);
          }
        } catch (InterruptedException e) {
          logger.info(&quot;Source runner interrupted. Exiting&quot;);
          counterGroup.incrementAndGet(&quot;runner.interruptions&quot;);
        } ......
      }

      logger.debug(&quot;Polling runner exiting. Metrics:{}&quot;, counterGroup);
    }
</code></pre>

<p>比如KafkaSource ,它的逻辑就在process方法中，</p>

<pre><code class="language-java">          // get next message
          MessageAndMetadata&lt;byte[], byte[]&gt; messageAndMetadata = it.next();
          kafkaMessage = messageAndMetadata.message();
          kafkaKey = messageAndMetadata.key();

          // Add headers to event (topic, timestamp, and key)
          headers = new HashMap&lt;String, String&gt;();
          headers.put(KafkaSourceConstants.TIMESTAMP,
                  String.valueOf(System.currentTimeMillis()));
          headers.put(KafkaSourceConstants.TOPIC, topic);
          
</code></pre>

<h3 id="toc_5">EventDrivenSourceRunner</h3>

<pre><code>@Override
  public void start() {
    Source source = getSource();//获取source
    ChannelProcessor cp = source.getChannelProcessor();//获取source对应的ChannelProcessor
    cp.initialize();//初始化channelprocessor
    source.start();//启动source
    lifecycleState = LifecycleState.START;//标记状态为START
  }
</code></pre>

<p>可以看到EventDrivenSourceRunner和PollableSourceRunnner 启动流程大致相同,只是PollableSourceRunner会额外启动一个线程去轮询source。</p>

<h2 id="toc_6">channel的实现</h2>

<p>source 获取到数据后，会交给channelProcessor处理，发送到channel。最后由sink消费掉。<br/>
所以channel是source，sink实现异步化的关键。</p>

<p>channelProcessor 中两格重要的成员</p>

<pre><code class="language-java">  private final ChannelSelector selector;//channel选择器
  private final InterceptorChain interceptorChain; //过滤链
</code></pre>

<p>InterceptorChain 是有多个Interceptor组成,并且实现了Interceptor接口</p>

<pre><code>public class InterceptorChain implements Interceptor {
     private List&lt;Interceptor&gt; interceptors;
}
</code></pre>

<p>Interceptor.java</p>

<pre><code class="language-java">public interface Interceptor {
  public void initialize();// 做一些处理话工作
  public Event intercept(Event event);//拦截单个event并且返回
  public List&lt;Event&gt; intercept(List&lt;Event&gt; events);//批量拦截event
  public void close();
    public interface Builder extends Configurable {
    public Interceptor build();
  }//用来创建特定的Interceptor
}
</code></pre>

<p>Interceptor定义了一些处理Event的接口，再Event处理之后都会返回改Envent</p>

<p>从source的分析中我们可以知道,如果是PollableSourceRunner会调用source 中的process()方法。如果是EventDrivenSourceRunner，就会用特定的方法来获取source，比如httpSource 利用FlumeHTTPServlet来接受消息</p>

<pre><code class="language-java">try {
        events = handler.getEvents(request);//从请求中获取event
      ...
            try {
        getChannelProcessor().processEventBatch(events);//通过ChanneProcess进行的processEventBatch方法进行批量处理
      } catch (ChannelException ex) {
</code></pre>

<p>比如KafkaSource 是PollableSourceRunner 那么会调用KafkaSource中的process()方法。</p>

<pre><code class="language-java">public Status process() throws EventDeliveryException {
   ...
   
   // get next message
          MessageAndMetadata&lt;byte[], byte[]&gt; messageAndMetadata = it.next();
          kafkaMessage = messageAndMetadata.message();
          kafkaKey = messageAndMetadata.key();

          // Add headers to event (topic, timestamp, and key)
          headers = new HashMap&lt;String, String&gt;();
          headers.put(KafkaSourceConstants.TIMESTAMP,
                  String.valueOf(System.currentTimeMillis()));
          headers.put(KafkaSourceConstants.TOPIC, topic);
          if (kafkaKey != null) {
            headers.put(KafkaSourceConstants.KEY, new String(kafkaKey));
          }
          ......
          event = EventBuilder.withBody(kafkaMessage, headers);
          eventList.add(event);
          ......
          
        if()......  
        getChannelProcessor().processEventBatch(eventList);//交给channelProcessor处理
        counter.addToEventAcceptedCount(eventList.size());
        ...
      }
}

</code></pre>

<p>从以上分析不管source是轮询还是事件驱动的，都会触发ChannelProcessor中的processEvent或者ProcesEventBatch方法</p>

<pre><code class="language-java">public void processEventBatch(List&lt;Event&gt; events) {
    events = interceptorChain.intercept(events);//调用Interceptor处理events
    
    List&lt;Channel&gt; reqChannels = selector.getRequiredChannels(event);//获取必须成功处理的Channel ,写失败了必须回滚source
    List&lt;Channel&gt; optChannels = selector.getOptionalChannels(event);//获取非必须成功处理的channel，写失败了就忽略
    
    
    // 這裡分析處理必須成功channel的情況。非必須的channel處理情況一樣
    for (Channel reqChannel : reqChannelQueue.keySet()) {
      Transaction tx = reqChannel.getTransaction();//获取该channel上的事务
      Preconditions.checkNotNull(tx, &quot;Transaction object must not be null&quot;);
      try {
        tx.begin();//开始事务

        List&lt;Event&gt; batch = reqChannelQueue.get(reqChannel);//获取events

        for (Event event : batch) {
          reqChannel.put(event);//处理Channel
        }

        tx.commit();//提交事务
      } catch (Throwable t) {
        tx.rollback();//发生异常回滚
        if (t instanceof Error) {
          LOG.error(&quot;Error while writing to required channel: &quot; +
              reqChannel, t);
          throw (Error) t;
        } else {
          throw new ChannelException(&quot;Unable to put batch on required &quot; +
              &quot;channel: &quot; + reqChannel, t);
        }
      } finally {
        if (tx != null) {
          tx.close();//关闭事务
        }
      }
    }
}
</code></pre>

<p>最后就是ChannelSelector ，flume默认提供两种实现多路复用和复制。多路复用选择器可以根据header中的值而选择不同的channel，复制就会把event复制到多个channel中。flume默认是复制选择器。</p>

<p><img src="media/14961334971521/14962112541107.jpg" alt="" style="width:315px;"/></p>

<p>同样Selector的创建也是通过ChannelSelectorFactory创建的.</p>

<pre><code class="language-java"> 
 public static ChannelSelector create(List&lt;Channel&gt; channels,
      ChannelSelectorConfiguration conf) {
    String type = ChannelSelectorType.REPLICATING.toString();
    if (conf != null){
      type = conf.getType();
    }
    ChannelSelector selector = getSelectorForType(type);
    selector.setChannels(channels);
    Configurables.configure(selector, conf);
    return selector;
  }
</code></pre>

<p>默认提供复制选择器，如果配置文件中配置了选择器那么就从配置文件中获取。</p>

<p>上面看到在processEventBatch 方法中调用channel的put方法。channel中提供了基本的<br/>
 put和take方法来实现Event的流转。</p>

<pre><code class="language-java"> public interface Channel extends LifecycleAware, NamedComponent {
  public void put(Event event) throws ChannelException;//向channel中存放
  public Event take() throws ChannelException;//消费event
  public Transaction getTransaction();//获取事务
}
</code></pre>

<p>flume提供的默认channel如下图所示:<br/>
 <img src="media/14961334971521/14962117376319.jpg" alt=""/></p>

<h2 id="toc_7">sink的实现</h2>

<h3 id="toc_8">sink定义：</h3>

<pre><code class="language-java">public interface Sink extends LifecycleAware, NamedComponent {
  public void setChannel(Channel channel);
  public Channel getChannel();
  public Status process() throws EventDeliveryException;
  public static enum Status {
    READY, BACKOFF
  }
}
</code></pre>

<p>提供了channel的setter,getter方法。process方法用来消费。并返回状态READY,BACKOFF</p>

<h3 id="toc_9">sink的创建</h3>

<pre><code class="language-java">SinkConfiguration config = (SinkConfiguration) comp;
 Sink sink = sinkFactory.create(comp.getComponentName(),
 comp.getType());
</code></pre>

<p>sink的创建也是通过sinkFactory</p>

<pre><code class="language-java">public Sink create(String name, String type) throws FlumeException {
    Preconditions.checkNotNull(name, &quot;name&quot;);
    Preconditions.checkNotNull(type, &quot;type&quot;);
    logger.info(&quot;Creating instance of sink: {}, type: {}&quot;, name, type);
    Class&lt;? extends Sink&gt; sinkClass = getClass(type);//获取sink对应的类型的Class
    try {
      Sink sink = sinkClass.newInstance();//创建实例
      sink.setName(name);
      return sink;
    } catch (Exception ex) {
      throw new FlumeException(&quot;Unable to create sink: &quot; + name
          + &quot;, type: &quot; + type + &quot;, class: &quot; + sinkClass.getName(), ex);
    }
  }
</code></pre>

<p>通过传入的type找到对应的Class 要是没有找到则直接通过Class.forNamae(String name)来创建</p>

<p>sink还提供了分组功能。该功能由SinkGroup实现。在SinkGroup内部如何调度多个Sink，则交给SinkProcessor完成。</p>

<h3 id="toc_10">sink的启动</h3>

<p>和Source一样，flume也为Sink提供了SinkRunner来流转Sink<br/>
在sinkRunner中</p>

<pre><code class="language-java">public void start() {
    SinkProcessor policy = getPolicy();
    policy.start();//启动SinkProcessor
    runner = new PollingRunner();//单独启动一个线程，从channel中消费数据
    runner.policy = policy;
    runner.counterGroup = counterGroup;
    runner.shouldStop = new AtomicBoolean();
    runnerThread = new Thread(runner);
    runnerThread.setName(&quot;SinkRunner-PollingRunner-&quot; +
        policy.getClass().getSimpleName());
    runnerThread.start();
    lifecycleState = LifecycleState.START;
  }

</code></pre>

<p>sinkRunner中通过启动SinkProcessor 间接启动Sink，并且单独启动一个线程，不停地调用process()方法从channel中消费数据<br/>
在SinkProcessor中，如果是DefaultSinkProcessor 那么直接调用sink.start()方法启动sink。如果是LoadBalancingSinkProcessor，FailoverSinkProcessor由于这两种处理器中包含多个Sink，所以会依次遍历sink 调用start()方法启动</p>

<pre><code class="language-java">public void run() {
      logger.debug(&quot;Polling sink runner starting&quot;);

      while (!shouldStop.get()) {//判断是否停止
        try {
          if (policy.process().equals(Sink.Status.BACKOFF)) {//调用SinkProcessor的proces()方法进行处理
            counterGroup.incrementAndGet(&quot;runner.backoffs&quot;);
            Thread.sleep(Math.min(
                counterGroup.incrementAndGet(&quot;runner.backoffs.consecutive&quot;)
                * backoffSleepIncrement, maxBackoffSleep));
          } else {
            counterGroup.set(&quot;runner.backoffs.consecutive&quot;, 0L);
          }
        }
      }
    }
</code></pre>

<p>该线程会不停的执行SinkProcessor的process()方法，而SinkProcessor的process()方法会调用对应的Sink的process()方法。然后判断处理状态如果是失败补偿，那么等待超时时间后重试</p>

<h3 id="toc_11">SinkGroup</h3>

<pre><code class="language-java">public class SinkGroup implements Configurable, ConfigurableComponent {
  List&lt;Sink&gt; sinks;
  SinkProcessor processor;
  SinkGroupConfiguration conf;
  ......  
}
</code></pre>

<p>SinkGroup中包含多个Sink,并且提供一个SinkProcessor来处理SinkGroup内部调度</p>

<h3 id="toc_12">SinkProcessor</h3>

<p>SinkProcessor 默认提供三种实现。DefaultSinkProcessor,LoadBalancingSinkProcessor,FailoverSinkProcessor</p>

<p><img src="media/14961334971521/14962397506282.jpg" alt="" style="width:496px;"/></p>

<p>DefaultSinkProcessor：默认实现，适用于单个sink<br/>
LoadBalancingSinkProcessor：提供负载均衡<br/>
FailoverSinkProcessor：提供故障转移</p>

<h4 id="toc_13">DefaultSinkProcessor</h4>

<pre><code class="language-java">public class DefaultSinkProcessor implements SinkProcessor,
ConfigurableComponent {
  private Sink sink;
  private LifecycleState lifecycleState;

  @Override
  public void start() {
    Preconditions.checkNotNull(sink, &quot;DefaultSinkProcessor sink not set&quot;);
    sink.start();//启动sink
    lifecycleState = LifecycleState.START;
  }

  @Override
  public Status process() throws EventDeliveryException {
    return sink.process();
  }

  @Override
  public void setSinks(List&lt;Sink&gt; sinks) {
    Preconditions.checkNotNull(sinks);
    Preconditions.checkArgument(sinks.size() == 1, &quot;DefaultSinkPolicy can &quot;
        + &quot;only handle one sink, &quot;
        + &quot;try using a policy that supports multiple sinks&quot;);
    sink = sinks.get(0);
  }
}
</code></pre>

<p>从上面可以看出DefaultSinkProcessor 只能处理一个Sink。在process方法中调用sink的方法。具体到某个具体的Sink，比如HDFSEventSink,那么就执行该sink的process方法</p>

<p>接下来分析SinkProcessor中负载均衡和故障转移 是如何具体实现的。</p>

<h4 id="toc_14">FailOverSinkProcessor 实现分析</h4>

<p><strong>FailOverSinkProcessor</strong>  中process()方法实现如下:</p>

<pre><code class="language-java">@Override
  public Status process() throws EventDeliveryException {
    Long now = System.currentTimeMillis();
    while(!failedSinks.isEmpty() &amp;&amp; failedSinks.peek().getRefresh() &lt; now) {//检查失败队列是否有sink，并且队列中第一个sink过了失败补偿时间
      FailedSink cur = failedSinks.poll();//从失败队列中获取第一个sink，并且在队列中删除
      Status s;
      try {
        s = cur.getSink().process();//调用sink的process()方法进行处理
        if (s  == Status.READY) {//如果状态是就绪
          liveSinks.put(cur.getPriority(), cur.getSink());//将该sink放入存活队列
          activeSink = liveSinks.get(liveSinks.lastKey());//重新赋值给activeSink
          logger.debug(&quot;Sink {} was recovered from the fail list&quot;,
                  cur.getSink().getName());
        } else {//sink 处理失败
          failedSinks.add(cur);//加入失败队列
        }
        return s;
      } catch (Exception e) {
        cur.incFails();//发生异常，增加失败次数
        failedSinks.add(cur);//放入失败队列
      }
    }

    //如果失败队列为空，或者失败队列中所有的sink都没有达到失败补偿时间，那么交给activeSink进行处理，
    Status ret = null;
    while(activeSink != null) {
      try {
        ret = activeSink.process();//交给activeSink 处理
        return ret;
      } catch (Exception e) {
        logger.warn(&quot;Sink {} failed and has been sent to failover list&quot;,
                activeSink.getName(), e);
        activeSink = moveActiveToDeadAndGetNext();//如果activeSink处理失败，则把activeSink从存活队列中移动到失败队列中
      }
    }
    throw new EventDeliveryException(&quot;All sinks failed to process, &quot; +
        &quot;nothing left to failover to&quot;);
  }
</code></pre>

<ul>
<li>存活队列是一个SortMap<Key,Value> 其中key是sink的优先级。activeSink 默认取存活队列中的最后一个，存活队列是根据配置的sink优先级来排序的</li>
<li><p>失败队列是一个优先队列,按照FailSink的refresh属性进行排序</p>

<pre><code class="language-java">@Override
public int compareTo(FailedSink arg0) {
  return refresh.compareTo(arg0.refresh);
}
</code></pre>

<p>refresh 属性，在FailSink创建时和sink 处理发生异常时 会触发调整<br/>
refresh 调整策略 如下:</p>

<pre><code class="language-java">private void adjustRefresh() {
  refresh = System.currentTimeMillis()
          + Math.min(maxPenalty, (1 &lt;&lt; sequentialFailures) * FAILURE_PENALTY);
}
</code></pre>

<p>refresh 等于系统当前的毫秒加上最大等待时间(默认30s)和失败次数指数级增长值中最小的一个。FAILURE_PENALTY等1s;(1 &lt;&lt; sequentialFailures) * FAILURE_PENALTY)用于实现根据失败次数等待时间指数级递增。</p></li>
</ul>

<p>一个配置的failOver具体的例子:</p>

<pre><code class="language-groovy">  host1.sinkgroups = group1
  host1.sinkgroups.group1.sinks = sink1 sink2
  host1.sinkgroups.group1.processor.type = failover
  host1.sinkgroups.group1.processor.priority.sink1 = 5
  host1.sinkgroups.group1.processor.priority.sink2 = 10
  host1.sinkgroups.group1.processor.maxpenalty = 10000
</code></pre>

<h4 id="toc_15">LoadBalancingSinkProcessor实现分析</h4>

<p>loadBalaneingSinkProcessor 用于实现sink的负载均衡，其功能通过SinkSelector实现。类似于ChannelSelector和Channel的关系</p>

<p><img src="media/14961334971521/14965813235880.jpg" alt="" style="width:384px;"/></p>

<p><img src="media/14961334971521/14965813925723.jpg" alt="" style="width:618px;"/></p>

<p>SinkSelector中模式有三种实现<br/>
 1.固定顺序<br/>
 2.轮询<br/>
 3.随机</p>

<p>LoadBalancingSinkProcessor 中使用均衡负载的方式</p>

<pre><code class="language-java">  @Override
  public Status process() throws EventDeliveryException {
    Status status = null;
    Iterator&lt;Sink&gt; sinkIterator = selector.createSinkIterator();//使用sinkSelector创建Sink迭代器。三种方式有各自不同的实现
    while (sinkIterator.hasNext()) {//遍历Sink
      Sink sink = sinkIterator.next();//获取sink
      try {
        status = sink.process();//调用sink处理
        break;//如果处理成功那么本次负载均衡就算完成
      } catch (Exception ex) {
        selector.informSinkFailed(sink);//如果发生异常则通知SinkSelector，采用相应的补偿算法进行处理
        LOGGER.warn(&quot;Sink failed to consume event. &quot;
            + &quot;Attempting next sink if available.&quot;, ex);
      }
    }
    if (status == null) {
      throw new EventDeliveryException(&quot;All configured sinks have failed&quot;);
    }
    return status;
  }
</code></pre>

<p>在上面的解释中，最大的两个疑惑就是</p>

<ul>
<li>这个Sink迭代器也就是createSinkIterator() 是如何实现的</li>
<li><p>发生异常后SinkSelector的处理是如何实现的</p>

<p>先来看createSinkIterator 的实现。首先看RoundRobinSinkSelector的实现</p>

<p><img src="media/14961334971521/14965819912139.jpg" alt="" style="width:256px;"/></p>

<p>如上图所示RoundRobinSinkSelector 内部包含一个OrderSelector的属性。</p>

<pre><code class="language-java">private OrderSelector&lt;Sink&gt; selector;

RoundRobinSinkSelector(boolean backoff){
  selector = new RoundRobinOrderSelector&lt;Sink&gt;(backoff);
}

@Override
public Iterator&lt;Sink&gt; createSinkIterator() {
  return selector.createIterator();
}
</code></pre>

<p>内部通过一个RoundRobinOrderSelector 来实现。查看起createIterator实现</p>

<pre><code class="language-java">@Override
public Iterator&lt;T&gt; createIterator() {
List&lt;Integer&gt; activeIndices = getIndexList();//获取存活sink的索引
int size = activeIndices.size();//存活sink的個數
//如果下一個sink的位置超過了存活sin的個數，重新指向头
if (nextHead &gt;= size) {
  nextHead = 0;
}
int begin = nextHead++; //获取起始位置
if (nextHead == activeIndices.size()) {//检查是否超过范围，超过了从头开始
  nextHead = 0;
}
int[] indexOrder = new int[size];//创建一个数组，来存放访问的顺序
for (int i = 0; i &lt; size; i++) {
  indexOrder[i] = activeIndices.get((begin + i) % size);//用取模的方法实现轮询，每次都从上一个sink的下一个sink 索引开始，由begin控制
}
//indexOrder 是访问顺序，getObjects返回相关所有的sink
return new SpecificOrderIterator&lt;T&gt;(indexOrder, getObjects());
}
</code></pre>

<p>接下来看一下getIndexList 的实现</p>

<pre><code class="language-java">protected List&lt;Integer&gt; getIndexList() {
long now = System.currentTimeMillis();//当前时间

List&lt;Integer&gt; indexList = new ArrayList&lt;Integer&gt;();//用来存放sink的索引

int i = 0;
for (T obj : stateMap.keySet()) {//获取所有sink
  if (!isShouldBackOff() || stateMap.get(obj).restoreTime &lt; now) {
     //如果没有开启退避，或者该sink 到失败补偿的时间，那么将改sink的索引放入IndexList
    indexList.add(i);
  }
  i++;
}
return indexList;
}
</code></pre>

<p>stateMap是一个LinkedHashMap<T,FailState>其中T在这里指的是Sink。<br/>
如果没有开启了退避算法，那么会认为每个sink都是存活的，所有的sink都加到IndexList。否则等到了失败补偿时间才会加入到IndexList。可以通过processor.backoff = true配置开启</p>

<p>最后分析一下当sink处理失败SinkSelector是如何处理的</p>

<pre><code class="language-java">public void informFailure(T failedObject) {
if (!shouldBackOff) {//如果没有开启退避算法，当然就不做任何处理
  return;
}
FailureState state = stateMap.get(failedObject);//获取当前失败sink的状态对象
long now = System.currentTimeMillis();//当前时间
long delta = now - state.lastFail;//自从上次失败的经过的时间
long lastBackoffLength = Math.min(maxTimeout, 1000 * (1 &lt;&lt; state.sequentialFails));//计算上一次退避等待的时间
long allowableDiff = lastBackoffLength + CONSIDER_SEQUENTIAL_RANGE;
if (allowableDiff &gt; delta) {//如果上次失败到现在最后退避时间后的一个小时内，并且是失败次数小于期望的退避次数限制，那么就增加state.sequentialFails 实际上就增加了退避的等待时间
  if (state.sequentialFails &lt; EXP_BACKOFF_COUNTER_LIMIT) {
    state.sequentialFails++;
  }
} else {
  state.sequentialFails = 1;//否则就不再增加退避等待时间
}
state.lastFail = now;//更新最后失败时间
state.restoreTime = now + Math.min(maxTimeout, 1000 * (1 &lt;&lt; state.sequentialFails));//更新退避等待时间
}
</code></pre>

<p>CONSIDER_SEQUENTIAL_RANGE 是一个常量 只为1小时，EXP_BACKOFF_COUNTER_LIMIT 为期望最大的退避次数 值为16.如果上次失败到现在的是哪在上次退避等待时间超过一个小时后 或者 退避次数超过了EXP_BACKOFF_COUNTER_LIMIT 那么退避的等待时间将不再增加。</p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[flume-ng源码分析-整体架构2【常用架构篇】]]></title>
    <link href="http://www.blacklight.xin/14961128486144.html"/>
    <updated>2017-05-30T10:54:08+08:00</updated>
    <id>http://www.blacklight.xin/14961128486144.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">数据流模型</h2>

<p><img src="media/14961128486144/14961138534005.jpg" alt=""/></p>

<span id="more"></span><!-- more -->

<p><strong>flume 中基本概念</strong> </p>

<p>1.Event:一个流经Agent的基本数据单位;Event从Source流向Channel最后到Sink。实现了Event接口。在Event的流向中,可以设置可选的header参数。<br/>
2.Agent:一个Agent表示一个拥有一些组件(source,channel,sink)的jvm进程。这些组件允许，Events从一个外部源,流向下一个目的地。<br/>
3.flow:表示一个数据流向<br/>
4.source:source 消费可以识别格式的Events。这些Events可以通过像web server的客户端发送。例如arvoSource 可以从client或者其他flume agent 接受arvoEvents.当Source接受到Events时，将它存储在一个或者多个Channel中<br/>
5.Channel:是一个被动存储。他会存储接收到的Events直到这些Events被Sink消费掉。比如fileChannel，用贝本地文件系统作为后备存储；<br/>
6.Sink:Sink消费channel中的数据。sink负责将Event从channel中移除，并且将Events放在外部存储如HDFS(这可以通过HDFSSINK实现)或者传送给下一个Flume agent中的source。因为Flume中有Channel的存在,在一个给定的Agent中source和sink可以异步的执行</p>

<h2 id="toc_1">简单的日志收集</h2>

<p>假设我们要收集ng的日志。我们可以按照以下的方案进行部署。</p>

<p><img src="media/14961128486144/14961161955654.jpg" alt=""/></p>

<p><strong>流程如下</strong></p>

<p>1.每台websever 上部署一个flume agent<br/>
2.使用tail 命令<br/>
3.channel可以使用memorychannel<br/>
4.sink统一写到es服务器中。前端使用kibana查询</p>

<p>虽然上面的应用场景可以满足需求。但是缺点也是非常的明显</p>

<p>1.各环节丢失数据的可能性较大（如果可以容忍数据丢失，则关系不大）<br/>
2.每台webserver上部署一个flume agent，不利于维护。比如 sink还想往kafka写，那么所有的flume agent都需要更改。</p>

<h2 id="toc_2">复杂的日志收集</h2>

<p>先来看看Event在flume flow中的扭转流程:</p>

<p>1.source 接收Event<br/>
2.source 将Event传送给<strong>ChannelProcessor</strong><br/>
3.在ChannelProcessor中收件会将event传递给<strong>InterceptorChain</strong>,InterceptorChain中包含多个Interceptor。Interceptor的概念就相当于java web开发中的servlet的概念。提供了一种修改或者删除Event的能力.比如Timestamp Interceptor 将会在Event的header中加入Event被处理的时间戳,key为timestamp。<br/>
4.当Event被Interceptor处理后就会通过ChannelSelector 选择合适的channel，将Event发送到Channel中。<br/>
flume中提供了两种方式 :<br/>
* <strong>MultiplexingChannelSelector</strong> 多路复用选择器<br/>
* <strong>ReplicatingChannelSelector</strong> 复制选择器</p>

<p>5.sink从channel中消费数据，这里和source向channel中存放数据是异步的。所以sink，只需要监听和自己关联的channel的变化即可。对于sink,提供了三种策略:</p>

<ul>
<li><strong>DefaultSinkProcess</strong> ：失败了就失败了，稍后进行重试</li>
<li><strong>LoadBalancingSinkProcessor</strong>:负载均衡，有RandomOrder,RoundRobin和FixedOrderSelector三种选择</li>
<li><strong>FailoverSinkProcessor</strong>:给多个sink定义优先级，如果其中一个失败了，则发送到下一个优先级的Sink。如果执行过程中Sink抛出异常，那么将该Sink从存活的队列中移除。然后指数级时间重试。默认开始等待1s重试。最大等待时间是30s。当Sink恢复后将会加入存活的队列中。</li>
</ul>

<p>基于上面分析，可以基本画出flume event的基本流转</p>

<p><img src="media/14961128486144/14961302820778.jpg" alt=""/></p>

<p>基于第一种方式的搜集方式和Event数据流转的分析。如果我们需要采集日志，整体架构可以采用下面的方式进行部署。</p>

<p><img src="media/14961128486144/14961289968437.jpg" alt=""/></p>

<ul>
<li>在收集层，agent和web server部署在同一台机器上。(这里我们也可以开发flume的Embedded agent)通过RPC将数据流入聚合层。这一层应该快速的将日志收集到聚合层。</li>
<li>聚合层进行日志数据的聚合和收集，在这一层，可以做容错处理，如负债均衡或者failover.以提升可靠性。在这一层，数据量大时可以打开fileChannel，作为数据缓冲区，避免数据的丢失。以后主要的维护工作也主要在这一层上面。</li>
<li>在存储层，一般会流向hdfs，kafka 以供离线和实时的数据分析。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[flume-ng 源码分析-整体架构1【启动篇】]]></title>
    <link href="http://www.blacklight.xin/14953349711216.html"/>
    <updated>2017-05-21T10:49:31+08:00</updated>
    <id>http://www.blacklight.xin/14953349711216.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">什么是flume</a>
</li>
<li>
<a href="#toc_1">flume源码结构</a>
</li>
<li>
<a href="#toc_3">flume启动脚本flume-ng分析</a>
</li>
<li>
<a href="#toc_4">agent的启动分析Application.java</a>
</li>
<li>
<a href="#toc_5">配置载入分析</a>
</li>
<li>
<a href="#toc_6">flume如何获自定义的key</a>
</li>
<li>
<a href="#toc_7">总结</a>
</li>
</ul>


<h2 id="toc_0">什么是flume</h2>

<p>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。flume常用场景:log--&gt;flume--&gt;[hdfs,hbase,kafka],收集日志并落地到各种不同的存储，以供不同需求的计算。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_1">flume源码结构</h2>

<p><img src="media/14953349711216/14953354121029.jpg" alt="" style="width:422px;"/></p>

<blockquote>
<p>主要模块介绍</p>
</blockquote>

<ul>
<li><p><strong>flume-ng-core</strong></p>

<p>flume的整个核心框架，包含了各个模块的接口以及逻辑关系实现。core下大部分代码都是source，channle，sink中</p></li>
<li><p><strong>flume-ng-channels</strong></p>

<p>里面包含了fileChannel,jdbcChannel,kafkaChannel,spillableMemoryChannel等通道实现</p></li>
<li><p><strong>flume-ng-sinks</strong></p>

<p>各种sink的实现,包括但不限于:hdfsSink,hiveSink,esSink,kafkaSink</p></li>
<li><p><strong>flume-ng-sources</strong></p>

<p>各种source的实现,包括但不限于:   jms,kafka,scirbe,twitter.其他source则在flume-ng-core模块中</p></li>
<li><p><strong>flume-ng-node</strong></p>

<p>实现flume的一些基本类。包括agent的main(Application).这也是我们的分析代码的入口类</p>

<h2 id="toc_2">flume 逻辑结构</h2>

<p><img src="media/14953349711216/14953368783609.jpg" alt="" style="width:487px;"/></p></li>
</ul>

<p>一个agent包含三个基本组件</p>

<ul>
<li>sourace</li>
<li>channel</li>
<li>sink</li>
</ul>

<h2 id="toc_3">flume启动脚本flume-ng分析</h2>

<pre><code>######################################################################
# constants flume常量的设定，不通环境执行不同的类
######################################################################

FLUME_AGENT_CLASS=&quot;org.apache.flume.node.Application&quot;
FLUME_AVRO_CLIENT_CLASS=&quot;org.apache.flume.client.avro.AvroCLIClient&quot;
FLUME_VERSION_CLASS=&quot;org.apache.flume.tools.VersionInfo&quot;
FLUME_TOOLS_CLASS=&quot;org.apache.flume.tools.FlumeToolsMain&quot;
</code></pre>

<pre><code>######################################################################
#真正启动flume,具体由$FLUME_APPLICATON_CLASS指定
######################################################################
run_flume() {
  local FLUME_APPLICATION_CLASS

  if [ &quot;$#&quot; -gt 0 ]; then
    FLUME_APPLICATION_CLASS=$1
    shift
  else
    error &quot;Must specify flume application class&quot; 1
  fi

  if [ ${CLEAN_FLAG} -ne 0 ]; then
    set -x
  fi
  $EXEC $JAVA_HOME/bin/java $JAVA_OPTS $FLUME_JAVA_OPTS &quot;${arr_java_props[@]}&quot; -cp &quot;$FLUME_CLASSPATH&quot; \
      -Djava.library.path=$FLUME_JAVA_LIBRARY_PATH &quot;$FLUME_APPLICATION_CLASS&quot; $*
}
</code></pre>

<pre><code>##################################################
# main 启动过程中用到的变量，都可以在启动的时指定
# 如果不设置java堆空间大小，默认大小为20M,可以在flume.sh
# 中进行设置
##################################################

# set default params
FLUME_CLASSPATH=&quot;&quot;
FLUME_JAVA_LIBRARY_PATH=&quot;&quot;
JAVA_OPTS=&quot;-Xmx20m&quot;
LD_LIBRARY_PATH=&quot;&quot;

opt_conf=&quot;&quot;
opt_classpath=&quot;&quot;
opt_plugins_dirs=&quot;&quot;
arr_java_props=()
arr_java_props_ct=0
opt_dryrun=&quot;&quot;

mode=$1
shift
</code></pre>

<pre><code>##################################################
#最后根据不同参数启动不同的类，可以看到启动agent时,
#执行的是flume-ng-node中Applicaton.java
# finally, invoke the appropriate command
##################################################
if [ -n &quot;$opt_agent&quot; ] ; then
  run_flume $FLUME_AGENT_CLASS $args
elif [ -n &quot;$opt_avro_client&quot; ] ; then
  run_flume $FLUME_AVRO_CLIENT_CLASS $args
elif [ -n &quot;${opt_version}&quot; ] ; then
  run_flume $FLUME_VERSION_CLASS $args
elif [ -n &quot;${opt_tool}&quot; ] ; then
  run_flume $FLUME_TOOLS_CLASS $args
else
  error &quot;This message should never appear&quot; 1
fi
</code></pre>

<h2 id="toc_4">agent的启动分析Application.java</h2>

<p>从上面的分析可以知道当我们启动一个Agent时，执行的是org.apache.flume.node.Application.</p>

<p>看main函数的源码</p>

<pre><code class="language-java"> Options options = new Options();

      Option option = new Option(&quot;n&quot;, &quot;name&quot;, true, &quot;the name of this agent&quot;);
      option.setRequired(true);
      options.addOption(option);

      option = new Option(&quot;f&quot;, &quot;conf-file&quot;, true,
          &quot;specify a config file (required if -z missing)&quot;);
      option.setRequired(false);
      options.addOption(option);

      option = new Option(null, &quot;no-reload-conf&quot;, false,
          &quot;do not reload config file if changed&quot;);
      options.addOption(option);

      // Options for Zookeeper
      option = new Option(&quot;z&quot;, &quot;zkConnString&quot;, true,
          &quot;specify the ZooKeeper connection to use (required if -f missing)&quot;);
      option.setRequired(false);
      options.addOption(option);

      option = new Option(&quot;p&quot;, &quot;zkBasePath&quot;, true,
          &quot;specify the base path in ZooKeeper for agent configs&quot;);
      option.setRequired(false);
      options.addOption(option);

      option = new Option(&quot;h&quot;, &quot;help&quot;, false, &quot;display help text&quot;);
      options.addOption(option);

      CommandLineParser parser = new GnuParser();
      CommandLine commandLine = parser.parse(options, args);

      if (commandLine.hasOption(&#39;h&#39;)) {
        new HelpFormatter().printHelp(&quot;flume-ng agent&quot;, options, true);
        return;
      }

      String agentName = commandLine.getOptionValue(&#39;n&#39;);
      boolean reload = !commandLine.hasOption(&quot;no-reload-conf&quot;);
      
</code></pre>

<p>主要是对名利行参数的校验和解析<br/>
在我们启动Agent时，会指定，-n -f等一些参数<br/>
继续往下看</p>

<pre><code class="language-java">//是否包含zk配置
if (commandLine.hasOption(&#39;z&#39;) || commandLine.hasOption(&quot;zkConnString&quot;)) {
        isZkConfigured = true;
      }
      Application application = null;
      if (isZkConfigured) {
        // get options
        String zkConnectionStr = commandLine.getOptionValue(&#39;z&#39;);
        String baseZkPath = commandLine.getOptionValue(&#39;p&#39;);

        if (reload) {//如果是需要重新加载（配置文件改变时）
          EventBus eventBus = new EventBus(agentName + &quot;-event-bus&quot;);
          List&lt;LifecycleAware&gt; components = Lists.newArrayList();
          PollingZooKeeperConfigurationProvider zookeeperConfigurationProvider =
            new PollingZooKeeperConfigurationProvider(
              agentName, zkConnectionStr, baseZkPath, eventBus);
          components.add(zookeeperConfigurationProvider);
          application = new Application(components);
          eventBus.register(application);
        } else {//不需要检车配置文件的变更
          StaticZooKeeperConfigurationProvider zookeeperConfigurationProvider =
            new StaticZooKeeperConfigurationProvider(
              agentName, zkConnectionStr, baseZkPath);
          application = new Application();
          application.handleConfigurationEvent(zookeeperConfigurationProvider
            .getConfiguration());
        }
      }
</code></pre>

<p>从以上代码我们可以看出，当配置文件是配置的是zk上的路径时，如果需要reload，则会启动PollingZooKeeperConfigurationProvider，该类里面会监听zk的变化，再通过guava的EventBus(类似于观察者模式，<a href="https://github.com/google/guava/wiki/EventBusExplained">EventBus</a>)，传递消息.</p>

<p><u><strong>注意</strong></u><br/>
   此时只是将PollingZooKeeperConfigurationProvider加入components中,并没有正真的启动</p>

<pre><code class="language-java">   private final List&lt;LifecycleAware&gt; components;
</code></pre>

<p>PollingZooKeeperConfigurationProvider 部分关键代码</p>

<pre><code class="language-java">try {
        agentNodeCache = new NodeCache(client, basePath + &quot;/&quot; + getAgentName());
        agentNodeCache.start();
        agentNodeCache.getListenable().addListener(new NodeCacheListener() {
          @Override
          public void nodeChanged() throws Exception {
            refreshConfiguration();
          }
        });
      } catch (Exception e) {
        client.close();
        throw e;
      }
</code></pre>

<p>在zk node上设置listener，如果zk node有任何的变化则会触发refreshConfiguration方法</p>

<pre><code class="language-java">private void refreshConfiguration() throws IOException {
    LOGGER.info(&quot;Refreshing configuration from ZooKeeper&quot;);
    byte[] data = null;
    ChildData childData = agentNodeCache.getCurrentData();
    if (childData != null) {
      data = childData.getData();
    }
    flumeConfiguration = configFromBytes(data);
    //发送时间消息，所有注册到该eventBus上的handler都会收到该事件
    eventBus.post(getConfiguration());
  }
</code></pre>

<hr/>

<p>好了我们继续分析Application的代码。上面讲到了利用zk来做flume配置文件的代码。当然flume也支持本地文件的方式。代码如下：</p>

<pre><code class="language-java">File configurationFile = new File(commandLine.getOptionValue(&#39;f&#39;));

        /*
         * The following is to ensure that by default the agent will fail on
         * startup if the file does not exist.
         */
        if (!configurationFile.exists()) {
          // If command line invocation, then need to fail fast
          if (System.getProperty(Constants.SYSPROP_CALLED_FROM_SERVICE) ==
            null) {
            String path = configurationFile.getPath();
            try {
              path = configurationFile.getCanonicalPath();
            } catch (IOException ex) {
              logger.error(&quot;Failed to read canonical path for file: &quot; + path,
                ex);
            }
            throw new ParseException(
              &quot;The specified configuration file does not exist: &quot; + path);
          }
        }
        List&lt;LifecycleAware&gt; components = Lists.newArrayList();

        if (reload) {
          EventBus eventBus = new EventBus(agentName + &quot;-event-bus&quot;);
          PollingPropertiesFileConfigurationProvider configurationProvider =
            new PollingPropertiesFileConfigurationProvider(
              agentName, configurationFile, eventBus, 30);
          components.add(configurationProvider);
          application = new Application(components);
          eventBus.register(application);
        } else {
          PropertiesFileConfigurationProvider configurationProvider =
            new PropertiesFileConfigurationProvider(
              agentName, configurationFile);
          application = new Application();
          application.handleConfigurationEvent(configurationProvider
            .getConfiguration());
        }
      }
</code></pre>

<p>如果-f 指定的配置文件不存在，那么将快速失败，抛出异常。<br/>
再判断配置文件发生改变时是否需要重新reload，套路和用zk保存配置文件一个道理<br/>
如果需要动态加载配置文件，那么启动PollingPropertiesFileConfigurationProvider，每三十秒<br/>
加载一次配置文件 </p>

<hr/>

<p>之后执行application.start()方法。让我们继续看start()方法</p>

<pre><code class="language-java">private final LifecycleSupervisor supervisor;
public synchronized void start() {
    for(LifecycleAware component : components) {
      supervisor.supervise(component,
          new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);
    }
  }
</code></pre>

<p>在start方法中遍历compents 执行supervisor.suervise()方法.</p>

<p>在继续分析之前我们先看一下LifecycleSupervisor,PollingPropertiesFileConfigurationProvider 的类结构</p>

<p><img src="media/14953349711216/14953484325272.jpg" alt="" style="width:517px;"/></p>

<p><img src="media/14953349711216/14953484595334.jpg" alt="" style="width:291px;"/></p>

<p>从以上两图中可以看出它们都实现了LifecycleAware接口。这个接口定义了flume组件的生命周期。LifecycleSupervisor提供了实现。</p>

<p>LifecycleAware.java</p>

<pre><code class="language-java">/**
   * &lt;p&gt;
   * Starts a service or component.
   * &lt;/p&gt;
   * &lt;p&gt;
   * Implementations should determine the result of any start logic and effect
   * the return value of {@link #getLifecycleState()} accordingly.
   * &lt;/p&gt;
   *
   * @throws LifecycleException
   * @throws InterruptedException
   */
  public void start();

  /**
   * &lt;p&gt;
   * Stops a service or component.
   * &lt;/p&gt;
   * &lt;p&gt;
   * Implementations should determine the result of any stop logic and effect
   * the return value of {@link #getLifecycleState()} accordingly.
   * &lt;/p&gt;
   *
   * @throws LifecycleException
   * @throws InterruptedException
   */
  public void stop();

  /**
   * &lt;p&gt;
   * Return the current state of the service or component.
   * &lt;/p&gt;
   */
  public LifecycleState getLifecycleState();
</code></pre>

<p>让我们继续分析LifecycleSupervisor.supervise()方法</p>

<pre><code class="language-java">public synchronized void supervise(LifecycleAware lifecycleAware,
      SupervisorPolicy policy, LifecycleState desiredState) {
    if(this.monitorService.isShutdown()
        || this.monitorService.isTerminated()
        || this.monitorService.isTerminating()){
      throw new FlumeException(&quot;Supervise called on &quot; + lifecycleAware + &quot; &quot; +
          &quot;after shutdown has been initiated. &quot; + lifecycleAware + &quot; will not&quot; +
          &quot; be started&quot;);
    }

    Preconditions.checkState(!supervisedProcesses.containsKey(lifecycleAware),
        &quot;Refusing to supervise &quot; + lifecycleAware + &quot; more than once&quot;);

    if (logger.isDebugEnabled()) {
      logger.debug(&quot;Supervising service:{} policy:{} desiredState:{}&quot;,
          new Object[] { lifecycleAware, policy, desiredState });
    }

    Supervisoree process = new Supervisoree();
    process.status = new Status();

    process.policy = policy;
    process.status.desiredState = desiredState;
    process.status.error = false;

    MonitorRunnable monitorRunnable = new MonitorRunnable();
    monitorRunnable.lifecycleAware = lifecycleAware;
    monitorRunnable.supervisoree = process;
    monitorRunnable.monitorService = monitorService;

    supervisedProcesses.put(lifecycleAware, process);

    ScheduledFuture&lt;?&gt; future = monitorService.scheduleWithFixedDelay(
        monitorRunnable, 0, 3, TimeUnit.SECONDS);
    monitorFutures.put(lifecycleAware, future);
  }
</code></pre>

<p>在上面的代码中创建了一个MonitorRunnable对象,通过jdk的scheduleWithFixedDelay进行定时调用,每次执行完成延迟3秒调度。</p>

<p>再看monitorRunable中的内容 <br/>
run 方法中部分内容</p>

<pre><code class="language-java">if (!lifecycleAware.getLifecycleState().equals(
              supervisoree.status.desiredState)) {

            logger.debug(&quot;Want to transition {} from {} to {} (failures:{})&quot;,
                new Object[] { lifecycleAware, supervisoree.status.lastSeenState,
                    supervisoree.status.desiredState,
                    supervisoree.status.failures });

            switch (supervisoree.status.desiredState) {
              case START:
                try {
                  lifecycleAware.start();
                } catch (Throwable e) {
                  logger.error(&quot;Unable to start &quot; + lifecycleAware
                      + &quot; - Exception follows.&quot;, e);
                  if (e instanceof Error) {
                    // This component can never recover, shut it down.
                    supervisoree.status.desiredState = LifecycleState.STOP;
                    try {
                      lifecycleAware.stop();
                      logger.warn(&quot;Component {} stopped, since it could not be&quot;
                          + &quot;successfully started due to missing dependencies&quot;,
                          lifecycleAware);
                    } catch (Throwable e1) {
                      logger.error(&quot;Unsuccessful attempt to &quot;
                          + &quot;shutdown component: {} due to missing dependencies.&quot;
                          + &quot; Please shutdown the agent&quot;
                          + &quot;or disable this component, or the agent will be&quot;
                          + &quot;in an undefined state.&quot;, e1);
                      supervisoree.status.error = true;
                      if (e1 instanceof Error) {
                        throw (Error) e1;
                      }
                      // Set the state to stop, so that the conf poller can
                      // proceed.
                    }
                  }
                  supervisoree.status.failures++;
                }
                break;
              case STOP:
                try {
                  lifecycleAware.stop();
                } catch (Throwable e) {
                  logger.error(&quot;Unable to stop &quot; + lifecycleAware
                      + &quot; - Exception follows.&quot;, e);
                  if (e instanceof Error) {
                    throw (Error) e;
                  }
                  supervisoree.status.failures++;
                }
                break;
              default:
                logger.warn(&quot;I refuse to acknowledge {} as a desired state&quot;,
                    supervisoree.status.desiredState);
            }

            if (!supervisoree.policy.isValid(lifecycleAware, supervisoree.status)) {
              logger.error(
                  &quot;Policy {} of {} has been violated - supervisor should exit!&quot;,
                  supervisoree.policy, lifecycleAware);
            }
          }
</code></pre>

<p>首先因为monitorRunnbale对象时重复调用的，所以在run方法中作了一个状态判断，当该组件的状态不等于期望的状态时继续往下执行，否则什么都不做。这样避免重复启动。当组件第一次被启动的时候，组件本身的状态是IDEL，而desired state 是START，此时就会执行组件的start方法。</p>

<p>总结一下启动的时序图</p>

<p><img src="media/14953349711216/14953507745459.jpg" alt="" style="width:741px;"/></p>

<p>比如启动PollingPropertiesFileConfigurationProvider组件，这个组件的作用就是定时去获取flume的配置。那么会调用PollingPropertiesFileConfigurationProvider的start方法。</p>

<p>下面以PollingPropertiesFileConfigurationProvider为列 分析flume的配置时如何动态载入的。</p>

<h2 id="toc_5">配置载入分析</h2>

<p>从上面分析得知，启动PollingPropertiesFileConfigurationProvider ,则执行该组件的start方法。查看start方法如下</p>

<pre><code> @Override
  public void start() {
    LOGGER.info(&quot;Configuration provider starting&quot;);

    Preconditions.checkState(file != null,
        &quot;The parameter file must not be null&quot;);

    executorService = Executors.newSingleThreadScheduledExecutor(
            new ThreadFactoryBuilder().setNameFormat(&quot;conf-file-poller-%d&quot;)
                .build());

    FileWatcherRunnable fileWatcherRunnable =
        new FileWatcherRunnable(file, counterGroup);

    executorService.scheduleWithFixedDelay(fileWatcherRunnable, 0, interval,
        TimeUnit.SECONDS);

    lifecycleState = LifecycleState.START;

    LOGGER.debug(&quot;Configuration provider started&quot;);
  }
</code></pre>

<p>在start方法中单独启动一个线程，执行FileWatcherRunnable,并设置状态为START</p>

<p>继续看fileWatcher</p>

<pre><code class="language-java">public void run() {
      LOGGER.debug(&quot;Checking file:{} for changes&quot;, file);

      counterGroup.incrementAndGet(&quot;file.checks&quot;);

      long lastModified = file.lastModified();

      if (lastModified &gt; lastChange) {
        LOGGER.info(&quot;Reloading configuration file:{}&quot;, file);

        counterGroup.incrementAndGet(&quot;file.loads&quot;);

        lastChange = lastModified;

        try {
          eventBus.post(getConfiguration());
        } catch (Exception e) {
          LOGGER.error(&quot;Failed to load configuration data. Exception follows.&quot;,
              e);
        } catch (NoClassDefFoundError e) {
          LOGGER.error(&quot;Failed to start agent because dependencies were not &quot; +
              &quot;found in classpath. Error follows.&quot;, e);
        } catch (Throwable t) {
          // caught because the caller does not handle or log Throwables
          LOGGER.error(&quot;Unhandled error&quot;, t);
        }
      }
    }
</code></pre>

<p>在fileWatcher中通过对文件修改时间来判断配置文件是否发生变化。如果配置文件发生变化<br/>
调用<strong>eventBus.post(getConfiguration());</strong> 将配置文件的内容发布。</p>

<p>在Application.java 中有如下代码</p>

<pre><code class="language-java">@Subscribe
  public synchronized void handleConfigurationEvent(MaterializedConfiguration conf) {
    stopAllComponents();
    startAllComponents(conf);
  }
</code></pre>

<p>此方法订阅了eventBus的消息。当一有消息将会触发该方法，此方法的功能相当于重启flume组件。还记得上面分析的代码吗？要是用户配置no-reload-conf 那么将会直接调用该方法。</p>

<p>那么getConfiguration()方法是如何实现的呢？</p>

<pre><code class="language-java">protected abstract FlumeConfiguration getFlumeConfiguration();
public MaterializedConfiguration getConfiguration() {
    MaterializedConfiguration conf = new SimpleMaterializedConfiguration();
    FlumeConfiguration fconfig = getFlumeConfiguration();
    AgentConfiguration agentConf = fconfig.getConfigurationFor(getAgentName());
    if (agentConf != null) {
      Map&lt;String, ChannelComponent&gt; channelComponentMap = Maps.newHashMap();
      Map&lt;String, SourceRunner&gt; sourceRunnerMap = Maps.newHashMap();
      Map&lt;String, SinkRunner&gt; sinkRunnerMap = Maps.newHashMap();
      try {
        loadChannels(agentConf, channelComponentMap);
        loadSources(agentConf, channelComponentMap, sourceRunnerMap);
        loadSinks(agentConf, channelComponentMap, sinkRunnerMap);
        Set&lt;String&gt; channelNames =
            new HashSet&lt;String&gt;(channelComponentMap.keySet());
        for(String channelName : channelNames) {
          ChannelComponent channelComponent = channelComponentMap.
              get(channelName);
          if(channelComponent.components.isEmpty()) {
            LOGGER.warn(String.format(&quot;Channel %s has no components connected&quot; +
                &quot; and has been removed.&quot;, channelName));
            channelComponentMap.remove(channelName);
            Map&lt;String, Channel&gt; nameChannelMap = channelCache.
                get(channelComponent.channel.getClass());
            if(nameChannelMap != null) {
              nameChannelMap.remove(channelName);
            }
          } else {
            LOGGER.info(String.format(&quot;Channel %s connected to %s&quot;,
                channelName, channelComponent.components.toString()));
            conf.addChannel(channelName, channelComponent.channel);
          }
        }
        for(Map.Entry&lt;String, SourceRunner&gt; entry : sourceRunnerMap.entrySet()) {
          conf.addSourceRunner(entry.getKey(), entry.getValue());
        }
        for(Map.Entry&lt;String, SinkRunner&gt; entry : sinkRunnerMap.entrySet()) {
          conf.addSinkRunner(entry.getKey(), entry.getValue());
        }
      } catch (InstantiationException ex) {
        LOGGER.error(&quot;Failed to instantiate component&quot;, ex);
      } finally {
        channelComponentMap.clear();
        sourceRunnerMap.clear();
        sinkRunnerMap.clear();
      }
    } else {
      LOGGER.warn(&quot;No configuration found for this host:{}&quot;, getAgentName());
    }
    return conf;
  }
</code></pre>

<p>getConfiguration()中调用了getFlumeConfiguration()方法;getFlumeConfiguration() 是一个抽象方法，以PollingPropertiesFileConfigurationProvider 实现为列 。该实现在父类中。</p>

<pre><code>@Override
  public FlumeConfiguration getFlumeConfiguration() {
    BufferedReader reader = null;
    try {
      reader = new BufferedReader(new FileReader(file));
      Properties properties = new Properties();
      properties.load(reader);
      return new FlumeConfiguration(toMap(properties));
    } catch (IOException ex) {
      LOGGER.error(&quot;Unable to load file:&quot; + file
          + &quot; (I/O failure) - Exception follows.&quot;, ex);
    } finally {
      if (reader != null) {
        try {
          reader.close();
        } catch (IOException ex) {
          LOGGER.warn(
              &quot;Unable to close file reader for file: &quot; + file, ex);
        }
      }
    }
    return new FlumeConfiguration(new HashMap&lt;String, String&gt;());
  }
</code></pre>

<p>该方法通过基本的流加载方法返回FlumeConfigruation对象。该对象封装一个Map对象<br/>
。在FlumeConfigruation的构造函数中将会遍历这个Map对象，调用addRawProperty方法<br/>
该方法首先会进行一些合法性的检查，并且该方法会创建一个AgentConfiguration对象的aoconf<br/>
该方法最后调用aconf.addProperty 方法</p>

<p>在aconf.addProperty方法中会区分source，channel，sink ，sinkgroup。将对应的配置信息放在<br/>
sourceContextMap，channelContextMap，sinkContextMap，sinkGroupContextMap。这些信息封装在AgentConfiguration，AgentConfiguration封装在FlumeConfiguration中，key是agentName。使用时可以通过getConfigurationFor(String hostname) 来获取。</p>

<h2 id="toc_6">flume如何获自定义的key</h2>

<p>在上面的分析中addProperty方法中，调用了parseConfigKey方法</p>

<pre><code>cnck = parseConfigKey(key,
          BasicConfigurationConstants.CONFIG_SINKGROUPS_PREFIX);
</code></pre>

<p>具体实现如下：</p>

<pre><code>private ComponentNameAndConfigKey parseConfigKey(String key, String prefix) {
      // key must start with prefix
      if (!key.startsWith(prefix)) {
        return null;
      }

      // key must have a component name part after the prefix of the format:
      // &lt;prefix&gt;&lt;component-name&gt;.&lt;config-key&gt;
      int index = key.indexOf(&#39;.&#39;, prefix.length() + 1);

      if (index == -1) {
        return null;
      }

      String name = key.substring(prefix.length(), index);
      String configKey = key.substring(prefix.length() + name.length() + 1);

      // name and config key must be non-empty
      if (name.length() == 0 || configKey.length() == 0) {
        return null;
      }

      return new ComponentNameAndConfigKey(name, configKey);
    }
</code></pre>

<p>上面代码中prefix为定义的常量如下：</p>

<pre><code>public final class BasicConfigurationConstants {

  public static final String CONFIG_SOURCES = &quot;sources&quot;;
  public static final String CONFIG_SOURCES_PREFIX = CONFIG_SOURCES + &quot;.&quot;;
  public static final String CONFIG_SOURCE_CHANNELSELECTOR_PREFIX = &quot;selector.&quot;;


  public static final String CONFIG_SINKS = &quot;sinks&quot;;
  public static final String CONFIG_SINKS_PREFIX = CONFIG_SINKS + &quot;.&quot;;
  public static final String CONFIG_SINK_PROCESSOR_PREFIX = &quot;processor.&quot;;

  public static final String CONFIG_SINKGROUPS = &quot;sinkgroups&quot;;
  public static final String CONFIG_SINKGROUPS_PREFIX = CONFIG_SINKGROUPS + &quot;.&quot;;

  public static final String CONFIG_CHANNEL = &quot;channel&quot;;
  public static final String CONFIG_CHANNELS = &quot;channels&quot;;
  public static final String CONFIG_CHANNELS_PREFIX = CONFIG_CHANNELS + &quot;.&quot;;

  public static final String CONFIG_CONFIG = &quot;config&quot;;
  public static final String CONFIG_TYPE = &quot;type&quot;;

  private BasicConfigurationConstants() {
    // disable explicit object creation
  }

}
</code></pre>

<ul>
<li>比如我们配置的格式是agent1.sources.source1.type=avro(注意在后面parse时，agent1.已经被截取掉)</li>
<li>在上面的parseKey方法中首先会判断prefix的后面有多少个字符</li>
<li>解析出name 。source1就是name</li>
<li>解析出configKey 。type就是configKey</li>
<li>封装为ComponentNameAndConfigKey</li>
<li>然后有上面的分析把sources、channel、sink配置信息，分别存放到sourceContextMap、channelConfigMap、sinkConfigMap三个HashMap，这些信息封装在AgentConfiguration，AgentConfiguration封装在FlumeConfiguration中，key是agentName。使用时可以通过getConfigurationFor(String hostname) 来获取</li>
</ul>

<h2 id="toc_7">总结</h2>

<p>以上分析了flume启动agent的流程。部分源码没有贴出来，可以自行阅读；以及flume中如何解析<br/>
用户自定义的source,channel,sink;以及flume如何用zk listener和fileWatcher实现配置文件的动态加载。下篇主要讲解flume整体架构--常用架构篇</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[flume 总结]]></title>
    <link href="http://www.blacklight.xin/14909422875977.html"/>
    <updated>2017-03-31T14:38:07+08:00</updated>
    <id>http://www.blacklight.xin/14909422875977.html</id>
    <content type="html"><![CDATA[
<p>这里只考虑flume本身的一些东西，对于JVM、HDFS、HBase等得暂不涉及。。。。</p>

<h2 id="toc_0">一、关于Source：</h2>

<p>1、spool-source：适合静态文件，即文件本身不是动态变化的；</p>

<p>2、avro source可以适当提高线程数量来提高此source性能；</p>

<p>3、ThriftSource在使用时有个问题需要注意，使用批量操作时出现异常并不会打印异常内容而是&quot;Thrift source %s could not append events to the channel.&quot;，这是因为源码中在出现异常时，它并未捕获异常而是获取组件名称，这是源码中的一个bug，也可以说明thrift很少有人用，否则这个问题也不会存在在很多版本中；</p>

<p>4、如果一个source对应多个channel，默认就是每个channel是同样的一份数据，会把这批数据复制N份发送到N个channel中，所以如果某个channel满了会影响整体的速度的哦；</p>

<p>5、ExecSource官方文档已经说明是异步的，可能会丢数据哦，尽量使用tail -F，注意是大写的；</p>

<span id="more"></span><!-- more -->

<h2 id="toc_1">二、关于Channel：</h2>

<p>1、采集节点建议使用新的复合类型的SpillableMemoryChannel，汇总节点建议采用memory channel，具体还要看实际的数据量，一般每分钟数据量超过120MB大小的flume agent都建议用memory channel(自己测的file channel处理速率大概是2M/s，不同机器、不同环境可能不同，这里只提供参考)，因为一旦此agent的channel出现溢出情况，将会导致大多数时间处于file channel(SpillableMemoryChannel本身是file channel的一个子类，而且复合channel会保证一定的event的顺序的使得读完内存中的数据后，再需要把溢出的拿走，可能这时内存已满又会溢出。。。)，性能大大降低，汇总一旦成为这样后果可想而知；</p>

<p>2、调整memory 占用物理内存空间，需要两个参数byteCapacityBufferPercentage(默认是20)和byteCapacity(默认是JVM最大可用内存的0.8)来控制，计算公式是：byteCapacity = (int)((context.getLong(&quot;byteCapacity&quot;, defaultByteCapacity).longValue() * (1 - byteCapacityBufferPercentage * .01 )) /byteCapacitySlotSize)，很明显可以调节这两个参数来控制，至于byteCapacitySlotSize默认是100，将物理内存转换成槽(slot)数，这样易于管理，但是可能会浪费空间，至少我是这样想的。。。；</p>

<p>3、还有一个有用的参数&quot;keep-alive&quot;这个参数用来控制channel满时影响source的发送，channel空时影响sink的消费，就是等待时间，默认是3s，超过这个时间就甩异常，一般不需配置，但是有些情况很有用，比如你得场景是每分钟开头集中发一次数据，这时每分钟的开头量可能比较大，后面会越来越小，这时你可以调大这个参数，不至于出现channel满了得情况；</p>

<h2 id="toc_2">三、关于Sink：</h2>

<p>1、avro sink的batch-size可以设置大一点，默认是100，增大会减少RPC次数，提高性能；</p>

<p>2、内置hdfs sink的解析时间戳来设置目录或者文件前缀非常损耗性能，因为是基于正则来匹配的，可以通过修改源码来替换解析时间功能来极大提升性能，稍后我会写一篇文章来专门说明这个问题；</p>

<p>3、RollingFileSink文件名不能自定义，而且不能定时滚动文件，只能按时间间隔滚动，可以自己定义sink，来做定时写文件；</p>

<p>4、hdfs sink的文件名中的时间戳部分不能省去，可增加前缀、后缀以及正在写的文件的前后缀等信息；&quot;hdfs.idleTimeout&quot;这个参数很有意义，指的是正在写的hdfs文件多长时间不更新就关闭文件，建议都配置上，比如你设置了解析时间戳存不同的目录、文件名，而且rollInterval=0、rollCount=0、rollSize=1000000，如果这个时间内的数据量达不到rollSize的要求而且后续的写入新的文件中了，就是一直打开，类似情景不注意的话可能很多；&quot;hdfs.callTimeout&quot;这个参数指的是每个hdfs操作(读、写、打开、关闭等)规定的最长操作时间，每个操作都会放入&quot;hdfs.threadsPoolSize&quot;指定的线程池中得一个线程来操作；</p>

<p>5、关于HBase sink(非异步hbase sink：AsyncHBaseSink)，rowkey不能自定义，而且一个serializer只能写一列，一个serializer按正则匹配多个列，性能可能存在问题，建议自己根据需求写一个hbase sink；</p>

<p>6、avro sink可以配置failover和loadbalance，所用的组件和sinkgroup中的是一样的，而且也可以在此配置压缩选项，需要在avro source中配置解压缩；</p>

<h2 id="toc_3">四、关于SinkGroup：</h2>

<p>1、不管是loadbalance或者是failover的多个sink需要共用一个channel；</p>

<p>2、loadbalance的多个sink如果都是直接输出到同一种设备，比如都是hdfs，性能并不会有明显增加，因为sinkgroup是单线程的它的process方法会轮流调用每个sink去channel中take数据，并确保处理正确，使得是顺序操作的，但是如果是发送到下一级的flume agent就不一样了，take操作是顺序的，但是下一级agent的写入操作是并行的，所以肯定是快的；</p>

<p>3、其实用loadbalance在一定意义上可以起到failover的作用，生产环境量大建议loadbalance；</p>

<h2 id="toc_4">五、关于监控monitor：</h2>

<p>1、监控我这边做得还是比较少的，但是目前已知的有以下几种吧：cloudera manager（前提是你得安装CDH版本）、ganglia(这个天生就是支持的)、http(其实就是将统计信息jmx信息，封装成json串，使用jetty展示在浏览器中而已)、再一个就是自己实现收集监控信息，自己做(可以收集http的信息或者自己实现相应的接口实现自己的逻辑，具体可以参考我以前的博客)；</p>

<p>2、简单说一下cloudera manager这种监控，最近在使用，确实很强大，可以查看实时的channel进出数据速率、channel实时容量、sink的出速率、source的入速率等等，图形化的东西确实很丰富很直观，可以提供很多flume agent整体运行情况的信息和潜在的一些信息；</p>

<h2 id="toc_5">六、关于flume启动：</h2>

<p>1、flume组件启动顺序：channels——&gt;sinks——&gt;sources，关闭顺序：sources——&gt;sinks——&gt;channels；</p>

<p>2、自动加载配置文件功能，会先关闭所有组件，再重启所有组件；</p>

<p>3、关于AbstractConfigurationProvider中的Map<Class<? extends Channel>, Map<String, Channel>&gt; channelCache这个对象，始终存储着agent中得所有channel对象，因为在动态加载时，channel中可能还有未消费完的数据，但是需要对channel重新配置，所以用以来缓存channel对象的所有数据及配置信息；</p>

<p>4、通过在启动命令中添加 &quot;no-reload-conf&quot;参数为true来取消自动加载配置文件功能；</p>

<h2 id="toc_6">七、关于interceptor：</h2>

<p>请看我的关于这个组件的博客，传送门；</p>

<p>八、关于自定义组件：sink、source、channel：</p>

<p>1、channel不建议自定义哦，这个要求比较高，其他俩都是框架式的开发，往指定的方法填充自己配置、启动、关闭、业务逻辑即可，以后有机会单独写一篇文章来介绍；</p>

<p>2、关于自定义组件请相信github，上面好多好多好多，可以直接用的自定义组件....；</p>

<p>九、关于Flume-NG集群网络拓扑方案：</p>

<p>1、在每台采集节点上部署一个flume agent，然后做一到多个汇总flume agent(loadbalance)，采集只负责收集数据发往汇总，汇总可以写HDFS、HBase、spark、本地文件、kafka等等，这样一般修改会只在汇总，agent少，维护工作少；</p>

<p>2、采集节点没有部署flume agent，可能发往mongo、redis等，这时你需要自定义source或者使用sdk来将其中的数据取出并发往flume agent，这样agent就又可以充当“采集节点”或者汇总节点了，但是这样在前面相当于加了一层控制，就又多了一层风险；</p>

<p>3、由于能力有限，其它未知，上面两种，第一种好些，这里看看美团的架构———— 传送门 ；</p>

<p>东西比较简单，容易消化。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[flume 注意事项]]></title>
    <link href="http://www.blacklight.xin/14891158030188.html"/>
    <updated>2017-03-10T11:16:43+08:00</updated>
    <id>http://www.blacklight.xin/14891158030188.html</id>
    <content type="html"><![CDATA[
<p>这里只考虑flume本身的一些东西，对于JVM、HDFS、HBase等得暂不涉及。。。。</p>

<h2 id="toc_0">一、关于Source：</h2>

<p>1、spool-source：适合静态文件，即文件本身不是动态变化的；</p>

<p>2、avro source可以适当提高线程数量来提高此source性能；</p>

<p>3、ThriftSource在使用时有个问题需要注意，使用批量操作时出现异常并不会打印异常内容而是&quot;Thrift source %s could not append events to the channel.&quot;，这是因为源码中在出现异常时，它并未捕获异常而是获取组件名称，这是源码中的一个bug，也可以说明thrift很少有人用，否则这个问题也不会存在在很多版本中；</p>

<p>4、如果一个source对应多个channel，默认就是每个channel是同样的一份数据，会把这批数据复制N份发送到N个channel中，所以如果某个channel满了会影响整体的速度的哦；</p>

<p>5、ExecSource官方文档已经说明是异步的，可能会丢数据哦，尽量使用tail -F，注意是大写的；</p>

<span id="more"></span><!-- more -->

<h2 id="toc_1">二、关于Channel：</h2>

<p>1、采集节点建议使用新的复合类型的SpillableMemoryChannel，汇总节点建议采用memory channel，具体还要看实际的数据量，一般每分钟数据量超过120MB大小的flume agent都建议用memory channel(自己测的file channel处理速率大概是2M/s，不同机器、不同环境可能不同，这里只提供参考)，因为一旦此agent的channel出现溢出情况，将会导致大多数时间处于file channel(SpillableMemoryChannel本身是file channel的一个子类，而且复合channel会保证一定的event的顺序的使得读完内存中的数据后，再需要把溢出的拿走，可能这时内存已满又会溢出。。。)，性能大大降低，汇总一旦成为这样后果可想而知；</p>

<p>2、调整memory 占用物理内存空间，需要两个参数byteCapacityBufferPercentage(默认是20)和byteCapacity(默认是JVM最大可用内存的0.8)来控制，计算公式是：byteCapacity = (int)((context.getLong(&quot;byteCapacity&quot;, defaultByteCapacity).longValue() * (1 - byteCapacityBufferPercentage * .01 )) /byteCapacitySlotSize)，很明显可以调节这两个参数来控制，至于byteCapacitySlotSize默认是100，将物理内存转换成槽(slot)数，这样易于管理，但是可能会浪费空间，至少我是这样想的。。。；</p>

<p>3、还有一个有用的参数&quot;keep-alive&quot;这个参数用来控制channel满时影响source的发送，channel空时影响sink的消费，就是等待时间，默认是3s，超过这个时间就甩异常，一般不需配置，但是有些情况很有用，比如你得场景是每分钟开头集中发一次数据，这时每分钟的开头量可能比较大，后面会越来越小，这时你可以调大这个参数，不至于出现channel满了得情况；</p>

<h2 id="toc_2">三、关于Sink：</h2>

<p>1、avro sink的batch-size可以设置大一点，默认是100，增大会减少RPC次数，提高性能；</p>

<p>2、内置hdfs sink的解析时间戳来设置目录或者文件前缀非常损耗性能，因为是基于正则来匹配的，可以通过修改源码来替换解析时间功能来极大提升性能，稍后我会写一篇文章来专门说明这个问题；</p>

<p>3、RollingFileSink文件名不能自定义，而且不能定时滚动文件，只能按时间间隔滚动，可以自己定义sink，来做定时写文件；</p>

<p>4、hdfs sink的文件名中的时间戳部分不能省去，可增加前缀、后缀以及正在写的文件的前后缀等信息；&quot;hdfs.idleTimeout&quot;这个参数很有意义，指的是正在写的hdfs文件多长时间不更新就关闭文件，建议都配置上，比如你设置了解析时间戳存不同的目录、文件名，而且rollInterval=0、rollCount=0、rollSize=1000000，如果这个时间内的数据量达不到rollSize的要求而且后续的写入新的文件中了，就是一直打开，类似情景不注意的话可能很多；&quot;hdfs.callTimeout&quot;这个参数指的是每个hdfs操作(读、写、打开、关闭等)规定的最长操作时间，每个操作都会放入&quot;hdfs.threadsPoolSize&quot;指定的线程池中得一个线程来操作；</p>

<p>5、关于HBase sink(非异步hbase sink：AsyncHBaseSink)，rowkey不能自定义，而且一个serializer只能写一列，一个serializer按正则匹配多个列，性能可能存在问题，建议自己根据需求写一个hbase sink；</p>

<p>6、avro sink可以配置failover和loadbalance，所用的组件和sinkgroup中的是一样的，而且也可以在此配置压缩选项，需要在avro source中配置解压缩；</p>

<h2 id="toc_3">四、关于SinkGroup：</h2>

<p>1、不管是loadbalance或者是failover的多个sink需要共用一个channel；</p>

<p>2、loadbalance的多个sink如果都是直接输出到同一种设备，比如都是hdfs，性能并不会有明显增加，因为sinkgroup是单线程的它的process方法会轮流调用每个sink去channel中take数据，并确保处理正确，使得是顺序操作的，但是如果是发送到下一级的flume agent就不一样了，take操作是顺序的，但是下一级agent的写入操作是并行的，所以肯定是快的；</p>

<p>3、其实用loadbalance在一定意义上可以起到failover的作用，生产环境量大建议loadbalance；</p>

<h2 id="toc_4">五、关于监控monitor：</h2>

<p>1、监控我这边做得还是比较少的，但是目前已知的有以下几种吧：cloudera manager（前提是你得安装CDH版本）、ganglia(这个天生就是支持的)、http(其实就是将统计信息jmx信息，封装成json串，使用jetty展示在浏览器中而已)、再一个就是自己实现收集监控信息，自己做(可以收集http的信息或者自己实现相应的接口实现自己的逻辑，具体可以参考我以前的博客)；</p>

<p>2、简单说一下cloudera manager这种监控，最近在使用，确实很强大，可以查看实时的channel进出数据速率、channel实时容量、sink的出速率、source的入速率等等，图形化的东西确实很丰富很直观，可以提供很多flume agent整体运行情况的信息和潜在的一些信息；</p>

<h2 id="toc_5">六、关于flume启动：</h2>

<p>1、flume组件启动顺序：channels——&gt;sinks——&gt;sources，关闭顺序：sources——&gt;sinks——&gt;channels；</p>

<p>2、自动加载配置文件功能，会先关闭所有组件，再重启所有组件；</p>

<p>3、关于AbstractConfigurationProvider中的Map<Class<? extends Channel>, Map<String, Channel>&gt; channelCache这个对象，始终存储着agent中得所有channel对象，因为在动态加载时，channel中可能还有未消费完的数据，但是需要对channel重新配置，所以用以来缓存channel对象的所有数据及配置信息；</p>

<p>4、通过在启动命令中添加 &quot;no-reload-conf&quot;参数为true来取消自动加载配置文件功能；</p>

<h2 id="toc_6">七、关于interceptor：</h2>

<p>请看我的关于这个组件的博客，传送门；</p>

<h2 id="toc_7">八、关于自定义组件：sink、source、channel：</h2>

<p>1、channel不建议自定义哦，这个要求比较高，其他俩都是框架式的开发，往指定的方法填充自己配置、启动、关闭、业务逻辑即可，以后有机会单独写一篇文章来介绍；</p>

<p>2、关于自定义组件请相信github，上面好多好多好多，可以直接用的自定义组件....；</p>

<h2 id="toc_8">九、关于Flume-NG集群网络拓扑方案：</h2>

<p>1、在每台采集节点上部署一个flume agent，然后做一到多个汇总flume agent(loadbalance)，采集只负责收集数据发往汇总，汇总可以写HDFS、HBase、spark、本地文件、kafka等等，这样一般修改会只在汇总，agent少，维护工作少；</p>

<p>2、采集节点没有部署flume agent，可能发往mongo、redis等，这时你需要自定义source或者使用sdk来将其中的数据取出并发往flume agent，这样agent就又可以充当“采集节点”或者汇总节点了，但是这样在前面相当于加了一层控制，就又多了一层风险；</p>

<p>3、由于能力有限，其它未知，上面两种，第一种好些，这里看看美团的架构———— 传送门 ；</p>

<p>东西比较简单，容易消化。</p>

<p>未完，待续。。。欢迎补充</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[剖析spark-shell]]></title>
    <link href="http://www.blacklight.xin/14822419760078.html"/>
    <updated>2016-12-20T21:52:56+08:00</updated>
    <id>http://www.blacklight.xin/14822419760078.html</id>
    <content type="html"><![CDATA[
<p>我们首先来看看spark-shell 到底做了什么，spark-shell 中有一段脚本内容如下:</p>

<pre><code>function main() {
  if $cygwin; then
    # Workaround for issue involving JLine and Cygwin
    # (see http://sourceforge.net/p/jline/bugs/40/).
    # If you&#39;re using the Mintty terminal emulator in Cygwin, may need to set the
    # &quot;Backspace sends ^H&quot; setting in &quot;Keys&quot; section of the Mintty options
    # (see https://github.com/sbt/sbt/issues/562).
    stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1
    export SPARK_SUBMIT_OPTS=&quot;$SPARK_SUBMIT_OPTS -Djline.terminal=unix&quot;
    &quot;${SPARK_HOME}&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot;
    stty icanon echo &gt; /dev/null 2&gt;&amp;1
  else
    export SPARK_SUBMIT_OPTS
    &quot;${SPARK_HOME}&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot;
  fi
}
</code></pre>

<span id="more"></span><!-- more -->

<p>在上面的脚本中，实际上市执行了spark-submit,查看spark-submit代码：</p>

<pre><code>if [ -z &quot;${SPARK_HOME}&quot; ]; then
  export SPARK_HOME=&quot;$(cd &quot;`dirname &quot;$0&quot;`&quot;/..; pwd)&quot;
fi

# disable randomized hash for string in Python 3.3+
export PYTHONHASHSEED=0

exec &quot;${SPARK_HOME}&quot;/bin/spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot;
</code></pre>

<p>非常简单，执行spark-class 并传入参数.继续查看spark-class 脚本内容:</p>

<pre><code>if [ -z &quot;${SPARK_HOME}&quot; ]; then
  export SPARK_HOME=&quot;$(cd &quot;`dirname &quot;$0&quot;`&quot;/..; pwd)&quot;
fi

. &quot;${SPARK_HOME}&quot;/bin/load-spark-env.sh
</code></pre>

<blockquote>
<p>执行load-spark-env.sh 加载环境变量，稍后在讨论这个脚本</p>
</blockquote>

<pre><code>......
build_command() {
  &quot;$RUNNER&quot; -Xmx128m -cp &quot;$LAUNCH_CLASSPATH&quot; org.apache.spark.launcher.Main &quot;$@&quot;
  printf &quot;%d\0&quot; $?
}
......
CMD=()
while IFS= read -d &#39;&#39; -r ARG; do
  CMD+=(&quot;$ARG&quot;)
done &lt; &lt;(build_command &quot;$@&quot;)
......
CMD=(&quot;${CMD[@]:0:$LAST}&quot;)
exec &quot;${CMD[@]}&quot;

</code></pre>

<p>c从上面可以看出执行了org.apache.spark.launcher.Main ,继续打开org.apache.spark.launcher.Main 查看代码</p>

<pre><code>public static void main(String[] argsArray) throws Exception {
    checkArgument(argsArray.length &gt; 0, &quot;Not enough arguments: missing class name.&quot;);

    List&lt;String&gt; args = new ArrayList&lt;&gt;(Arrays.asList(argsArray));
    String className = args.remove(0);

    boolean printLaunchCommand = !isEmpty(System.getenv(&quot;SPARK_PRINT_LAUNCH_COMMAND&quot;));
    AbstractCommandBuilder builder;
    if (className.equals(&quot;org.apache.spark.deploy.SparkSubmit&quot;)) {
      try {
        builder = new SparkSubmitCommandBuilder(args);
      } catch (IllegalArgumentException e) {
        printLaunchCommand = false;
        System.err.println(&quot;Error: &quot; + e.getMessage());
        System.err.println();

        MainClassOptionParser parser = new MainClassOptionParser();
        try {
          parser.parse(args);
        } catch (Exception ignored) {
          // Ignore parsing exceptions.
        }

        List&lt;String&gt; help = new ArrayList&lt;&gt;();
        if (parser.className != null) {
          help.add(parser.CLASS);
          help.add(parser.className);
        }
        help.add(parser.USAGE_ERROR);
        builder = new SparkSubmitCommandBuilder(help);
      }
    } else {
      builder = new SparkClassCommandBuilder(className, args);
    }

    Map&lt;String, String&gt; env = new HashMap&lt;&gt;();
    List&lt;String&gt; cmd = builder.buildCommand(env);
    if (printLaunchCommand) {
      System.err.println(&quot;Spark Command: &quot; + join(&quot; &quot;, cmd));
      System.err.println(&quot;========================================&quot;);
    }

    if (isWindows()) {
      System.out.println(prepareWindowsCommand(cmd, env));
    } else {
      // In bash, use NULL as the arg separator since it cannot be used in an argument.
      List&lt;String&gt; bashCmd = prepareBashCommand(cmd, env);
      for (String c : bashCmd) {
        System.out.print(c);
        System.out.print(&#39;\0&#39;);
      }
    }
  }
</code></pre>

<p>从上面的分析我们可知,spark-submit 传递给spark-class 的参数为org.apache.spark.deploy.SparkSubmit,所以在org.apache.spark.launcher.Main 执行的应该是</p>

<pre><code>builder = new SparkSubmitCommandBuilder(args);
</code></pre>

<p>设置一些参数信息</p>

<p>继续往下执行<strong>builder.buildCommand(env);</strong> 查看 buildCommand 内容</p>

<pre><code>@Override
  public List&lt;String&gt; buildCommand(Map&lt;String, String&gt; env)
      throws IOException, IllegalArgumentException {
    if (PYSPARK_SHELL.equals(appResource) &amp;&amp; isAppResourceReq) {
      return buildPySparkShellCommand(env);
    } else if (SPARKR_SHELL.equals(appResource) &amp;&amp; isAppResourceReq) {
      return buildSparkRCommand(env);
    } else {
      return buildSparkSubmitCommand(env);
    }
  }
</code></pre>

<p>判断启动时的哪种环境py,shell,or submit 然后构建命令，继续查看<strong>buildSparkSubmitCommand</strong>函数</p>

<p>其中有如下代码:</p>

<pre><code>...
addPermGenSizeOpt(cmd);
    cmd.add(&quot;org.apache.spark.deploy.SparkSubmit&quot;);
    cmd.addAll(buildSparkSubmitArgs());
    return cmd;
</code></pre>

<p>好了继续查看org.apache.spark.deploy.SparkSubmit</p>

<p>spark main 线程dump 信息</p>

<pre><code>&quot;main&quot; #1 prio=5 os_prio=31 tid=0x00007f807180c800 nid=0x1c03 runnable [0x0000700003b08000]
   java.lang.Thread.State: RUNNABLE
    at java.io.FileInputStream.read0(Native Method)
    at java.io.FileInputStream.read(FileInputStream.java:207)
    at jline.internal.NonBlockingInputStream.read(NonBlockingInputStream.java:169)
    - locked &lt;0x00000007830bf508&gt; (a jline.internal.NonBlockingInputStream)
    at jline.internal.NonBlockingInputStream.read(NonBlockingInputStream.java:137)
    at jline.internal.NonBlockingInputStream.read(NonBlockingInputStream.java:246)
    at jline.internal.InputStreamReader.read(InputStreamReader.java:261)
    - locked &lt;0x00000007830bf508&gt; (a jline.internal.NonBlockingInputStream)
    at jline.internal.InputStreamReader.read(InputStreamReader.java:198)
    - locked &lt;0x00000007830bf508&gt; (a jline.internal.NonBlockingInputStream)
    at jline.console.ConsoleReader.readCharacter(ConsoleReader.java:2145)
    at jline.console.ConsoleReader.readLine(ConsoleReader.java:2349)
    at jline.console.ConsoleReader.readLine(ConsoleReader.java:2269)
    at scala.tools.nsc.interpreter.jline.InteractiveReader.readOneLine(JLineReader.scala:57)
    at scala.tools.nsc.interpreter.InteractiveReader$$anonfun$readLine$2.apply(InteractiveReader.scala:37)
    at scala.tools.nsc.interpreter.InteractiveReader$$anonfun$readLine$2.apply(InteractiveReader.scala:37)
    at scala.tools.nsc.interpreter.InteractiveReader$.restartSysCalls(InteractiveReader.scala:44)
    at scala.tools.nsc.interpreter.InteractiveReader$class.readLine(InteractiveReader.scala:37)
    at scala.tools.nsc.interpreter.jline.InteractiveReader.readLine(JLineReader.scala:28)
    at scala.tools.nsc.interpreter.ILoop.readOneLine(ILoop.scala:404)
        at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:413)
    at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:923)
    at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)
    at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)
    at scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97)
    at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:909)
    at org.apache.spark.repl.Main$.doMain(Main.scala:68)
    at org.apache.spark.repl.Main$.main(Main.scala:51)
    at org.apache.spark.repl.Main.main(Main.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

   Locked ownable synchronizers:
    - None

</code></pre>

<p>从堆栈中信息中我们可以看出程序的调用顺序:SparkSubmit.main =&gt; repl.Main.main =&gt; ILoop.process<br/>
ILoop.process 中如下代码:</p>

<pre><code>def process(settings: Settings): Boolean = savingContextLoader {
    this.settings = settings
    createInterpreter()

    // sets in to some kind of reader depending on environmental cues
    in = in0.fold(chooseReader(settings))(r =&gt; SimpleReader(r, out, interactive = true))
    globalFuture = future {
      intp.initializeSynchronous()
      loopPostInit()
      !intp.reporter.hasErrors
    }
    loadFiles(settings)
    printWelcome()

    try loop() match {
      case LineResults.EOF =&gt; out print Properties.shellInterruptedString
      case _               =&gt;
    }
    catch AbstractOrMissingHandler()
    finally closeInterpreter()

    true
  }
</code></pre>

<p>在process中我们发现调用了loadFiles并且打印Welcome信息</p>

<p>SparkLoop 继承了loadFiles并且复写了loadFiles 方法 如下:</p>

<pre><code>override def loadFiles(settings: Settings): Unit = {
    initializeSpark()
    super.loadFiles(settings)
  }
</code></pre>

<p>在loadFiles中调度initalizeSpark ,查看源码如下:</p>

<pre><code>def initializeSpark() {
    intp.beQuietDuring {
      processLine(&quot;&quot;&quot;
        @transient val spark = if (org.apache.spark.repl.Main.sparkSession != null) {
            org.apache.spark.repl.Main.sparkSession
          } else {
            org.apache.spark.repl.Main.createSparkSession()
          }
        @transient val sc = {
          val _sc = spark.sparkContext
          _sc.uiWebUrl.foreach(webUrl =&gt; println(s&quot;Spark context Web UI available at ${webUrl}&quot;))
          println(&quot;Spark context available as &#39;sc&#39; &quot; +
            s&quot;(master = ${_sc.master}, app id = ${_sc.applicationId}).&quot;)
          println(&quot;Spark session available as &#39;spark&#39;.&quot;)
          _sc
        }
        &quot;&quot;&quot;)
      processLine(&quot;import org.apache.spark.SparkContext._&quot;)
      processLine(&quot;import spark.implicits._&quot;)
      processLine(&quot;import spark.sql&quot;)
      processLine(&quot;import org.apache.spark.sql.functions._&quot;)
      replayCommandStack = Nil // remove above commands from session history.
    }
  }
</code></pre>

<p>从上面可以看出，如果SparkSession 已存在，那么直接返回，否则调用<strong>createSparkSession</strong></p>

<p>最后从SparkSession中返回SparkContext 查看<strong>createSparkSession</strong>源码</p>

<pre><code>def createSparkSession(): SparkSession = {
    val execUri = System.getenv(&quot;SPARK_EXECUTOR_URI&quot;)
    conf.setIfMissing(&quot;spark.app.name&quot;, &quot;Spark shell&quot;)
    // SparkContext will detect this configuration and register it with the RpcEnv&#39;s
    // file server, setting spark.repl.class.uri to the actual URI for executors to
    // use. This is sort of ugly but since executors are started as part of SparkContext
    // initialization in certain cases, there&#39;s an initialization order issue that prevents
    // this from being set after SparkContext is instantiated.
    conf.set(&quot;spark.repl.class.outputDir&quot;, outputDir.getAbsolutePath())
    if (execUri != null) {
      conf.set(&quot;spark.executor.uri&quot;, execUri)
    }
    if (System.getenv(&quot;SPARK_HOME&quot;) != null) {
      conf.setSparkHome(System.getenv(&quot;SPARK_HOME&quot;))
    }

    val builder = SparkSession.builder.config(conf)
    if (conf.get(CATALOG_IMPLEMENTATION.key, &quot;hive&quot;).toLowerCase == &quot;hive&quot;) {
      if (SparkSession.hiveClassesArePresent) {
        // In the case that the property is not set at all, builder&#39;s config
        // does not have this value set to &#39;hive&#39; yet. The original default
        // behavior is that when there are hive classes, we use hive catalog.
        sparkSession = builder.enableHiveSupport().getOrCreate()
        logInfo(&quot;Created Spark session with Hive support&quot;)
      } else {
        // Need to change it back to &#39;in-memory&#39; if no hive classes are found
        // in the case that the property is set to hive in spark-defaults.conf
        builder.config(CATALOG_IMPLEMENTATION.key, &quot;in-memory&quot;)
        sparkSession = builder.getOrCreate()
        logInfo(&quot;Created Spark session&quot;)
      }
    } else {
      // In the case that the property is set but not to &#39;hive&#39;, the internal
      // default is &#39;in-memory&#39;. So the sparkSession will use in-memory catalog.
      sparkSession = builder.getOrCreate()
      logInfo(&quot;Created Spark session&quot;)
    }
    sparkContext = sparkSession.sparkContext
    Signaling.cancelOnInterrupt(sparkContext)
    sparkSession
  }
</code></pre>

<p>这里最后使用SparkConf 设置一些必要的参数并且通过Builder 创建sparkSession 并且判断是否需要启用Hive的支持。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[spark 源码编译流程]]></title>
    <link href="http://www.blacklight.xin/14815990344230.html"/>
    <updated>2016-12-13T11:17:14+08:00</updated>
    <id>http://www.blacklight.xin/14815990344230.html</id>
    <content type="html"><![CDATA[
<p>已v2.0.2 版本为列进行源码编译</p>

<ol>
<li>从github 上clone源码 <strong>git clone git://git.apache.org/spark.git</strong></li>
<li>切换到v2.0.2 <strong>git checkout v2.0.2 -b dev_2.0.2</strong></li>
<li>build/mvn -DskipTests install 此部分等待时间最长，下面必要的软件和jar包</li>
<li>给idea 装scala 插件，具体就不说了，要是网速不好可以下载下来，选择从磁盘安装<a href="https://plugins.jetbrains.com/?idea">idea plugin</a></li>
<li>导入idea，在官方文档上有下面一些话</li>
</ol>

<span id="more"></span><!-- more -->

<blockquote>
<p>Some of the modules have pluggable source directories based on Maven profiles (i.e. to support both Scala 2.11 and 2.10 or to allow cross building against different versions of Hive). In some cases IntelliJ’s does not correctly detect use of the maven-build-plugin to add source directories. In these cases, you may need to add source locations explicitly to compile the entire project. If so, open the “Project Settings” and select “Modules”. Based on your selected Maven profiles, you may need to add source folders to the following modules:<br/>
<strong>spark-hive: add v0.13.1/src/main/scala</strong><br/>
<strong>spark-streaming-flume-sink: add target\scala-2.10\src_managed\main\compiled_avro</strong></p>
</blockquote>

<p>意思就是idea 在某些情况下不能正确的决定source 目录，需要我们手动标记。我的情况是spark-streaming-flume-sink 这个moudle没有正确的标记出来，手动标记一下source 就行。<br/>
不然运行example时候回报classnotfound</p>

<ol>
<li>在运行过程中发现找不到guava中的类,clipse jetty 类找不到等，找打spark—parent-2.11下的pom </li>
</ol>

<p><img src="media/14815990344230/14816790522215.jpg" alt=""/><br/>
如下图所示，将guava 和jetty 相关的依赖scope 都改成compile<br/>
<img src="media/14815990344230/14816791028761.jpg" alt=""/></p>

<p>重新运行example中的列子，正确执行。</p>

<p>开始你的源码之旅吧！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[maven 遇到不能下载问题]]></title>
    <link href="http://www.blacklight.xin/14813767804827.html"/>
    <updated>2016-12-10T21:33:00+08:00</updated>
    <id>http://www.blacklight.xin/14813767804827.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">问题描述</h2>

<pre><code>Caused by: org.eclipse.aether.transfer.ArtifactTransferException: Could not transfer artifact org.apache.spark:spark-core_2.11:pom:2.0.2 from/to central (https://repo.maven.apache.org/maven2): sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
</code></pre>

<h2 id="toc_1">原因</h2>

<p>maven 用的默认配置，中央仓库https：xxx，所以改成<a href="http://repo.maven.apache.org/maven2%EF%BC%8C%E9%97%AE%E9%A2%98%E5%8D%B3%E5%8F%AF%E8%A7%A3%E5%86%B3">http://repo.maven.apache.org/maven2，问题即可解决</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[提交spark应用]]></title>
    <link href="http://www.blacklight.xin/14812978853380.html"/>
    <updated>2016-12-09T23:38:05+08:00</updated>
    <id>http://www.blacklight.xin/14812978853380.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">提交应用</h2>

<p>在bin目录中spark-submit 脚本用于向集群启动你的应用，通过统一的接口，可以支持所有spark 支持的clusterManager，所以你没有必要为你的每个应用坐一些特殊的配置。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_1">打包你的应用程序</h2>

<p>如果你的代码依赖与其他的项目，为了分发所有代码到集群，需要把他们和你的应用打包在一起，所以你可以打一个assembly jar 或者&quot;uber jar&quot; 包含你的代码和依赖。sbt和maven 都有assembly 插件。在创建assembly jar 时，将hadoop和spark的依赖设置成为provided，这些依赖在运行时cluster已经包含了，所以不需要用户打包进来。</p>

<h2 id="toc_2">Loading Configuration from a File</h2>

<p>spark-submit 会自动从配置文件中加载默认的属性值传递给你的应用程序。默认读取的是conf/spark-default.conf<br/>
通过这种方式加载配置属性可以取消spark-submit 上的参数，列如：加入spark.master 在应用程序中已经设置了那么就可以在spark-submit 取消--master 选项。<strong>总之，通过SparkConf 设置的属性具有最高的优先级，其次是通过spark-submit 传递的，最后是配置文件中默认的。</strong></p>

<h2 id="toc_3">用spark-submit 启动你的应用</h2>

<p>一旦你的应用打包好了，你就可以用spark-submit 脚本启动你的应用了。这个脚本会设置spark的classpath和它的依赖。并且可以支持spark所支持的所有<strong>clusterManager</strong> 和 <strong>deployMode</strong></p>

<pre><code class="language-shell">./bin/spark-submit \
  --class &lt;main-class&gt; \
  --master &lt;master-url&gt; \
  --deploy-mode &lt;deploy-mode&gt; \
  --conf &lt;key&gt;=&lt;value&gt; \
  ... # other options
  &lt;application-jar&gt; \
  [application-arguments]
</code></pre>

<p>下面讲解一些通用的选项：</p>

<ul>
<li>--class:程序的切入点（eg:org.apache.spark.examples.SparkPi）</li>
<li>--master:集群的master url（eg.spark://xxx.xxx.xxx.xxx:xxx）</li>
<li>--deploy-mode:决定你的driver程序是否在worker node（cluster模式下），或者作为一个外部client（clinet 模式）<strong>默认值是client</strong></li>
<li>--conf 以key=value的形式设置任意的spark properties，如果value包含空格，用双引号括起来，</li>
<li><application-jar> 你的应用程序和依赖的jar的路径，URL 必须对集群是全局可见的，列如:hdfs://  或者 a file:// 路径必须在所有的node都可见</li>
<li>application-arguments 如果有额外的参数需要传递给你的main程序，请用这个选项</li>
</ul>

<p>一个常用的提交应用的策略是通过一个网关机器(<strong>gateway machine</strong>),并且这个机器在物理位置上接近你的worker node，这种方式client模式是非常适合的。在client 模式下，dirver直接被spark-submit启动扮演集群一个client的角色。应用的输出都在console上，因此这种模式非常十分的适合那些需要REPL的应用（比如Spark shell）.</p>

<p>有一个可用选项可以指定clusterManager，在standalone集群中，cluster 模式下，你可以指定--supervise 以确保driver程序非正常退出的情况下自动重启driver。--help 显示所有可用的选项。</p>

<p><strong>下面是一些可用的选项</strong></p>

<pre><code class="language-shell"># Run application locally on 8 cores
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master local[8] \
  /path/to/examples.jar \
  100

# Run on a Spark standalone cluster in client deploy mode
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --executor-memory 20G \
  --total-executor-cores 100 \
  /path/to/examples.jar \
  1000

# Run on a Spark standalone cluster in cluster deploy mode with supervise
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --deploy-mode cluster \
  --supervise \
  --executor-memory 20G \
  --total-executor-cores 100 \
  /path/to/examples.jar \
  1000

# Run on a YARN cluster
export HADOOP_CONF_DIR=XXX
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master yarn \
  --deploy-mode cluster \  # can be client for client mode
  --executor-memory 20G \
  --num-executors 50 \
  /path/to/examples.jar \
  1000

# Run a Python application on a Spark standalone cluster
./bin/spark-submit \
  --master spark://207.184.161.138:7077 \
  examples/src/main/python/pi.py \
  1000

# Run on a Mesos cluster in cluster deploy mode with supervise
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master mesos://207.184.161.138:7077 \
  --deploy-mode cluster \
  --supervise \
  --executor-memory 20G \
  --total-executor-cores 100 \
  http://path/to/examples.jar \
  1000
</code></pre>

<h2 id="toc_4">Master URLS</h2>

<p>master url 可以是以下的格式：</p>

<table>
<thead>
<tr>
<th>MasterURL</th>
<th>解释</th>
</tr>
</thead>

<tbody>
<tr>
<td>local</td>
<td>本地模式一个worker线程（没有并发）</td>
</tr>
<tr>
<td>local[k]</td>
<td>本地模式K个worker线程（设置成你机器的core数）</td>
</tr>
<tr>
<td>local[*]</td>
<td>本地模式多个worker线程数—&gt;你机器的逻辑core数量</td>
</tr>
<tr>
<td>spark://HOST:PORT</td>
<td>连接到一个spark standalone 集群，端口号必须是master指定的，7007是默认的</td>
</tr>
<tr>
<td>mesos://HOST:PORT</td>
<td>连接到一个Messon集群</td>
</tr>
<tr>
<td>yarn</td>
<td>连接到一个yarn集群。—deploy-mode 指定client或者cluster模式，集群地址通过HADOOP_CONF_DIR or YARN_CONF_DIR 自动发现</td>
</tr>
</tbody>
</table>

<h2 id="toc_5">通过文件配置</h2>

<p>spark-submit 会自动从配置文件中加载默认的属性值传递给你的应用程序。默认读取的是conf/spark-default.conf<br/>
通过这种方式加载配置属性可以取消spark-submit 上的参数，列如：加入spark.master 在应用程序中已经设置了那么就可以在spark-submit 取消--master 选项。<strong>总之，通过SparkConf 设置的属性具有最高的优先级，其次是通过spark-submit 传递的，最后是配置文件中默认的。</strong></p>

]]></content>
  </entry>
  
</feed>
